{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee85d2bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/atarashansky/Desktop/czi/single-cell-data-portal\n"
     ]
    }
   ],
   "source": [
    "cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44f3e308",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "* Owlready2 * Warning: optimized Cython parser module 'owlready2_optimized' is not available, defaulting to slower Python implementation\n"
     ]
    }
   ],
   "source": [
    "import cellxgene_census\n",
    "from backend.wmg.data.rollup import rollup_across_cell_type_descendants\n",
    "import owlready2\n",
    "import json\n",
    "import tiledb\n",
    "from backend.wmg.data.ontology_labels import ontology_term_label, ontology_term_id_labels\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def traverse_with_counting(node):\n",
    "    global traverse_node_counter\n",
    "    global all_unique_nodes\n",
    "    node_count = traverse_node_counter.get(node.name, 0)\n",
    "    traverse_node_counter[node.name] = node_count + 1\n",
    "    all_unique_nodes.add(node.name +\"__\"+str(node_count))\n",
    "    \n",
    "    subclasses = list(node.subclasses())\n",
    "    node_id = node.name.replace('_',':')\n",
    "    if len(subclasses) == 0:\n",
    "        return {\"id\": node.name+\"__\"+str(node_count),\n",
    "                \"name\": id_to_name[node_id] if node_id in id_to_name else node_id,\n",
    "                \"n_cells_rollup\": int(cell_counts_df_rollup[node_id] if node_id in cell_counts_df_rollup else 0),\n",
    "                \"n_cells\": int(cell_counts_df[node_id] if node_id in cell_counts_df else 0),\n",
    "               }\n",
    "        \n",
    "    children = []\n",
    "    for child in subclasses:\n",
    "        children.append(traverse_with_counting(child))\n",
    "\n",
    "    return {\"id\": node.name+\"__\"+str(node_count),\n",
    "                \"name\": id_to_name[node_id] if node_id in id_to_name else node_id,\n",
    "                \"n_cells_rollup\": int(cell_counts_df_rollup[node_id] if node_id in cell_counts_df_rollup else 0),\n",
    "                \"n_cells\": int(cell_counts_df[node_id] if node_id in cell_counts_df else 0),\n",
    "                \"children\": children,\n",
    "               }\n",
    "\n",
    "def _descendants(cell_type):\n",
    "    cell_type_iri = cell_type.replace(\":\", \"_\")\n",
    "    entity = ontology.search_one(iri=f\"http://purl.obolibrary.org/obo/{cell_type_iri}\")\n",
    "    descendants = [i.name.replace(\"_\", \":\") for i in entity.descendants()] if entity else [cell_type]\n",
    "    return descendants\n",
    "\n",
    "def _ancestors(cell_type):\n",
    "    cell_type_iri = cell_type.replace(\":\", \"_\")\n",
    "    entity = ontology.search_one(iri=f\"http://purl.obolibrary.org/obo/{cell_type_iri}\")\n",
    "    ancestors = [i.name.replace(\"_\", \":\") for i in entity.ancestors() if i.name!= \"Thing\"] if entity else [cell_type]\n",
    "    return ancestors\n",
    "\n",
    "def _children(cell_type):\n",
    "    cell_type_iri = cell_type.replace(\":\", \"_\")\n",
    "    entity = ontology.search_one(iri=f\"http://purl.obolibrary.org/obo/{cell_type_iri}\")\n",
    "    children = [i.name.replace(\"_\", \":\") for i in entity.subclasses()] if entity else [cell_type]\n",
    "    return children\n",
    "\n",
    "def _parents(cell_type):\n",
    "    cell_type_iri = cell_type.replace(\":\", \"_\")\n",
    "    entity = ontology.search_one(iri=f\"http://purl.obolibrary.org/obo/{cell_type_iri}\")    \n",
    "    parent_names = [parent.name.replace(\"_\",\":\") for parent in entity.is_a if isinstance(parent, owlready2.ThingClass) and parent.name!= \"Thing\"]\n",
    "    return parent_names\n",
    "\n",
    "def dfs(parents, end, start, node=None, path = None, all_paths = []):\n",
    "    if path is None and node is None:\n",
    "        path = [end]\n",
    "        node = end\n",
    "\n",
    "    if node == start:\n",
    "        return path\n",
    "    \n",
    "    for parent in parents.get(node,[]):\n",
    "        full_path = dfs(parents, end, start, node=parent, path = path+[parent], all_paths=all_paths)\n",
    "        if full_path:\n",
    "            all_paths.append(full_path)\n",
    "            \n",
    "def truncate_graph(graph,valid_nodes):   \n",
    "    if graph['id'] not in valid_nodes:\n",
    "        return False\n",
    "\n",
    "    children= graph.get(\"children\",[])\n",
    "    valid_children = []\n",
    "    append_dummy = False\n",
    "    \n",
    "    invalid_children_ids = []\n",
    "    for child in children:\n",
    "        is_valid = truncate_graph(child, valid_nodes)\n",
    "        if is_valid:\n",
    "            valid_children.append(child)\n",
    "        elif child['id']!='':\n",
    "            invalid_children_ids.append(child['id'])\n",
    "            append_dummy = True\n",
    "\n",
    "    if append_dummy and len(valid_children) > 0:\n",
    "        valid_children.append(\n",
    "            {\"id\": \"\",\n",
    "            \"name\": \"\",\n",
    "            \"n_cells_rollup\": 0,\n",
    "            \"n_cells\": 0,\n",
    "             \"invalid_children_ids\": invalid_children_ids,\n",
    "            \"parent\": graph['id']\n",
    "            }        \n",
    "        )\n",
    "    if len(valid_children) > 0:\n",
    "        graph['children'] = valid_children\n",
    "    else:\n",
    "        if 'children' in graph:\n",
    "            del graph['children']\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "def truncate_graph2(graph, visited_nodes_in_paths):\n",
    "    # i want every node to only show children once\n",
    "    # this means deleting \"children\" if seen more than once\n",
    "    # EXCEPT if one of your children is in a path leading to acinar cell.\n",
    "    # Then, you collapse the remaining children\n",
    "    global nodesWithChildrenFound\n",
    "    if graph['id'].split(\"__\")[0] in nodesWithChildrenFound:\n",
    "        if 'children' in graph:\n",
    "            children = graph['children']            \n",
    "            new_children = []\n",
    "            invalid_children_ids = []\n",
    "            for child in children:\n",
    "                if child['id'] in visited_nodes_in_paths:\n",
    "                    new_children.append(child)\n",
    "                elif child['id'] != '':\n",
    "                    invalid_children_ids.append(child['id'])\n",
    "                    \n",
    "            if len(children) > len(new_children) and len(new_children) > 0:\n",
    "                # append dummy\n",
    "                new_children.append(\n",
    "                    {\"id\": \"\",\n",
    "                    \"name\": \"\",\n",
    "                    \"n_cells_rollup\": 0,\n",
    "                    \"n_cells\": 0,\n",
    "                     \"invalid_children_ids\": invalid_children_ids,\n",
    "                     \"parent\": graph['id']\n",
    "                    }        \n",
    "                )\n",
    "            if len(new_children) > 0:\n",
    "                graph['children'] = new_children\n",
    "            else:\n",
    "                del graph['children']\n",
    "    elif 'children' in graph:\n",
    "        nodesWithChildrenFound.add(graph['id'].split(\"__\")[0])\n",
    "    \n",
    "    \n",
    "    children = graph.get(\"children\",[])\n",
    "    for child in children:\n",
    "        if child['id'] != \"\":\n",
    "            truncate_graph2(child, visited_nodes_in_paths)\n",
    "\n",
    "\n",
    "def prune_node_distinguishers(graph):\n",
    "    graph['id'] = graph['id'].split('__')[0]\n",
    "    for child in graph.get('children',[]):\n",
    "        prune_node_distinguishers(child)\n",
    "\n",
    "def delete_unknown_terms(graph):\n",
    "    new_children = []\n",
    "    for child in graph.get('children',[]):\n",
    "        unknown = child['name'].startswith('CL:')\n",
    "        if not unknown:\n",
    "            new_children.append(child)\n",
    "    if len(new_children) > 0:\n",
    "        graph['children'] = new_children\n",
    "    elif 'children' in graph:\n",
    "        del graph['children']\n",
    "    \n",
    "    for child in graph.get('children',[]):\n",
    "        delete_unknown_terms(child)\n",
    "        \n",
    "def truncate_graph_one_target(graph, target):\n",
    "    global targetFound\n",
    "    if targetFound and graph['id'].split(\"__\")[0] == target.split(\"__\")[0]:\n",
    "        del graph['children']\n",
    "    elif graph['id'] == target:\n",
    "        targetFound = True\n",
    "    \n",
    "    children = graph.get(\"children\",[])\n",
    "    for child in children:\n",
    "        truncate_graph_one_target(child, target)\n",
    "\n",
    "def build_children(graph):\n",
    "    global all_children\n",
    "    children = graph.get('children',[])\n",
    "    if len(children) == 0:\n",
    "        ids = []\n",
    "    else:\n",
    "        ids = [child['id'] for child in children]\n",
    "        \n",
    "    all_children[graph['id']] = ids\n",
    "    \n",
    "    for child in children:\n",
    "        build_children(child)\n",
    "\n",
    "def build_parents(graph):\n",
    "    global all_parents\n",
    "    children = graph.get('children',[])\n",
    "    \n",
    "    for child in children:\n",
    "        all_parents[child['id']]=[graph['id']]\n",
    "        build_parents(child)\n",
    "        \n",
    "def getExpandedData(graph):\n",
    "    global isExpandedNodes\n",
    "    if 'children' in graph:\n",
    "        isExpandedNodes.append(graph['id'])\n",
    "        for child in graph['children']:\n",
    "            getExpandedData(child)\n",
    "                \n",
    "        \n",
    "def getShownData(graph):\n",
    "    global notShownWhenExpandedNodes\n",
    "    \n",
    "    if 'children' in graph:\n",
    "        for child in graph['children']:\n",
    "            if child['id'] == \"\":\n",
    "                if len(child[\"invalid_children_ids\"]) > 0:\n",
    "                    notShownWhenExpandedNodes.append({child['parent']: list(set(child[\"invalid_children_ids\"]))})\n",
    "            else:\n",
    "                getShownData(child)\n",
    "        \n",
    "def _to_dict(a, b):\n",
    "    \"\"\"\n",
    "    convert a flat key array (a) and a value array (b) into a dictionary with values grouped by keys\n",
    "    \"\"\"\n",
    "    a = np.array(a)\n",
    "    b = np.array(b)\n",
    "    idx = np.argsort(a)\n",
    "    a = a[idx]\n",
    "    b = b[idx]\n",
    "    bounds = np.where(a[:-1] != a[1:])[0] + 1\n",
    "    bounds = np.append(np.append(0, bounds), a.size)\n",
    "    bounds_left = bounds[:-1]\n",
    "    bounds_right = bounds[1:]\n",
    "    slists = [b[bounds_left[i] : bounds_right[i]] for i in range(bounds_left.size)]\n",
    "    d = dict(zip(np.unique(a), [list(set(x)) for x in slists]))\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de757824",
   "metadata": {},
   "source": [
    "# Build ontology tree JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd79ef1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The \"stable\" release is currently 2023-05-15. Specify 'census_version=\"2023-05-15\"' in future calls to open_soma() to ensure data consistency.\n"
     ]
    }
   ],
   "source": [
    "census = cellxgene_census.open_soma()\n",
    "c = census['census_info']['summary_cell_counts'].read().concat().to_pandas()\n",
    "cell_counts_df = c[[i.startswith('CL:') for i in c['ontology_term_id']]].groupby('ontology_term_id').sum(numeric_only=True)[['unique_cell_count']]\n",
    "cell_counts_df['n_cells'] = cell_counts_df['unique_cell_count']\n",
    "del cell_counts_df['unique_cell_count']\n",
    "cell_counts_df['cell_type_ontology_term_id'] = cell_counts_df.index.values\n",
    "cell_counts_df=cell_counts_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fda95a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = census['census_data']['homo_sapiens']['obs'].read().concat().to_pandas()\n",
    "\n",
    "a2,b2=obs[['cell_type_ontology_term_id','tissue_ontology_term_id']].values.T\n",
    "\n",
    "uberon_by_celltype = _to_dict(b2,a2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06a16a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "ontology = owlready2.get_ontology(\"https://github.com/obophenotype/cell-ontology/releases/latest/download/cl-basic.owl\")\n",
    "ontology.load()\n",
    "\n",
    "all_cell_types = []\n",
    "classes = ontology.classes()\n",
    "all_cell_type_owl_descriptions = {}\n",
    "id_to_name = {}\n",
    "for c in classes :\n",
    "    if not c.name.startswith(\"CL_\"):\n",
    "        continue\n",
    "    if c.deprecated :\n",
    "        continue\n",
    "    all_cell_types.append(\n",
    "    {\n",
    "        \"label\": c.label.first(),\n",
    "        \"id\": c.name.replace(\"_\",\":\")\n",
    "    }\n",
    "    )\n",
    "    id_to_name[c.name.replace(\"_\",\":\")] = c.label.first()\n",
    "    \n",
    "    if str(c.IAO_0000115.first()) == 'None':\n",
    "        all_cell_type_owl_descriptions[c.name.replace(\"_\",\":\")] = ''\n",
    "    else:\n",
    "        all_cell_type_owl_descriptions[c.name.replace(\"_\",\":\")] = str(c.IAO_0000115.first())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "faf8e25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "uberon_by_celltype = _to_dict(b2,a2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc857f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cell_types_ids = [i[\"id\"] for i in all_cell_types]\n",
    "to_attach = pd.DataFrame()\n",
    "to_attach['cell_type_ontology_term_id']=[i for i in all_cell_types_ids if i not in cell_counts_df['cell_type_ontology_term_id'].values]\n",
    "to_attach['n_cells']=0\n",
    "\n",
    "cell_counts_df = pd.concat([cell_counts_df,to_attach],axis=0)\n",
    "cell_counts_df_rollup = rollup_across_cell_type_descendants(cell_counts_df).set_index('cell_type_ontology_term_id')['n_cells']\n",
    "cell_counts_df = cell_counts_df.set_index('cell_type_ontology_term_id')['n_cells']\n",
    "\n",
    "root_node = ontology.world[\"http://purl.obolibrary.org/obo/CL_0000548\"]\n",
    "\n",
    "traverse_node_counter = {}\n",
    "all_unique_nodes = set()\n",
    "a = traverse_with_counting(root_node) \n",
    "all_unique_nodes = list(all_unique_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d83b31b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_children={}\n",
    "all_parents={}    \n",
    "build_children(a)\n",
    "build_parents(a) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7ddd9f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 34\u001b[0m\n\u001b[1;32m     29\u001b[0m siblings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(\u001b[38;5;28msum\u001b[39m(siblings,[])))\n\u001b[1;32m     32\u001b[0m valid_nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(visited_nodes_in_paths \u001b[38;5;241m+\u001b[39m children1 \u001b[38;5;241m+\u001b[39m children2 \u001b[38;5;241m+\u001b[39m siblings))\n\u001b[0;32m---> 34\u001b[0m a_copy \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(\u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     35\u001b[0m truncate_graph(a_copy,valid_nodes) \n\u001b[1;32m     37\u001b[0m nodesWithChildrenFound\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda/envs/scdp2/lib/python3.9/json/__init__.py:231\u001b[0m, in \u001b[0;36mdumps\u001b[0;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;66;03m# cached encoder\u001b[39;00m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m skipkeys \u001b[38;5;129;01mand\u001b[39;00m ensure_ascii \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    228\u001b[0m     check_circular \u001b[38;5;129;01mand\u001b[39;00m allow_nan \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m indent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m separators \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    230\u001b[0m     default \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sort_keys \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 231\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_encoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONEncoder\n",
      "File \u001b[0;32m~/miniconda/envs/scdp2/lib/python3.9/json/encoder.py:199\u001b[0m, in \u001b[0;36mJSONEncoder.encode\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m encode_basestring(o)\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# This doesn't pass the iterator directly to ''.join() because the\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# exceptions aren't as detailed.  The list call should be roughly\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# equivalent to the PySequence_Fast that ''.join() would do.\u001b[39;00m\n\u001b[0;32m--> 199\u001b[0m chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_one_shot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(chunks, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m    201\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(chunks)\n",
      "File \u001b[0;32m~/miniconda/envs/scdp2/lib/python3.9/json/encoder.py:257\u001b[0m, in \u001b[0;36mJSONEncoder.iterencode\u001b[0;34m(self, o, _one_shot)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    253\u001b[0m     _iterencode \u001b[38;5;241m=\u001b[39m _make_iterencode(\n\u001b[1;32m    254\u001b[0m         markers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault, _encoder, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindent, floatstr,\n\u001b[1;32m    255\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_separator, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitem_separator, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msort_keys,\n\u001b[1;32m    256\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskipkeys, _one_shot)\n\u001b[0;32m--> 257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_iterencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_node = 'CL_0000548__0'\n",
    "\n",
    "all_states_per_cell_type = {}\n",
    "for i,end_node in enumerate(all_cell_types_ids):\n",
    "    if i%100==0:\n",
    "        print(i)\n",
    "    end_node = end_node.replace(\":\",\"_\")\n",
    "    if end_node in traverse_node_counter:\n",
    "        all_paths=[]\n",
    "        for i in range(traverse_node_counter[end_node]):\n",
    "            paths = []    \n",
    "            dfs(all_parents,end_node+\"__\"+str(i),start_node,all_paths=paths)\n",
    "            paths = [i[::-1] for i in paths] \n",
    "            if len(paths) == 0:\n",
    "                all_paths.append([end_node+\"__\"+str(i)])\n",
    "            else:\n",
    "                all_paths.append(paths[0])\n",
    "\n",
    "        ### RULES ###\n",
    "        # 1. We only want to show terms that are CHILDREN, GRANDCHILDREN, SIBLINGS OF TARGET, or IN A PATH TO TARGET\n",
    "        visited_nodes_in_paths = list(set(sum(all_paths,[])))\n",
    "\n",
    "        children1 = all_children.get(end_node+\"__0\",[]) #children\n",
    "        children2 = sum([all_children.get(child,[]) for child in children1],[]) #grandchildren\n",
    "        siblings=[]\n",
    "        for i in range(traverse_node_counter[end_node]):\n",
    "            sibs = sum([all_children.get(parent,[]) for parent in all_parents.get(end_node+\"__\"+str(i),[])],[]) #siblings\n",
    "            siblings.append(sibs)\n",
    "        siblings = list(set(sum(siblings,[])))\n",
    "\n",
    "\n",
    "        valid_nodes = list(set(visited_nodes_in_paths + children1 + children2 + siblings))\n",
    "\n",
    "        a_copy = json.loads(json.dumps(a))\n",
    "        truncate_graph(a_copy,valid_nodes) \n",
    "\n",
    "        nodesWithChildrenFound=set()\n",
    "        truncate_graph2(a_copy, visited_nodes_in_paths)\n",
    "        delete_unknown_terms(a_copy)\n",
    "        \n",
    "        # now, given this graph, populate what you need - specifically, we need \"notShownWhenExpanded\" and \"isExpanded\"\n",
    "        notShownWhenExpandedNodes=[]\n",
    "        isExpandedNodes=[]\n",
    "        \n",
    "        getExpandedData(a_copy)\n",
    "        getShownData(a_copy)\n",
    "\n",
    "        assert(len(list(set([list(i.keys())[0] for i in notShownWhenExpandedNodes])))==len(notShownWhenExpandedNodes))        \n",
    "        \n",
    "        notShownWhenExpanded = {}\n",
    "        for i in notShownWhenExpandedNodes:\n",
    "            notShownWhenExpanded.update(i)\n",
    "            \n",
    "        all_states_per_cell_type[end_node] = {'isExpandedNodes': list(set(isExpandedNodes)), 'notShownWhenExpandedNodes': notShownWhenExpanded}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9b3cafee",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_node = 'CL_0000548__0'\n",
    "\n",
    "all_states_per_tissue = {}\n",
    "for tissue in uberon_by_celltype:\n",
    "    end_nodes = uberon_by_celltype[tissue]\n",
    "    \n",
    "    end_nodes = [e.replace(\":\",\"_\") for e in end_nodes]\n",
    "    all_paths=[]\n",
    "    for end_node in end_nodes:\n",
    "        if end_node in traverse_node_counter:\n",
    "            for i in range(traverse_node_counter[end_node]):\n",
    "                paths = []    \n",
    "                dfs(all_parents,end_node+\"__\"+str(i),start_node,all_paths=paths)\n",
    "                paths = [i[::-1] for i in paths] \n",
    "                if len(paths) == 0:\n",
    "                    all_paths.append([end_node+\"__\"+str(i)])\n",
    "                else:\n",
    "                    all_paths.append(paths[0])\n",
    "\n",
    "    ### RULES ###\n",
    "    # 1. We only want to show terms that are CHILDREN, GRANDCHILDREN, SIBLINGS OF TARGET, or IN A PATH TO TARGET\n",
    "    visited_nodes_in_paths = list(set(sum(all_paths,[])))\n",
    "\n",
    "    children1 = list(set(sum([all_children.get(e+\"__0\",[]) for e in end_nodes],[])))\n",
    "\n",
    "    valid_nodes = list(set(visited_nodes_in_paths + children1))\n",
    "\n",
    "    a_copy = json.loads(json.dumps(a))\n",
    "    truncate_graph(a_copy,valid_nodes) \n",
    "\n",
    "    nodesWithChildrenFound=set()\n",
    "    truncate_graph2(a_copy, visited_nodes_in_paths)\n",
    "    delete_unknown_terms(a_copy)\n",
    "\n",
    "    # now, given this graph, populate what you need - specifically, we need \"notShownWhenExpanded\" and \"isExpanded\"\n",
    "    notShownWhenExpandedNodes=[]\n",
    "    isExpandedNodes=[]\n",
    "\n",
    "    getExpandedData(a_copy)\n",
    "    getShownData(a_copy)\n",
    "\n",
    "    assert(len(list(set([list(i.keys())[0] for i in notShownWhenExpandedNodes])))==len(notShownWhenExpandedNodes))        \n",
    "\n",
    "    notShownWhenExpanded = {}\n",
    "    for i in notShownWhenExpandedNodes:\n",
    "        notShownWhenExpanded.update(i)\n",
    "\n",
    "    all_states_per_tissue[tissue] = {'isExpandedNodes': list(set(isExpandedNodes)), 'notShownWhenExpandedNodes': notShownWhenExpanded}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a2cbd637",
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(all_states_per_tissue,open('ontologyTreeStatePerTissue.json','w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "665b6607",
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_unknown_terms(a)\n",
    "json.dump(a,open('ontologyTree.json','w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47ac0685",
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(all_states_per_cell_type,open('ontologyTreeStatePerCellType.json','w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77ef0d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv ontologyTree.json frontend/src/views/CellCards/common/fixtures/.\n",
    "!mv ontologyTreeStatePerCellType.json frontend/src/views/CellCards/common/fixtures/.\n",
    "!mv ontologyTreeStatePerTissue.json frontend/src/views/CellCards/common/fixtures/."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
