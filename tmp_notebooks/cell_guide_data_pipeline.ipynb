{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a057443",
   "metadata": {},
   "source": [
    "# CellGuide data pipeline prototype\n",
    "\n",
    "This assumes a local snapshot has been downloaded to the same location as this notebook.\n",
    "\n",
    "Here is a one-liner for downloading the latest prod snapshot:\n",
    "\n",
    "```\n",
    "AWS_PROFILE=single-cell-prod aws s3 sync s3://cellxgene-wmg-prod/$(AWS_PROFILE=single-cell-prod aws s3 cp s3://cellxgene-wmg-prod/latest_snapshot_identifier -)/ prod-snapshot/\n",
    "```\n",
    "\n",
    "**This file should be in the root folder of the `single-cell-data-portal` repo**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a600b5f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/single-cell-data-portal\n"
     ]
    }
   ],
   "source": [
    "cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8ed8504",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "* Owlready2 * Warning: optimized Cython parser module 'owlready2_optimized' is not available, defaulting to slower Python implementation\n"
     ]
    }
   ],
   "source": [
    "from backend.wmg.api.v1 import build_filter_dims_values\n",
    "from backend.wmg.data.ontology_labels import ontology_term_label, ontology_term_id_labels\n",
    "from backend.wmg.data.snapshot import WmgSnapshot\n",
    "from backend.wmg.data.query import WmgFiltersQueryCriteria\n",
    "import tiledb\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import openai\n",
    "from backend.wmg.data.utils import get_datasets_from_curation_api, get_collections_from_curation_api\n",
    "from backend.wmg.data.rollup import (\n",
    "    rollup_across_cell_type_descendants,\n",
    "    rollup_across_cell_type_descendants_array,\n",
    "    are_cell_types_colinear,\n",
    "    _descendants,\n",
    ")\n",
    "import glob\n",
    "import requests\n",
    "import itertools\n",
    "from scipy import stats\n",
    "import owlready2\n",
    "from pronto import Ontology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e20b2ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sn = \"prod-snapshot\"\n",
    "snapshot = WmgSnapshot(snapshot_identifier=\"\",\n",
    "    expression_summary_cube=tiledb.open(f'{sn}/expression_summary'),\n",
    "    marker_genes_cube=tiledb.open(f'{sn}/marker_genes'),\n",
    "    expression_summary_default_cube=tiledb.open(f'{sn}/expression_summary_default'),\n",
    "    expression_summary_fmg_cube=tiledb.open(f'{sn}/expression_summary_fmg'),                       \n",
    "    cell_counts_cube=tiledb.open(f'{sn}/cell_counts'),\n",
    "    cell_type_orderings=pd.read_json(f'{sn}/cell_type_orderings.json'),\n",
    "    primary_filter_dimensions=json.load(open(f'{sn}/primary_filter_dimensions.json','r')),\n",
    "    dataset_to_gene_ids=json.load(open(f'{sn}/dataset_to_gene_ids.json','r')), \n",
    "    filter_relationships=json.load(open(f'{sn}/filter_relationships.json','r')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac603523",
   "metadata": {},
   "source": [
    "## Generate all cell types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ee9cdab",
   "metadata": {},
   "outputs": [],
   "source": [
    "ontology = Ontology(\"https://github.com/obophenotype/cell-ontology/releases/latest/download/cl-basic.obo\")\n",
    "\n",
    "\n",
    "all_cell_types = json.load(open('frontend/src/views/CellCards/common/fixtures/allCellTypes.json','r'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e6b389",
   "metadata": {},
   "source": [
    "## Generate cell type descriptions using GPT 3.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e97b2a",
   "metadata": {},
   "source": [
    "System role:\n",
    "> You are a knowledgeable cell biologist that has professional experience writing and curating accurate and informative descriptions of cell types.\n",
    "\n",
    "User role:\n",
    "> I am making a knowledge-base about cell types. Each cell type is a term from the Cell Ontology and will have its own page with a detailed description of that cell type and its function. Please write me a description for \"{cell_type_name}\". Please return only the description and no other dialogue. The description should include information about the cell type's function. The description should be at least three paragraphs long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04d69e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_descs = {}#json.load(open('frontend/src/views/CellCards/common/fixtures/allCellTypeDescriptions.json','r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a4005d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "openai.organization = \"org-4kBCayJVUBGqH42cJhzZYQ6o\"\n",
    "openai.api_key = \"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\"\n",
    "openai.Model.list()\n",
    "\n",
    "def func(cname):\n",
    "    print(cname)\n",
    "    time.sleep(0.1)\n",
    "\n",
    "    try:\n",
    "        result = openai.ChatCompletion.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a knowledgeable cell biologist that has professional experience writing and curating accurate and informative descriptions of cell types.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"I am making a knowledge-base about cell types. Each cell type is a term from the Cell Ontology and will have its own page with a detailed description of that cell type and its function. Please write me a description for \\\"{cname}\\\". Please return only the description and no other dialogue. The description should include information about the cell type's function. The description should be at least three paragraphs long.\"},\n",
    "            ]\n",
    "        ) \n",
    "        message = result['choices'][0]['message']['content']\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        message = ''\n",
    "\n",
    "    return message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eb0a66c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_descs = all_cell_type_descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "79dc7e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_descs = {k: current_descs[k] for k in current_descs if current_descs[k]!=''}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "89fdbd84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "professional antigen presenting cell\n",
      "chromaffin cell\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import concurrent.futures\n",
    "\n",
    "all_cell_type_descriptions = {}\n",
    "z=0\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    # Start the load operations and mark each future with its URL\n",
    "    futures = {executor.submit(func, cell_type['label']): cell_type['id'] for cell_type in all_cell_types if cell_type['id'] not in current_descs}    \n",
    "\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        cid = futures[future]\n",
    "        \n",
    "        if z%10==0:\n",
    "            print(z)\n",
    "        z+=1\n",
    "        \n",
    "        try:\n",
    "            all_cell_type_descriptions[cid] = data = future.result()\n",
    "        except Exception as e:\n",
    "            pass\n",
    "        \n",
    "all_cell_type_descriptions.update(current_descs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3dc9f4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(all_cell_type_descriptions,open('build_graph_output/allCellTypeDescriptions.json','w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5747676",
   "metadata": {},
   "source": [
    "## Generate source data per cell type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c4ee30",
   "metadata": {},
   "outputs": [],
   "source": [
    "http://purl.org/ccf/ccf.owl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "787609a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: DEPLOYMENT_STAGE=test\n"
     ]
    }
   ],
   "source": [
    "%env DEPLOYMENT_STAGE=test\n",
    "\n",
    "def get_title_from_doi(doi):\n",
    "    url = f\"https://api.crossref.org/works/{doi}\"\n",
    "\n",
    "    # Send a GET request to the API\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # If the GET request is successful, the status code will be 200\n",
    "    if response.status_code == 200:\n",
    "        # Get the response data\n",
    "        data = response.json()\n",
    "\n",
    "        # Get the title and citation count from the data\n",
    "        try:\n",
    "            title = data['message']['title'][0]\n",
    "        except:\n",
    "            try:\n",
    "                title = data['message']['items'][0]['title'][0]\n",
    "            except:\n",
    "                return doi\n",
    "        return title\n",
    "    else:\n",
    "        return doi\n",
    "    \n",
    "def format_citation(metadata):\n",
    "    first_author = metadata['publisher_metadata']['authors'][0]\n",
    "    if \"family\" in first_author:\n",
    "        author_str = f\"{first_author['family']}, {first_author['given']} et al.\"\n",
    "    else:\n",
    "        author_str = f\"{first_author['name']} et al.\"\n",
    "    \n",
    "    journal = metadata['publisher_metadata']['journal']\n",
    "    year = metadata['publisher_metadata']['published_year']\n",
    "    \n",
    "    return f\"{author_str} ({year}) {journal}\"\n",
    "\n",
    "snapshot.build_dataset_metadata_dict()\n",
    "\n",
    "datasets = get_datasets_from_curation_api()\n",
    "collections = get_collections_from_curation_api()\n",
    "\n",
    "collections_dict = {collection['collection_id']: collection for collection in collections}\n",
    "datasets_dict = {dataset['dataset_id']: dataset for dataset in datasets}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "40d0a686",
   "metadata": {},
   "outputs": [],
   "source": [
    "cts = [i['id'] for i in all_cell_types]\n",
    "\n",
    "DATA = {}\n",
    "\n",
    "for i in cts:\n",
    "    seen_datasets = []\n",
    "    lineage =_descendants(i)    \n",
    "    assert i in lineage\n",
    "    datasets=[]\n",
    "    for organism in snapshot.primary_filter_dimensions['tissue_terms']:    \n",
    "        criteria = WmgFiltersQueryCriteria(organism_ontology_term_id=organism,\n",
    "                                           cell_type_ontology_term_ids=lineage)\n",
    "        res = build_filter_dims_values(criteria, snapshot)\n",
    "        data = res['datasets']\n",
    "        for datum in data:\n",
    "            if datum['id'] not in seen_datasets and datum['collection_id']!='':\n",
    "                seen_datasets.append(datum['id'])\n",
    "                datasets.append(datum)\n",
    "\n",
    "    collections_to_datasets = {}\n",
    "    for dataset in datasets:\n",
    "        dataset = datasets_dict[dataset['id']]\n",
    "        \n",
    "        a = collections_to_datasets.get(dataset['collection_id'],{})\n",
    "\n",
    "        a['collection_name'] = collections_dict[dataset['collection_id']]['name']\n",
    "        a['collection_url'] = collections_dict[dataset['collection_id']]['collection_url']\n",
    "        a['publication_url'] = collections_dict[dataset['collection_id']]['doi']\n",
    "        if collections_dict[dataset['collection_id']]['publisher_metadata']:\n",
    "            a['publication_title'] = format_citation(collections_dict[dataset['collection_id']])\n",
    "        else:\n",
    "            a['publication_title'] = \"Publication\"\n",
    "            \n",
    "        tissues = a.get(\"tissue\", [])\n",
    "        diseases = a.get(\"disease\", [])\n",
    "        organisms = a.get(\"organism\", [])\n",
    "        for tissue in dataset['tissue']:\n",
    "            if tissue['ontology_term_id'] not in [i['ontology_term_id'] for i in tissues]:\n",
    "                tissues.append(tissue)\n",
    "            \n",
    "        for disease in dataset['disease']:\n",
    "            if disease['ontology_term_id'] not in [i['ontology_term_id'] for i in diseases]:\n",
    "                diseases.append(disease)\n",
    "            \n",
    "        for organism in dataset['organism']:\n",
    "            if organism['ontology_term_id'] not in [i['ontology_term_id'] for i in organisms]:\n",
    "                organisms.append(organism)\n",
    "            \n",
    "        a['tissue'] = tissues\n",
    "        a['disease'] = diseases\n",
    "        a['organism'] = organisms\n",
    "    \n",
    "        collections_to_datasets[dataset['collection_id']]=a\n",
    "\n",
    "    DATA[i] = list(collections_to_datasets.values())                                 \n",
    "json.dump(DATA,open('build_graph_output/allSourceData.json','w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069bef8e",
   "metadata": {},
   "source": [
    "## Generate enriched genes and expression metrics per cell type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "369a3c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from numba import njit, prange\n",
    "import numpy as np\n",
    "\n",
    "@njit(parallel=True)\n",
    "def nanpercentile_2d(arr, percentile, axis):\n",
    "    \"\"\"\n",
    "    Calculate the specified percentile of a 2D array along an axis, ignoring NaN values.\n",
    "\n",
    "    Parameters:\n",
    "        arr: 2D array to calculate percentile of\n",
    "        percentile: percentile to calculate, as a number between 0 and 100\n",
    "        axis: axis along which to calculate percentile\n",
    "\n",
    "    Returns:\n",
    "        The specified percentile of the 2D array along the specified axis.\n",
    "    \"\"\"\n",
    "    if axis == 0:\n",
    "        result = np.empty(arr.shape[1])\n",
    "        for i in prange(arr.shape[1]):\n",
    "            arr_column = arr[:, i]\n",
    "            result[i] = nanpercentile(arr_column, percentile)\n",
    "        return result\n",
    "    else:\n",
    "        result = np.empty(arr.shape[0])\n",
    "        for i in prange(arr.shape[0]):\n",
    "            arr_row = arr[i, :]\n",
    "            result[i] = nanpercentile(arr_row, percentile)\n",
    "        return result\n",
    "\n",
    "@njit\n",
    "def nanpercentile(arr, percentile):\n",
    "    arr_without_nan = arr[np.logical_not(np.isnan(arr))]\n",
    "    length = len(arr_without_nan)\n",
    "\n",
    "    if length == 0:\n",
    "        return np.nan\n",
    "\n",
    "    return np.percentile(arr_without_nan,percentile)\n",
    "\n",
    "def _run_ttest(sum1, sumsq1, n1, sum2, sumsq2, n2):\n",
    "    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "        mean1 = sum1 / n1\n",
    "        meansq1 = sumsq1 / n1\n",
    "\n",
    "        mean2 = sum2 / n2\n",
    "        meansq2 = sumsq2 / n2\n",
    "\n",
    "        var1 = meansq1 - mean1**2\n",
    "        var1[var1 < 0] = 0\n",
    "        var2 = meansq2 - mean2**2\n",
    "        var2[var2 < 0] = 0\n",
    "\n",
    "        var1_n = var1 / n1\n",
    "        var2_n = var2 / n2\n",
    "        sum_var_n = var1_n + var2_n\n",
    "        dof = sum_var_n**2 / (var1_n**2 / (n1 - 1) + var2_n**2 / (n2 - 1))\n",
    "        tscores = (mean1 - mean2) / np.sqrt(sum_var_n)\n",
    "        effects = (mean1 - mean2) / np.sqrt(((n1 - 1) * var1 + (n2 - 1) * var2) / (n1 + n2 - 1))\n",
    "\n",
    "    pvals = stats.t.sf(tscores, dof)\n",
    "    return pvals, effects\n",
    "\n",
    "def _post_process_stats(\n",
    "    cell_type_target,\n",
    "    cell_types_context,\n",
    "    genes,\n",
    "    pvals,\n",
    "    effects,\n",
    "    percentile=0.05\n",
    "):\n",
    "    is_colinear = np.array(\n",
    "        [are_cell_types_colinear(cell_type, cell_type_target) for cell_type in cell_types_context]\n",
    "    )\n",
    "    effects[is_colinear] = np.nan\n",
    "    pvals[is_colinear] = np.nan\n",
    "    \n",
    "    pvals[:,np.all(np.isnan(pvals),axis=0)]=1\n",
    "    effects[:,np.all(np.isnan(effects),axis=0)]=0\n",
    "\n",
    "    # aggregate\n",
    "    effects = nanpercentile_2d(effects, percentile * 100, 0)\n",
    "    \n",
    "    effects[effects==0]=np.nan\n",
    "    \n",
    "    # pvals = np.array([stats.combine_pvalues(x[np.invert(np.isnan(x))] + 1e-300)[-1] for x in pvals.T])\n",
    "    pvals = np.sort(pvals,axis=0)[int(np.round(0.05*pvals.shape[0]))]\n",
    "    \n",
    "    markers = np.array(genes)[np.argsort(-effects)]\n",
    "    p = pvals[np.argsort(-effects)]\n",
    "    effects = effects[np.argsort(-effects)]\n",
    "    \n",
    "    statistics = []\n",
    "    final_markers = []\n",
    "    for i in range(len(p)):\n",
    "        pi = p[i]\n",
    "        ei = effects[i]\n",
    "        if ei is not np.nan and pi is not np.nan:\n",
    "            statistics.append({f\"p_value\": pi, f\"effect_size\": ei})\n",
    "            final_markers.append(markers[i])\n",
    "    return dict(zip(list(final_markers), statistics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ee82873",
   "metadata": {},
   "outputs": [],
   "source": [
    "organism_id_to_name = {}\n",
    "[organism_id_to_name.update(i) for i in snapshot.primary_filter_dimensions['organism_terms']];\n",
    "\n",
    "cell_counts_df = snapshot.cell_counts_cube.df[:]\n",
    "cell_counts_df = cell_counts_df.groupby(['organism_ontology_term_id','cell_type_ontology_term_id']).sum(numeric_only=True)\n",
    "\n",
    "expressions_df = snapshot.expression_summary_fmg_cube.df[:]\n",
    "expressions_df = expressions_df.groupby(['organism_ontology_term_id','cell_type_ontology_term_id','gene_ontology_term_id']).sum(numeric_only=True)\n",
    "expressions_df = expressions_df.reset_index()\n",
    "expressions_df = expressions_df[expressions_df['cell_type_ontology_term_id'].isin(all_cell_type_ids)]\n",
    "\n",
    "all_cell_type_ids = [i['id'] for i in all_cell_types]\n",
    "cell_counts_df = cell_counts_df[cell_counts_df.index.get_level_values('cell_type_ontology_term_id').isin(all_cell_type_ids)]\n",
    "\n",
    "index = pd.Index(list(itertools.product(cell_counts_df.index.get_level_values('organism_ontology_term_id').unique(),all_cell_type_ids)))\n",
    "index = index.set_names(['organism_ontology_term_id','cell_type_ontology_term_id'])\n",
    "universe_cell_counts_df = pd.DataFrame(index = index)\n",
    "universe_cell_counts_df['n_cells']=0\n",
    "universe_cell_counts_df['n_cells'][cell_counts_df.index] = cell_counts_df['n_cells']\n",
    "\n",
    "universe_cell_counts_df = rollup_across_cell_type_descendants(\n",
    "    universe_cell_counts_df.reset_index()\n",
    ")\n",
    "universe_cell_counts_df=universe_cell_counts_df[universe_cell_counts_df['n_cells']>0]\n",
    "universe_cell_counts_df = universe_cell_counts_df.groupby(['organism_ontology_term_id','cell_type_ontology_term_id']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4b0bbe14",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = list(set(list(universe_cell_counts_df.index.get_level_values('cell_type_ontology_term_id'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "88db58a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cell_types = [c for c in all_cell_types if c['id'] in a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "52e75b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(all_cell_types,open('build_graph_output/allCellTypes.json','w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f707c6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = list(zip(*expressions_df[['organism_ontology_term_id','cell_type_ontology_term_id']].values.T))\n",
    "y = list(expressions_df['gene_ontology_term_id'])\n",
    "\n",
    "xu = list(set(x))\n",
    "yu = list(set(y))\n",
    "\n",
    "x_index = pd.Series(index=pd.Index(xu),data=np.arange(len(xu)))\n",
    "y_index = pd.Series(index=pd.Index(yu),data=np.arange(len(yu)))\n",
    "\n",
    "e_nnz = np.zeros((len(xu),len(yu)))\n",
    "e_sum = np.zeros((len(xu),len(yu)))\n",
    "e_sqsum = np.zeros((len(xu),len(yu)))\n",
    "\n",
    "e_nnz[x_index[x].values, y_index[y].values] = expressions_df['nnz']\n",
    "e_sum[x_index[x].values, y_index[y].values] = expressions_df['sum']\n",
    "e_sqsum[x_index[x].values, y_index[y].values] = expressions_df['sqsum']\n",
    "\n",
    "available_combinations = set(list(universe_cell_counts_df.index.values))\n",
    "missing_combinations = available_combinations.difference(xu)\n",
    "\n",
    "xu = xu + list(missing_combinations)\n",
    "x_index = pd.Series(index=pd.Index(xu),data=np.arange(len(xu)))\n",
    "\n",
    "e_nnz = np.vstack((e_nnz,np.zeros((len(missing_combinations),e_nnz.shape[1]))))\n",
    "e_sum = np.vstack((e_sum,np.zeros((len(missing_combinations),e_sum.shape[1]))))\n",
    "e_sqsum = np.vstack((e_sqsum,np.zeros((len(missing_combinations),e_sqsum.shape[1]))))\n",
    "\n",
    "organisms = x_index.index.get_level_values(0)\n",
    "organisms_u = list(set(organisms))\n",
    "cell_types = x_index.index.get_level_values(1)\n",
    "\n",
    "e_nnz_rollup = np.zeros_like(e_nnz)\n",
    "e_sum_rollup = np.zeros_like(e_sum)\n",
    "e_sqsum_rollup = np.zeros_like(e_sqsum)\n",
    "\n",
    "n_cells = universe_cell_counts_df['n_cells'][x_index.index]\n",
    "n_cells = np.tile(n_cells.values[:,None],(1,e_nnz_rollup.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "ab36e17c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n"
     ]
    }
   ],
   "source": [
    "all_results = []\n",
    "for organism in organisms_u:\n",
    "    cell_types_o = cell_types[organisms==organism]\n",
    "    e_nnz_o = e_nnz[organisms==organism]\n",
    "    e_sum_o = e_sum[organisms==organism]    \n",
    "    e_sqsum_o = e_sqsum[organisms==organism]        \n",
    "    n_cells_o = n_cells[organisms==organism]\n",
    "    \n",
    "    e_nnz_o = rollup_across_cell_type_descendants_array(e_nnz_o,cell_types_o)\n",
    "    e_sum_o = rollup_across_cell_type_descendants_array(e_sum_o,cell_types_o)  \n",
    "    e_sqsum_o = rollup_across_cell_type_descendants_array(e_sqsum_o,cell_types_o)  \n",
    "    \n",
    "    e_nnz_rollup[organisms==organism]=e_nnz_o\n",
    "    e_sum_rollup[organisms==organism]=e_sum_o\n",
    "    e_sqsum_rollup[organisms==organism]=e_sqsum_o \n",
    "    \n",
    "    i_range = np.arange(e_sum_o.shape[0])\n",
    "    for i in range(e_sum_o.shape[0]):\n",
    "        print(i)\n",
    "        sum1 = e_sum_o[i][None,:].copy()\n",
    "        sumsq1 = e_sqsum_o[i][None,:].copy()\n",
    "        n1 = n_cells_o[i][None,:].copy()\n",
    "\n",
    "\n",
    "        pvals, effects = _run_ttest(sum1,sumsq1,n1,\n",
    "                   e_sum_o, e_sqsum_o, n_cells_o)\n",
    "\n",
    "        pvals[i] = np.nan\n",
    "        effects[i] = np.nan\n",
    "     \n",
    "        res = _post_process_stats(\n",
    "            cell_types_o[i],\n",
    "            cell_types_o,\n",
    "            y_index.index.values,\n",
    "            pvals,\n",
    "            effects,\n",
    "            percentile=0.05\n",
    "        )\n",
    "\n",
    "        res2 = pd.DataFrame()\n",
    "        res2.index = pd.Index([k for k in res])\n",
    "        res2['p_value'] = [res[k]['p_value'] for k in res]\n",
    "        res2['effect_size'] = [res[k]['effect_size'] for k in res]\n",
    "        res = res2     \n",
    "\n",
    "        res['cell_type_ontology_term_id'] = cell_types_o[i]\n",
    "        res['organism_ontology_term_id'] = organism\n",
    "        res['gene_ontology_term_id'] = res.index\n",
    "        res = res.reset_index(drop=True)\n",
    "        res = res[res['effect_size'].notnull()]\n",
    "        res = res[res['effect_size']>0]        \n",
    "        all_results.append(res)\n",
    "        \n",
    "x_new,y_new = (e_nnz_rollup+e_sum_rollup).nonzero()\n",
    "\n",
    "r_x_index = pd.Series(index=x_index.values,data=x_index.index.values)\n",
    "r_y_index = pd.Series(index=y_index.values,data=y_index.index.values)\n",
    "\n",
    "y_r_new = r_y_index[y_new].values\n",
    "\n",
    "x_r_new = r_x_index[x_new].values\n",
    "\n",
    "new_index = pd.Index([i+(j,) for i,j in zip(x_r_new,y_r_new)])\n",
    "\n",
    "nnz_flat = e_nnz_rollup[x_new,y_new]\n",
    "sum_flat = e_sum_rollup[x_new,y_new]\n",
    "\n",
    "new_index = new_index.set_names(['organism_ontology_term_id','cell_type_ontology_term_id','gene_ontology_term_id'])\n",
    "new_expression_rollup = pd.DataFrame(index=new_index)\n",
    "\n",
    "new_expression_rollup['nnz']=nnz_flat\n",
    "new_expression_rollup['sum']=sum_flat\n",
    "\n",
    "new_expression_rollup=new_expression_rollup.reset_index()\n",
    "\n",
    "markers_df = pd.concat(all_results,axis=0)\n",
    "markers_df=markers_df[markers_df['p_value']<1e-5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "e9119eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dims = [\"organism_ontology_term_id\",\"cell_type_ontology_term_id\",\"gene_ontology_term_id\"]\n",
    "markers_df = markers_df.groupby(dims).first()\n",
    "new_expression_rollup = new_expression_rollup.groupby(dims).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "fd46a7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_expression_rollup = new_expression_rollup.join(markers_df,on=dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "cb6d0edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_expression_rollup = new_expression_rollup.reset_index()\n",
    "markers_df = markers_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8933bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_per_group = markers_df.groupby(['organism_ontology_term_id','cell_type_ontology_term_id']).apply(lambda x: x.nlargest(100, 'effect_size'))\n",
    "\n",
    "columns = ['organism_ontology_term_id','cell_type_ontology_term_id','gene_ontology_term_id']\n",
    "o_ct_genes = list(zip(*top_per_group[columns].values.T))\n",
    "\n",
    "\n",
    "filt1 = new_expression_rollup['cell_type_ontology_term_id'].isin(top_per_group['cell_type_ontology_term_id'].unique())\n",
    "\n",
    "filt2 = new_expression_rollup['gene_ontology_term_id'].isin(top_per_group['gene_ontology_term_id'].unique())\n",
    "\n",
    "filt = np.logical_and(filt1,filt2)\n",
    "\n",
    "new_expression_rollup = new_expression_rollup[filt]\n",
    "\n",
    "new_expression_rollup.index = pd.Index(list(zip(*new_expression_rollup[['organism_ontology_term_id','cell_type_ontology_term_id','gene_ontology_term_id']].values.T)))\n",
    "\n",
    "universe_cell_counts_df = universe_cell_counts_df.groupby(['organism_ontology_term_id','cell_type_ontology_term_id']).sum()['n_cells']\n",
    "\n",
    "gene_id_to_symbol={}\n",
    "all_genes = []\n",
    "for k in snapshot.primary_filter_dimensions['gene_terms']:\n",
    "    for i in snapshot.primary_filter_dimensions['gene_terms'][k]:\n",
    "        gene_id_to_symbol.update(i)\n",
    "        all_genes.append(list(i.keys())[0])\n",
    "\n",
    "gene_id_to_name = pd.read_csv('ensembl_gene_ids_to_descriptions.tsv.gz',sep='\\t')\n",
    "\n",
    "gene_id_to_name = gene_id_to_name.set_index('Ensembl GeneIDs')['Description'].to_dict()\n",
    "\n",
    "data={}\n",
    "for i in o_ct_genes:\n",
    "    o,ct,gene = i\n",
    "    \n",
    "    nnz = new_expression_rollup['nnz'][i]\n",
    "    s = new_expression_rollup['sum'][i]\n",
    "    es = new_expression_rollup['effect_size'][i]\n",
    "    n_cells = universe_cell_counts_df[(o,ct)]\n",
    "    \n",
    "    a = data.get(ct,[])\n",
    "    a.append({\n",
    "        'me': s/nnz if nnz > 0 else 0,\n",
    "        'pc': nnz/n_cells,\n",
    "        'marker_score': es,\n",
    "        'symbol': gene_id_to_symbol[gene],\n",
    "        'name': gene_id_to_name.get(gene,gene_id_to_symbol[gene]),\n",
    "        'organism': organism_id_to_name[o]\n",
    "    })\n",
    "    data[ct]=a\n",
    "\n",
    "\n",
    "json.dump(data,open('build_graph_output/allEnrichedGenes.json','w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c786ce0",
   "metadata": {},
   "source": [
    "## Generate canonical marker genes data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021f8a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_165928/2107434810.py:19: DtypeWarning: Columns (36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,57,66,67,68) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  return pd.read_csv(strio,skiprows=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ".md\n",
      "brain\n"
     ]
    }
   ],
   "source": [
    "from io import StringIO\n",
    "def get_asctb_file(file):\n",
    "    version=4\n",
    "    succeeded=False\n",
    "    while not succeeded and version > 0:\n",
    "        a = requests.get(f\"https://hubmapconsortium.github.io/ccf-releases/v1.{version}/markdown/{file}\")\n",
    "        if a.status_code == 200:\n",
    "            succeeded=True\n",
    "        else:\n",
    "            version -= 1\n",
    "            \n",
    "    if a.status_code != 200:\n",
    "        return None\n",
    "    \n",
    "    csv_file = 'https://'+a.content.decode().split('Data Table:** |[')[-1].split('.csv')[0].split('https://')[-1]+'.csv'\n",
    "    a = requests.get(csv_file)\n",
    "    strio = StringIO(a.content.decode())\n",
    "    return pd.read_csv(strio,skiprows=10)\n",
    "\n",
    "def get_all_prefix_cols(prefix, cols):\n",
    "    i=1\n",
    "    prefix_cols = []\n",
    "    while True:\n",
    "        col = f\"{prefix}{i}\"\n",
    "        if col in cols:\n",
    "            prefix_cols.append(col)\n",
    "            i+=1\n",
    "        elif col.upper() in cols:\n",
    "            prefix_cols.append(col.upper())\n",
    "            i+=1            \n",
    "        elif col.lower() in cols:\n",
    "            prefix_cols.append(col.lower())\n",
    "            i+=1\n",
    "        else:\n",
    "            break   \n",
    "    return prefix_cols\n",
    "\n",
    "def get_all_suffix_cols(prefix,suffix, cols):\n",
    "    i=1\n",
    "    suffix_cols = []\n",
    "    while True:\n",
    "        col = f\"{prefix}{i}{suffix}\"\n",
    "        if col in cols:\n",
    "            suffix_cols.append(col)\n",
    "            i+=1\n",
    "        elif col.upper() in cols:\n",
    "            suffix_cols.append(col.upper())\n",
    "            i+=1            \n",
    "        elif col.lower() in cols:\n",
    "            suffix_cols.append(col.lower())\n",
    "            i+=1\n",
    "        else:\n",
    "            break    \n",
    "    return suffix_cols\n",
    "\n",
    "def get_gene_name(gene):\n",
    "    a = requests.get(f\"https://api.cellxgene.dev.single-cell.czi.technology/gene_info/v1/gene_info?gene={gene}\")\n",
    "    if a.status_code == 200:\n",
    "        r = a.json()\n",
    "        return r['name']\n",
    "    else:\n",
    "        return gene\n",
    "    \n",
    "def try_delete(d, k):\n",
    "    try:\n",
    "        del d[k]\n",
    "    except:\n",
    "        try:\n",
    "            del d[k[0]+k[1:].lower()]\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "def get_title_and_citation_from_doi(doi):\n",
    "    url = f\"https://api.crossref.org/works/{doi}\"\n",
    "\n",
    "    # Send a GET request to the API\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # If the GET request is successful, the status code will be 200\n",
    "    if response.status_code == 200:\n",
    "        # Get the response data\n",
    "        data = response.json()\n",
    "\n",
    "        # Get the title and citation count from the data\n",
    "        try:\n",
    "            title = data['message']['title'][0]\n",
    "            citation = format_citation_mg(data['message'])\n",
    "        except:\n",
    "            try:\n",
    "                title = data['message']['items'][0]['title'][0]\n",
    "                citation = format_citation_mg(data['message']['items'][0])                \n",
    "            except:\n",
    "                return doi\n",
    "        return f\"{title}\\n\\n - {citation}\"\n",
    "    else:\n",
    "        return doi\n",
    "    \n",
    "def format_citation_mg(message):\n",
    "    first_author = message['author'][0]\n",
    "    if \"family\" in first_author:\n",
    "        author_str = f\"{first_author['family']}, {first_author['given']} et al.\"\n",
    "    else:\n",
    "        author_str = f\"{first_author['name']} et al.\"\n",
    "    \n",
    "    journal = message['container-title'][0]\n",
    "    year = message['created']['date-parts'][0][0]\n",
    "    \n",
    "    return f\"{author_str} ({year}) {journal}\"\n",
    "\n",
    "def get_tissue_name(t):\n",
    "    t=t.replace(':','_')\n",
    "    urls = [\n",
    "        f\"https://www.ebi.ac.uk/ols4/api/ontologies/clo/terms/http%253A%252F%252Fpurl.obolibrary.org%252Fobo%252F{t}\",\n",
    "        f\"https://www.ebi.ac.uk/ols4/api/ontologies/envo/terms/http%253A%252F%252Fpurl.obolibrary.org%252Fobo%252F{t}\",\n",
    "        f\"https://www.ebi.ac.uk/ols4/api/ontologies/flopo/terms/http%253A%252F%252Fpurl.obolibrary.org%252Fobo%252F{t}\",\n",
    "        f\"https://www.ebi.ac.uk/ols4/api/ontologies/doid/terms/http%253A%252F%252Fpurl.obolibrary.org%252Fobo%252F{t}\",\n",
    "    ]\n",
    "    for url in urls:    \n",
    "        response = requests.get(url)\n",
    "        if response.status_code==200:\n",
    "            r = response.json()\n",
    "            return r['label']\n",
    "    return t\n",
    "\n",
    "pf = json.load(open('prod-snapshot/primary_filter_dimensions.json','r'))\n",
    "all_human_genes = [list(i.values())[0] for i in pf['gene_terms']['NCBITaxon:9606']]\n",
    "\n",
    "X = tiledb.open('prod-snapshot/cell_counts/')\n",
    "cc = X.df[:]\n",
    "tissue_original = list(set(cc['tissue_original_ontology_term_id']))\n",
    "tissue = list(set(cc['tissue_ontology_term_id']))\n",
    "\n",
    "m = {}\n",
    "[m.update(i) for i in pf['tissue_terms']['NCBITaxon:9606']];\n",
    "for i in tissue:\n",
    "    if i not in m:\n",
    "        m[i]=i\n",
    "        \n",
    "for i in tissue_original:\n",
    "    if i not in m:\n",
    "        m[i]=i\n",
    "\n",
    "\n",
    "a = requests.get('https://hubmapconsortium.github.io/ccf-releases/v1.4/docs/index.html')\n",
    "content = a.content.decode()\n",
    "start=1\n",
    "files = []\n",
    "while start!=0:\n",
    "    i = content.find('href=\"asct-b/',start)\n",
    "    files.append(content[i:].split('.html')[0].split('href=\"')[-1]+\".md\")\n",
    "    start=i+1\n",
    "    \n",
    "\n",
    "\n",
    "dfs = [get_asctb_file(f) for f in files]\n",
    "dfs = [i for i in dfs if i is not None]\n",
    "parsed_table_entries = []\n",
    "\n",
    "seen=[]\n",
    "for df in dfs:\n",
    "    print(df.iloc[0,0])\n",
    "    \n",
    "    assert df.columns[0]=='AS/1'\n",
    "\n",
    "    cols = list(df.columns)\n",
    "\n",
    "    ref_prefix = \"Ref/\"\n",
    "    ref_doi_suffix = \"/DOI\"\n",
    "\n",
    "    ref_prefixes = get_all_prefix_cols(ref_prefix,cols)\n",
    "    if len(ref_prefixes):\n",
    "        prefix=ref_prefixes[0].split('/')[0]+\"/\"\n",
    "        ref_suffixes=get_all_suffix_cols(prefix,ref_doi_suffix,cols)\n",
    "        ref_suffixes_notes=get_all_suffix_cols(prefix,\"/NOTES\",cols)\n",
    "    else:\n",
    "        ref_suffixes=[]\n",
    "        ref_suffixes_notes=[]\n",
    "\n",
    "\n",
    "    gene_prefix = \"BGene/\"\n",
    "    gene_label_suffix = \"/LABEL\"\n",
    "\n",
    "    gene_prefixes = get_all_prefix_cols(gene_prefix,cols)\n",
    "    if len(gene_prefixes):\n",
    "        prefix=gene_prefixes[0].split('/')[0]+\"/\"\n",
    "        gene_suffixes=get_all_suffix_cols(prefix,gene_label_suffix,cols)\n",
    "    else:\n",
    "        gene_suffixes=[]\n",
    "\n",
    "    tissue_prefix = \"AS/\"\n",
    "    protein_label_suffix = \"/ID\"\n",
    "\n",
    "    tissue_prefixes = get_all_prefix_cols(tissue_prefix,cols)\n",
    "    if len(tissue_prefixes):\n",
    "        prefix=tissue_prefixes[0].split('/')[0]+\"/\"\n",
    "        tissue_suffixes=get_all_suffix_cols(prefix,protein_label_suffix,cols) \n",
    "    else:\n",
    "        tissue_suffixes=[]\n",
    "\n",
    "    ct = \"CT/1\"\n",
    "    ctid = \"CT/1/ID\"\n",
    "    try:\n",
    "        assert len(ref_prefixes) > 0\n",
    "        assert len(ref_suffixes) > 0\n",
    "        assert len(gene_prefixes) > 0 \n",
    "        assert len(gene_suffixes) > 0 \n",
    "        assert len(tissue_suffixes) > 0\n",
    "    except:\n",
    "        print(\"Skipping\")\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    for n in range(df.shape[0]):\n",
    "        res = df.iloc[n].to_dict()\n",
    "        data_tmp = {i: res[i] for i in res if i in [ct, ctid] + ref_prefixes+ref_suffixes+gene_prefixes+gene_suffixes+tissue_suffixes+ref_suffixes_notes}\n",
    "        data = data_tmp.copy()\n",
    "        genes = []\n",
    "        gene_to_key = {}\n",
    "        for i in gene_prefixes:\n",
    "            data_tmp[i] = str(data_tmp[i]).split(' ')[0]\n",
    "            if data_tmp[i].upper() not in all_human_genes or data_tmp[i]=='nan':\n",
    "                try_delete(data,i)\n",
    "                try_delete(data,i+'/LABEL')\n",
    "            else:\n",
    "                data[i] = data_tmp[i].upper()\n",
    "                genes.append(data[i])\n",
    "                gene_to_key[data[i]] = i\n",
    "\n",
    "        valid_ref_accessors = []\n",
    "        for i in ref_suffixes:\n",
    "            i_prefix = '/'.join(i.split('/')[:-1])\n",
    "            \n",
    "            if str(data[i_prefix +'/DOI'])=='nan' or str(data[i_prefix +'/DOI'])=='No DOI':\n",
    "                try_delete(data,i)\n",
    "                try_delete(data,i_prefix)\n",
    "                try_delete(data,i_prefix+'/NOTES')\n",
    "            else:\n",
    "                data[i_prefix+'/DOI'] = data[i_prefix+'/DOI'].split(' ')[-1]\n",
    "                try_delete(data,i_prefix)\n",
    "                try_delete(data,i_prefix+'/NOTES')                \n",
    "                valid_ref_accessors.append(i_prefix)\n",
    "\n",
    "        refs = []\n",
    "        titles = []\n",
    "        for i in valid_ref_accessors:\n",
    "            doi = data[i+'/DOI']\n",
    "            if doi != \"\":\n",
    "                if doi[-1] == \".\":\n",
    "                    doi = doi[:-1]\n",
    "\n",
    "            title = get_title_and_citation_from_doi(doi)\n",
    "            refs.append(doi)\n",
    "            titles.append(title)\n",
    "            \n",
    "        refs = ';;'.join(refs)\n",
    "        titles = ';;'.join(titles)\n",
    "            \n",
    "        if not str(data[ctid]).startswith('CL:'):         \n",
    "            continue\n",
    "        \n",
    "        tissue_general = None\n",
    "        for i in tissue_suffixes[::-1]:\n",
    "            if data[i] in tissue:\n",
    "                tissue_general = data[i]\n",
    "                break\n",
    "\n",
    "        tissue_specific = None\n",
    "        for i in tissue_suffixes[::-1]:\n",
    "            if data[i] in tissue_original:\n",
    "                tissue_specific = data[i]\n",
    "                break\n",
    "\n",
    "        if tissue_general is None:\n",
    "            for i in tissue_suffixes:\n",
    "                if data[i].startswith(\"UBERON:\"):\n",
    "                    tissue_general=data[i]\n",
    "                    break\n",
    "                    \n",
    "        if tissue_specific is None:\n",
    "            for i in tissue_suffixes:\n",
    "                if data[i].startswith(\"UBERON:\"):\n",
    "                    tissue_specific=data[i]                    \n",
    "                    break\n",
    "                    \n",
    "        assert tissue_general is not None\n",
    "        assert tissue_specific is not None\n",
    "\n",
    "        for gene in genes:\n",
    "            label = str(data[gene_to_key[gene]+'/LABEL'])\n",
    "            if gene == label.upper() or label == 'nan':\n",
    "                label = get_gene_name(gene)\n",
    "\n",
    "\n",
    "            gene_dict = {\n",
    "                \"tissue_general\": tissue_general,\n",
    "                \"tissue_specific\": tissue_specific,\n",
    "                \"symbol\": gene,\n",
    "                \"name\": label,\n",
    "                \"publication\": refs,\n",
    "                \"publication_titles\": titles,\n",
    "                \"cell_type_ontology_term_id\": data[ctid]\n",
    "            }\n",
    "            hashed_dict = hash(json.dumps(gene_dict))\n",
    "            if hashed_dict not in seen:\n",
    "                parsed_table_entries.append(gene_dict)\n",
    "                seen.append(hashed_dict)\n",
    "                \n",
    "\n",
    "ts = list(set([i['tissue_general'] for i in parsed_table_entries]+[i['tissue_specific'] for i in parsed_table_entries]))\n",
    "\n",
    "tissues_by_id = {t: get_tissue_name(t) for t in ts}\n",
    "\n",
    "gene_infos = {}\n",
    "for entry in parsed_table_entries:\n",
    "    entry = entry.copy()\n",
    "    ct = entry['cell_type_ontology_term_id']\n",
    "    del entry['cell_type_ontology_term_id']\n",
    "    \n",
    "    a = gene_infos.get(ct,[])\n",
    "    entry['tissue_general'] = tissues_by_id.get(entry['tissue_general'],entry['tissue_general'])\n",
    "    entry['tissue_specific'] =tissues_by_id.get(entry['tissue_specific'],entry['tissue_specific'])\n",
    "    a.append(entry)\n",
    "    gene_infos[ct]=a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c79a2f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def func1(x):\n",
    "    y = [y for y in x.values if y != '']\n",
    "    z = []\n",
    "    for i in y:\n",
    "        if i not in z:\n",
    "            z.append(i)\n",
    "    res = ';;'.join(z)\n",
    "    return res\n",
    "\n",
    "def func2(x):\n",
    "    res = x.values\n",
    "    res2=[i not in gi2['symbol'].values for i in res]\n",
    "    index = 0\n",
    "    try:\n",
    "        index = res2.index(True)\n",
    "    except:\n",
    "        pass\n",
    "    return res[index]\n",
    "\n",
    "for key in gene_infos:\n",
    "    gi = pd.DataFrame(gene_infos[key])\n",
    "    gi2 = pd.DataFrame(gi)\n",
    "    \n",
    "    gi = gi.groupby([\"tissue_general\",\"symbol\"]).agg({'name': func2,'publication': func1, 'publication_titles': func1}).reset_index().to_dict(orient=\"records\")\n",
    "    \n",
    "    gi2 = pd.DataFrame(gi)\n",
    "    gi2['n']=1\n",
    "    gi3 = gi2.groupby(['symbol','tissue_general']).sum(numeric_only=True)['n']\n",
    "    valid_genes = list(set(gi3.index[gi3>0].get_level_values('symbol')))\n",
    "    gi2 = pd.DataFrame(gi)\n",
    "\n",
    "    gi2 = gi2[gi2['symbol'].isin(valid_genes)]\n",
    "    gi3 = gi2.groupby('symbol').agg({'name': func2,'publication': func1, 'publication_titles': func1})\n",
    "    gi3 = gi3.reset_index()\n",
    "    gi3['tissue_general']='All Tissues'\n",
    "    gi3['tissue_specific']='All Tissues'\n",
    "    gi3 = gi3[gi2.columns]\n",
    "    gi.extend(gi3.to_dict(orient=\"records\"))\n",
    "    gene_infos[key]=gi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9cd87db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(gene_infos,open('frontend/src/views/CellCards/common/fixtures/allCellTypeMarkerGenes.json','w'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
