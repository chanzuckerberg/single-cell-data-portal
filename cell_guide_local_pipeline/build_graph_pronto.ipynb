{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee85d2bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/atarashansky/czi/single-cell-data-portal\n"
     ]
    }
   ],
   "source": [
    "cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44f3e308",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "* Owlready2 * Warning: optimized Cython parser module 'owlready2_optimized' is not available, defaulting to slower Python implementation\n"
     ]
    }
   ],
   "source": [
    "import cellxgene_census\n",
    "from backend.wmg.data.rollup import rollup_across_cell_type_descendants\n",
    "import json\n",
    "import tiledb\n",
    "from backend.wmg.data.ontology_labels import ontology_term_label, ontology_term_id_labels\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from backend.wmg.pipeline.integrated_corpus.transform import get_high_level_tissue\n",
    "from pronto import Ontology\n",
    "\n",
    "def traverse_with_counting(node):\n",
    "    global traverse_node_counter\n",
    "    global all_unique_nodes\n",
    "    node_count = traverse_node_counter.get(node.id, 0)\n",
    "    traverse_node_counter[node.id] = node_count + 1\n",
    "    all_unique_nodes.add(node.id +\"__\"+str(node_count))\n",
    "    \n",
    "    subclasses = list(node.subclasses(with_self=False, distance=1))\n",
    "\n",
    "    if len(subclasses) == 0:\n",
    "        return {\"id\": node.id+\"__\"+str(node_count),\n",
    "                \"name\": id_to_name[node.id] if node.id in id_to_name else node.id,\n",
    "                \"n_cells_rollup\": int(cell_counts_df_rollup[node.id] if node.id in cell_counts_df_rollup else 0),\n",
    "                \"n_cells\": int(cell_counts_df[node.id] if node.id in cell_counts_df else 0),\n",
    "               }\n",
    "        \n",
    "    children = []\n",
    "    for child in subclasses:\n",
    "        children.append(traverse_with_counting(child))\n",
    "\n",
    "    return {\"id\": node.id+\"__\"+str(node_count),\n",
    "                \"name\": id_to_name[node.id] if node.id in id_to_name else node.id,\n",
    "                \"n_cells_rollup\": int(cell_counts_df_rollup[node.id] if node.id in cell_counts_df_rollup else 0),\n",
    "                \"n_cells\": int(cell_counts_df[node.id] if node.id in cell_counts_df else 0),\n",
    "                \"children\": children,\n",
    "               }\n",
    "\n",
    "\n",
    "def dfs(parents, end, start, node=None, path = None, all_paths = []):\n",
    "    if path is None and node is None:\n",
    "        path = [end]\n",
    "        node = end\n",
    "\n",
    "    if node == start:\n",
    "        return path\n",
    "    \n",
    "    for parent in parents.get(node,[]):\n",
    "        full_path = dfs(parents, end, start, node=parent, path = path+[parent], all_paths=all_paths)\n",
    "        if full_path:\n",
    "            all_paths.append(full_path)\n",
    "            \n",
    "def truncate_graph(graph,valid_nodes):   \n",
    "    if graph['id'] not in valid_nodes:\n",
    "        return False\n",
    "\n",
    "    children= graph.get(\"children\",[])\n",
    "    valid_children = []\n",
    "    append_dummy = False\n",
    "    \n",
    "    invalid_children_ids = []\n",
    "    for child in children:\n",
    "        is_valid = truncate_graph(child, valid_nodes)\n",
    "        if is_valid:\n",
    "            valid_children.append(child)\n",
    "        elif child['id']!='':\n",
    "            invalid_children_ids.append(child['id'])\n",
    "            append_dummy = True\n",
    "\n",
    "    if append_dummy and len(valid_children) > 0:\n",
    "        valid_children.append(\n",
    "            {\"id\": \"\",\n",
    "            \"name\": \"\",\n",
    "            \"n_cells_rollup\": 0,\n",
    "            \"n_cells\": 0,\n",
    "             \"invalid_children_ids\": invalid_children_ids,\n",
    "            \"parent\": graph['id']\n",
    "            }        \n",
    "        )\n",
    "    if len(valid_children) > 0:\n",
    "        graph['children'] = valid_children\n",
    "    else:\n",
    "        if 'children' in graph:\n",
    "            del graph['children']\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "def truncate_graph_per_tissue(graph, valid_nodes, total_count, tissue_cell_counts, depth=0):\n",
    "    global seen_nodes_per_tissue\n",
    "    \n",
    "    children = graph.get('children',[])\n",
    "    if len(children):\n",
    "        new_children = []\n",
    "        invalid_children_ids = []\n",
    "        for child in children:\n",
    "            outlier_branch = depth == 1 and (tissue_cell_counts.get(child['id'].split('__')[0],{'n_cells_rollup': 0})['n_cells_rollup'] / total_count * 100) < 0.1\n",
    "            if child['id'] in valid_nodes and child['id'] not in seen_nodes_per_tissue and not outlier_branch:\n",
    "                new_children.append(child)\n",
    "                seen_nodes_per_tissue.add(child['id'])\n",
    "            else:\n",
    "                invalid_children_ids.append(child['id'])\n",
    "        if len(new_children) == 0:\n",
    "            del graph['children']\n",
    "        elif len(invalid_children_ids) > 0:\n",
    "            new_children.append(\n",
    "                {\"id\": \"\",\n",
    "                \"name\": \"\",\n",
    "                \"n_cells_rollup\": 0,\n",
    "                \"n_cells\": 0,\n",
    "                 \"invalid_children_ids\": invalid_children_ids,\n",
    "                \"parent\": graph['id']\n",
    "                }                \n",
    "            )\n",
    "            graph['children'] = new_children\n",
    "        else:\n",
    "            graph['children'] = new_children\n",
    "        \n",
    "        for child in graph.get('children',[]):\n",
    "            if child['id'] != '':\n",
    "                truncate_graph_per_tissue(child, valid_nodes, total_count, tissue_cell_counts, depth = depth+1)\n",
    "\n",
    "def truncate_graph2(graph, visited_nodes_in_paths):\n",
    "    # i want every node to only show children once\n",
    "    # this means deleting \"children\" if seen more than once\n",
    "    # EXCEPT if one of your children is in a path leading to acinar cell.\n",
    "    # Then, you collapse the remaining children\n",
    "    global nodesWithChildrenFound\n",
    "    if graph['id'].split(\"__\")[0] in nodesWithChildrenFound:\n",
    "        if 'children' in graph:\n",
    "            children = graph['children']            \n",
    "            new_children = []\n",
    "            invalid_children_ids = []\n",
    "            for child in children:\n",
    "                if child['id'] in visited_nodes_in_paths:\n",
    "                    new_children.append(child)\n",
    "                elif child['id'] != '':\n",
    "                    invalid_children_ids.append(child['id'])\n",
    "                    \n",
    "            if len(children) > len(new_children) and len(new_children) > 0:\n",
    "                # append dummy\n",
    "                new_children.append(\n",
    "                    {\"id\": \"\",\n",
    "                    \"name\": \"\",\n",
    "                    \"n_cells_rollup\": 0,\n",
    "                    \"n_cells\": 0,\n",
    "                     \"invalid_children_ids\": invalid_children_ids,\n",
    "                     \"parent\": graph['id']\n",
    "                    }        \n",
    "                )\n",
    "            if len(new_children) > 0:\n",
    "                graph['children'] = new_children\n",
    "            else:\n",
    "                del graph['children']\n",
    "    elif 'children' in graph:\n",
    "        nodesWithChildrenFound.add(graph['id'].split(\"__\")[0])\n",
    "    \n",
    "    \n",
    "    children = graph.get(\"children\",[])\n",
    "    for child in children:\n",
    "        if child['id'] != \"\":\n",
    "            truncate_graph2(child, visited_nodes_in_paths)    \n",
    "\n",
    "def prune_node_distinguishers(graph):\n",
    "    graph['id'] = graph['id'].split('__')[0]\n",
    "    for child in graph.get('children',[]):\n",
    "        prune_node_distinguishers(child)\n",
    "\n",
    "def delete_unknown_terms(graph):\n",
    "    new_children = []\n",
    "    for child in graph.get('children',[]):\n",
    "        unknown = child['name'].startswith('CL:')\n",
    "        if not unknown:\n",
    "            new_children.append(child)\n",
    "    if len(new_children) > 0:\n",
    "        graph['children'] = new_children\n",
    "    elif 'children' in graph:\n",
    "        del graph['children']\n",
    "    \n",
    "    for child in graph.get('children',[]):\n",
    "        delete_unknown_terms(child)\n",
    "        \n",
    "def truncate_graph_one_target(graph, target):\n",
    "    global targetFound\n",
    "    if targetFound and graph['id'].split(\"__\")[0] == target.split(\"__\")[0]:\n",
    "        del graph['children']\n",
    "    elif graph['id'] == target:\n",
    "        targetFound = True\n",
    "    \n",
    "    children = graph.get(\"children\",[])\n",
    "    for child in children:\n",
    "        truncate_graph_one_target(child, target)\n",
    "\n",
    "def build_children(graph):\n",
    "    global all_children\n",
    "    children = graph.get('children',[])\n",
    "    if len(children) == 0:\n",
    "        ids = []\n",
    "    else:\n",
    "        ids = [child['id'] for child in children]\n",
    "        \n",
    "    all_children[graph['id']] = ids\n",
    "    \n",
    "    for child in children:\n",
    "        build_children(child)\n",
    "\n",
    "def build_parents(graph):\n",
    "    global all_parents\n",
    "    children = graph.get('children',[])\n",
    "    \n",
    "    for child in children:\n",
    "        all_parents[child['id']]=[graph['id']]\n",
    "        build_parents(child)\n",
    "        \n",
    "def getExpandedData(graph):\n",
    "    global isExpandedNodes\n",
    "    if 'children' in graph:\n",
    "        isExpandedNodes.append(graph['id'])\n",
    "        for child in graph['children']:\n",
    "            getExpandedData(child)\n",
    "                \n",
    "        \n",
    "def getShownData(graph):\n",
    "    global notShownWhenExpandedNodes\n",
    "    \n",
    "    if 'children' in graph:\n",
    "        for child in graph['children']:\n",
    "            if child['id'] == \"\":\n",
    "                if len(child[\"invalid_children_ids\"]) > 0:\n",
    "                    notShownWhenExpandedNodes.append({child['parent']: list(set(child[\"invalid_children_ids\"]))})\n",
    "            else:\n",
    "                getShownData(child)\n",
    "        \n",
    "def _to_dict(a, b):\n",
    "    \"\"\"\n",
    "    convert a flat key array (a) and a value array (b) into a dictionary with values grouped by keys\n",
    "    \"\"\"\n",
    "    a = np.array(a)\n",
    "    b = np.array(b)\n",
    "    idx = np.argsort(a)\n",
    "    a = a[idx]\n",
    "    b = b[idx]\n",
    "    bounds = np.where(a[:-1] != a[1:])[0] + 1\n",
    "    bounds = np.append(np.append(0, bounds), a.size)\n",
    "    bounds_left = bounds[:-1]\n",
    "    bounds_right = bounds[1:]\n",
    "    slists = [b[bounds_left[i] : bounds_right[i]] for i in range(bounds_left.size)]\n",
    "    d = dict(zip(np.unique(a), [list(set(x)) for x in slists]))\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de757824",
   "metadata": {},
   "source": [
    "# Build ontology tree JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91ef73c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ontology = Ontology(\"https://github.com/obophenotype/cell-ontology/releases/latest/download/cl-basic.obo\")\n",
    "\n",
    "\n",
    "all_cell_types = []\n",
    "classes = [i for i in ontology if i.startswith('CL:')]\n",
    "all_cell_type_owl_descriptions = {}\n",
    "id_to_name = {}\n",
    "for c in classes :\n",
    "    c = ontology[c]\n",
    "    if not c.id.startswith(\"CL:\"):\n",
    "        continue\n",
    "    if c.obsolete :\n",
    "        continue\n",
    "    all_cell_types.append(\n",
    "    {\n",
    "        \"label\": c.name,\n",
    "        \"id\": c.id\n",
    "    }\n",
    "    )\n",
    "    id_to_name[c.id] = c.name\n",
    "    \n",
    "    all_cell_type_owl_descriptions[c.id] = str(c.definition) if str(c.definition) != 'None' else ''"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e606d699",
   "metadata": {},
   "source": [
    "INCLUDED_ASSAYS = {\n",
    "    \"EFO:0010550\": \"sci-RNA-seq\",\n",
    "    \"EFO:0009901\": \"10x 3' v1\",\n",
    "    \"EFO:0011025\": \"10x 5' v1\",\n",
    "    \"EFO:0009899\": \"10x 3' v2\",\n",
    "    \"EFO:0009900\": \"10x 5' v2\",\n",
    "    \"EFO:0009922\": \"10x 3' v3\",\n",
    "    \"EFO:0030003\": \"10x 3' transcription profiling\",\n",
    "    \"EFO:0030004\": \"10x 5' transcription profiling\",\n",
    "    \"EFO:0008919\": \"Seq-Well\",\n",
    "    \"EFO:0008995\": \"10x technology\",\n",
    "    \"EFO:0008722\": \"Drop-seq\",\n",
    "    \"EFO:0010010\": \"CEL-seq2\",\n",
    "}\n",
    "\n",
    "obs = census['census_data']['homo_sapiens']['obs'].read().concat().to_pandas()\n",
    "obs = obs[obs['assay_ontology_term_id'].isin(list(INCLUDED_ASSAYS.keys()))]\n",
    "obs = obs[obs['is_primary_data']]\n",
    "\n",
    "obs = get_high_level_tissue(obs)\n",
    "\n",
    "a2,b2=obs[['cell_type_ontology_term_id','tissue_ontology_term_id']].values.T\n",
    "\n",
    "uberon_by_celltype = _to_dict(b2,a2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8640a4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tiledb.open('prod-snapshot/cell_counts')\n",
    "cc = X.df[:]\n",
    "cell_counts_df = cc.groupby('cell_type_ontology_term_id').sum(numeric_only=True)[['n_cells']]\n",
    "uberon_by_celltype = _to_dict(cc['tissue_ontology_term_id'].values,cc['cell_type_ontology_term_id'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd79ef1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_counts_df=cell_counts_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d939914e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cell_types_ids = [i[\"id\"] for i in all_cell_types]\n",
    "to_attach = pd.DataFrame()\n",
    "to_attach['cell_type_ontology_term_id']=[i for i in all_cell_types_ids if i not in cell_counts_df['cell_type_ontology_term_id'].values]\n",
    "to_attach['n_cells']=0\n",
    "\n",
    "cell_counts_df = pd.concat([cell_counts_df,to_attach],axis=0)\n",
    "cell_counts_df_rollup = rollup_across_cell_type_descendants(cell_counts_df).set_index('cell_type_ontology_term_id')['n_cells']\n",
    "cell_counts_df = cell_counts_df.set_index('cell_type_ontology_term_id')['n_cells']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "50d7d9c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    }
   ],
   "source": [
    "root_node = ontology['CL:0000548']\n",
    "\n",
    "traverse_node_counter = {}\n",
    "all_unique_nodes = set()\n",
    "a = traverse_with_counting(root_node) \n",
    "all_unique_nodes = list(all_unique_nodes)\n",
    "print(max(traverse_node_counter.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d83b31b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_children={}\n",
    "all_parents={}    \n",
    "build_children(a)\n",
    "build_parents(a) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7ddd9f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n"
     ]
    }
   ],
   "source": [
    "start_node = 'CL:0000548__0'\n",
    "\n",
    "all_states_per_cell_type = {}\n",
    "for i,end_node in enumerate(all_cell_types_ids):\n",
    "    if i%100==0:\n",
    "        print(i)\n",
    "    if end_node in traverse_node_counter:\n",
    "        all_paths=[]\n",
    "        for i in range(traverse_node_counter[end_node]):\n",
    "            paths = []    \n",
    "            dfs(all_parents,end_node+\"__\"+str(i),start_node,all_paths=paths)\n",
    "            paths = [i[::-1] for i in paths] \n",
    "            if len(paths) == 0:\n",
    "                all_paths.append([end_node+\"__\"+str(i)])\n",
    "            else:\n",
    "                all_paths.append(paths[0])\n",
    "\n",
    "        ### RULES ###\n",
    "        # 1. We only want to show terms that are CHILDREN, GRANDCHILDREN, SIBLINGS OF TARGET, or IN A PATH TO TARGET\n",
    "        visited_nodes_in_paths = list(set(sum(all_paths,[])))\n",
    "\n",
    "        children1 = all_children.get(end_node+\"__0\",[]) #children\n",
    "        children2 = sum([all_children.get(child,[]) for child in children1],[]) #grandchildren\n",
    "        siblings=[]\n",
    "        for i in range(traverse_node_counter[end_node]):\n",
    "            sibs = sum([all_children.get(parent,[]) for parent in all_parents.get(end_node+\"__\"+str(i),[])],[]) #siblings\n",
    "            siblings.append(sibs)\n",
    "        siblings = list(set(sum(siblings,[])))\n",
    "\n",
    "\n",
    "        valid_nodes = list(set(visited_nodes_in_paths + children1 + children2 + siblings))\n",
    "\n",
    "        a_copy = json.loads(json.dumps(a))\n",
    "        truncate_graph(a_copy,valid_nodes) \n",
    "\n",
    "        nodesWithChildrenFound=set()\n",
    "        truncate_graph2(a_copy, visited_nodes_in_paths)\n",
    "        delete_unknown_terms(a_copy)\n",
    "        \n",
    "        # now, given this graph, populate what you need - specifically, we need \"notShownWhenExpanded\" and \"isExpanded\"\n",
    "        notShownWhenExpandedNodes=[]\n",
    "        isExpandedNodes=[]\n",
    "        \n",
    "        getExpandedData(a_copy)\n",
    "        getShownData(a_copy)\n",
    "\n",
    "        assert(len(list(set([list(i.keys())[0] for i in notShownWhenExpandedNodes])))==len(notShownWhenExpandedNodes))        \n",
    "        \n",
    "        notShownWhenExpanded = {}\n",
    "        for i in notShownWhenExpandedNodes:\n",
    "            notShownWhenExpanded.update(i)\n",
    "            \n",
    "        all_states_per_cell_type[end_node] = {'isExpandedNodes': list(set(isExpandedNodes)), 'notShownWhenExpandedNodes': notShownWhenExpanded}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "450f0224",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cell_types_final = []\n",
    "\n",
    "for ct in all_cell_types:\n",
    "    if cell_counts_df_rollup[ct['id']] > 0:\n",
    "        all_cell_types_final.append(ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "424cb374",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/atarashansky/miniconda3/envs/dev/lib/python3.10/site-packages/pronto/parsers/_fastobo.py:49: NotImplementedWarning: cannot process `treat-xrefs-as-equivalent: AEO` macro\n",
      "  process_clause_header(clause, metadata, self.ont)\n",
      "/Users/atarashansky/miniconda3/envs/dev/lib/python3.10/site-packages/pronto/parsers/_fastobo.py:49: NotImplementedWarning: cannot process `treat-xrefs-as-equivalent: BILA` macro\n",
      "  process_clause_header(clause, metadata, self.ont)\n",
      "/Users/atarashansky/miniconda3/envs/dev/lib/python3.10/site-packages/pronto/parsers/_fastobo.py:49: NotImplementedWarning: cannot process `treat-xrefs-as-equivalent: BSPO` macro\n",
      "  process_clause_header(clause, metadata, self.ont)\n",
      "/Users/atarashansky/miniconda3/envs/dev/lib/python3.10/site-packages/pronto/parsers/_fastobo.py:49: NotImplementedWarning: cannot process `treat-xrefs-as-equivalent: CARO` macro\n",
      "  process_clause_header(clause, metadata, self.ont)\n",
      "/Users/atarashansky/miniconda3/envs/dev/lib/python3.10/site-packages/pronto/parsers/_fastobo.py:49: NotImplementedWarning: cannot process `treat-xrefs-as-equivalent: EFO` macro\n",
      "  process_clause_header(clause, metadata, self.ont)\n",
      "/Users/atarashansky/miniconda3/envs/dev/lib/python3.10/site-packages/pronto/parsers/_fastobo.py:49: NotImplementedWarning: cannot process `treat-xrefs-as-equivalent: GO` macro\n",
      "  process_clause_header(clause, metadata, self.ont)\n",
      "/Users/atarashansky/miniconda3/envs/dev/lib/python3.10/site-packages/pronto/parsers/_fastobo.py:49: NotImplementedWarning: cannot process `treat-xrefs-as-equivalent: OG` macro\n",
      "  process_clause_header(clause, metadata, self.ont)\n",
      "/Users/atarashansky/miniconda3/envs/dev/lib/python3.10/site-packages/pronto/parsers/_fastobo.py:49: NotImplementedWarning: cannot process `treat-xrefs-as-equivalent: VSAO` macro\n",
      "  process_clause_header(clause, metadata, self.ont)\n",
      "/Users/atarashansky/miniconda3/envs/dev/lib/python3.10/site-packages/pronto/parsers/_fastobo.py:49: NotImplementedWarning: cannot process `treat-xrefs-as-is_a: VHOG` macro\n",
      "  process_clause_header(clause, metadata, self.ont)\n",
      "/Users/atarashansky/miniconda3/envs/dev/lib/python3.10/site-packages/pronto/parsers/_fastobo.py:49: NotImplementedWarning: cannot process `treat-xrefs-as-has-subclass: EHDAA` macro\n",
      "  process_clause_header(clause, metadata, self.ont)\n",
      "/Users/atarashansky/miniconda3/envs/dev/lib/python3.10/site-packages/pronto/parsers/_fastobo.py:49: NotImplementedWarning: cannot process `treat-xrefs-as-has-subclass: EV` macro\n",
      "  process_clause_header(clause, metadata, self.ont)\n",
      "/Users/atarashansky/miniconda3/envs/dev/lib/python3.10/site-packages/pronto/parsers/_fastobo.py:49: NotImplementedWarning: cannot process `treat-xrefs-as-has-subclass: NCIT` macro\n",
      "  process_clause_header(clause, metadata, self.ont)\n",
      "/Users/atarashansky/miniconda3/envs/dev/lib/python3.10/site-packages/pronto/parsers/_fastobo.py:49: NotImplementedWarning: cannot process `treat-xrefs-as-has-subclass: OGES` macro\n",
      "  process_clause_header(clause, metadata, self.ont)\n",
      "/Users/atarashansky/miniconda3/envs/dev/lib/python3.10/site-packages/pronto/parsers/_fastobo.py:49: NotImplementedWarning: cannot process `treat-xrefs-as-has-subclass: SCTID` macro\n",
      "  process_clause_header(clause, metadata, self.ont)\n",
      "/Users/atarashansky/miniconda3/envs/dev/lib/python3.10/site-packages/pronto/parsers/_fastobo.py:49: NotImplementedWarning: cannot process `treat-xrefs-as-reverse-genus-differentia: AAO part_of NCBITaxon:8292` macro\n",
      "  process_clause_header(clause, metadata, self.ont)\n",
      "/Users/atarashansky/miniconda3/envs/dev/lib/python3.10/site-packages/pronto/parsers/_fastobo.py:49: NotImplementedWarning: cannot process `treat-xrefs-as-reverse-genus-differentia: DHBA part_of NCBITaxon:9606` macro\n",
      "  process_clause_header(clause, metadata, self.ont)\n",
      "/Users/atarashansky/miniconda3/envs/dev/lib/python3.10/site-packages/pronto/parsers/_fastobo.py:49: NotImplementedWarning: cannot process `treat-xrefs-as-reverse-genus-differentia: EHDAA2 part_of NCBITaxon:9606` macro\n",
      "  process_clause_header(clause, metadata, self.ont)\n",
      "/Users/atarashansky/miniconda3/envs/dev/lib/python3.10/site-packages/pronto/parsers/_fastobo.py:49: NotImplementedWarning: cannot process `treat-xrefs-as-reverse-genus-differentia: EMAPA part_of NCBITaxon:10090` macro\n",
      "  process_clause_header(clause, metadata, self.ont)\n",
      "/Users/atarashansky/miniconda3/envs/dev/lib/python3.10/site-packages/pronto/parsers/_fastobo.py:49: NotImplementedWarning: cannot process `treat-xrefs-as-reverse-genus-differentia: FBbt part_of NCBITaxon:7227` macro\n",
      "  process_clause_header(clause, metadata, self.ont)\n",
      "/Users/atarashansky/miniconda3/envs/dev/lib/python3.10/site-packages/pronto/parsers/_fastobo.py:49: NotImplementedWarning: cannot process `treat-xrefs-as-reverse-genus-differentia: FBdv part_of NCBITaxon:7227` macro\n",
      "  process_clause_header(clause, metadata, self.ont)\n",
      "/Users/atarashansky/miniconda3/envs/dev/lib/python3.10/site-packages/pronto/parsers/_fastobo.py:49: NotImplementedWarning: cannot process `treat-xrefs-as-reverse-genus-differentia: FMA part_of NCBITaxon:9606` macro\n",
      "  process_clause_header(clause, metadata, self.ont)\n",
      "/Users/atarashansky/miniconda3/envs/dev/lib/python3.10/site-packages/pronto/parsers/_fastobo.py:49: NotImplementedWarning: cannot process `treat-xrefs-as-reverse-genus-differentia: HAO part_of NCBITaxon:7399` macro\n",
      "  process_clause_header(clause, metadata, self.ont)\n",
      "/Users/atarashansky/miniconda3/envs/dev/lib/python3.10/site-packages/pronto/parsers/_fastobo.py:49: NotImplementedWarning: cannot process `treat-xrefs-as-reverse-genus-differentia: HBA part_of NCBITaxon:9606` macro\n",
      "  process_clause_header(clause, metadata, self.ont)\n",
      "/Users/atarashansky/miniconda3/envs/dev/lib/python3.10/site-packages/pronto/parsers/_fastobo.py:49: NotImplementedWarning: cannot process `treat-xrefs-as-reverse-genus-differentia: HsapDv part_of NCBITaxon:9606` macro\n",
      "  process_clause_header(clause, metadata, self.ont)\n",
      "/Users/atarashansky/miniconda3/envs/dev/lib/python3.10/site-packages/pronto/parsers/_fastobo.py:49: NotImplementedWarning: cannot process `treat-xrefs-as-reverse-genus-differentia: MA part_of NCBITaxon:10090` macro\n",
      "  process_clause_header(clause, metadata, self.ont)\n",
      "/Users/atarashansky/miniconda3/envs/dev/lib/python3.10/site-packages/pronto/parsers/_fastobo.py:49: NotImplementedWarning: cannot process `treat-xrefs-as-reverse-genus-differentia: MFO part_of NCBITaxon:8089` macro\n",
      "  process_clause_header(clause, metadata, self.ont)\n",
      "/Users/atarashansky/miniconda3/envs/dev/lib/python3.10/site-packages/pronto/parsers/_fastobo.py:49: NotImplementedWarning: cannot process `treat-xrefs-as-reverse-genus-differentia: MmusDv part_of NCBITaxon:10090` macro\n",
      "  process_clause_header(clause, metadata, self.ont)\n",
      "/Users/atarashansky/miniconda3/envs/dev/lib/python3.10/site-packages/pronto/parsers/_fastobo.py:49: NotImplementedWarning: cannot process `treat-xrefs-as-reverse-genus-differentia: OlatDv part_of NCBITaxon:8089` macro\n",
      "  process_clause_header(clause, metadata, self.ont)\n",
      "/Users/atarashansky/miniconda3/envs/dev/lib/python3.10/site-packages/pronto/parsers/_fastobo.py:49: NotImplementedWarning: cannot process `treat-xrefs-as-reverse-genus-differentia: PBA part_of NCBITaxon:9443` macro\n",
      "  process_clause_header(clause, metadata, self.ont)\n",
      "/Users/atarashansky/miniconda3/envs/dev/lib/python3.10/site-packages/pronto/parsers/_fastobo.py:49: NotImplementedWarning: cannot process `treat-xrefs-as-reverse-genus-differentia: SPD part_of NCBITaxon:6893` macro\n",
      "  process_clause_header(clause, metadata, self.ont)\n",
      "/Users/atarashansky/miniconda3/envs/dev/lib/python3.10/site-packages/pronto/parsers/_fastobo.py:49: NotImplementedWarning: cannot process `treat-xrefs-as-reverse-genus-differentia: TADS part_of NCBITaxon:6939` macro\n",
      "  process_clause_header(clause, metadata, self.ont)\n",
      "/Users/atarashansky/miniconda3/envs/dev/lib/python3.10/site-packages/pronto/parsers/_fastobo.py:49: NotImplementedWarning: cannot process `treat-xrefs-as-reverse-genus-differentia: TAO part_of NCBITaxon:32443` macro\n",
      "  process_clause_header(clause, metadata, self.ont)\n",
      "/Users/atarashansky/miniconda3/envs/dev/lib/python3.10/site-packages/pronto/parsers/_fastobo.py:49: NotImplementedWarning: cannot process `treat-xrefs-as-reverse-genus-differentia: TGMA part_of NCBITaxon:44484` macro\n",
      "  process_clause_header(clause, metadata, self.ont)\n",
      "/Users/atarashansky/miniconda3/envs/dev/lib/python3.10/site-packages/pronto/parsers/_fastobo.py:49: NotImplementedWarning: cannot process `treat-xrefs-as-reverse-genus-differentia: WBbt part_of NCBITaxon:6237` macro\n",
      "  process_clause_header(clause, metadata, self.ont)\n",
      "/Users/atarashansky/miniconda3/envs/dev/lib/python3.10/site-packages/pronto/parsers/_fastobo.py:49: NotImplementedWarning: cannot process `treat-xrefs-as-reverse-genus-differentia: WBls part_of NCBITaxon:6237` macro\n",
      "  process_clause_header(clause, metadata, self.ont)\n",
      "/Users/atarashansky/miniconda3/envs/dev/lib/python3.10/site-packages/pronto/parsers/_fastobo.py:49: NotImplementedWarning: cannot process `treat-xrefs-as-reverse-genus-differentia: XAO part_of NCBITaxon:8353` macro\n",
      "  process_clause_header(clause, metadata, self.ont)\n",
      "/Users/atarashansky/miniconda3/envs/dev/lib/python3.10/site-packages/pronto/parsers/_fastobo.py:49: NotImplementedWarning: cannot process `treat-xrefs-as-reverse-genus-differentia: ZFA part_of NCBITaxon:7954` macro\n",
      "  process_clause_header(clause, metadata, self.ont)\n",
      "/Users/atarashansky/miniconda3/envs/dev/lib/python3.10/site-packages/pronto/parsers/_fastobo.py:49: NotImplementedWarning: cannot process `treat-xrefs-as-reverse-genus-differentia: ZFS part_of NCBITaxon:7954` macro\n",
      "  process_clause_header(clause, metadata, self.ont)\n"
     ]
    }
   ],
   "source": [
    "uberon = Ontology(\"http://purl.obolibrary.org/obo/uberon.obo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bd23a8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "hemolymphoid_system = 'UBERON:0002193'\n",
    "hematopoietic_system = 'UBERON:0002390'\n",
    "blood = 'UBERON:0000178'\n",
    "immune_organ = 'UBERON:0005057'\n",
    "blacklist = [hemolymphoid_system, hematopoietic_system, blood, immune_organ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9b2b282c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UBERON:0000004\n",
      "UBERON:0000029\n",
      "Not filtering out immune cell for lymph node\n",
      "UBERON:0000030\n",
      "UBERON:0000057\n",
      "UBERON:0000059\n",
      "UBERON:0000160\n",
      "UBERON:0000178\n",
      "Not filtering out immune cell for blood\n",
      "UBERON:0000310\n",
      "UBERON:0000344\n",
      "UBERON:0000916\n",
      "UBERON:0000922\n",
      "UBERON:0000945\n",
      "UBERON:0000948\n",
      "UBERON:0000955\n",
      "UBERON:0000970\n",
      "UBERON:0000990\n",
      "UBERON:0000992\n",
      "UBERON:0000995\n",
      "UBERON:0001004\n",
      "UBERON:0001007\n",
      "UBERON:0001013\n",
      "UBERON:0001015\n",
      "UBERON:0001017\n",
      "UBERON:0001043\n",
      "UBERON:0001087\n",
      "UBERON:0001155\n",
      "UBERON:0001255\n",
      "UBERON:0001264\n",
      "UBERON:0001366\n",
      "UBERON:0001434\n",
      "UBERON:0001723\n",
      "UBERON:0001836\n",
      "UBERON:0001987\n",
      "UBERON:0002048\n",
      "UBERON:0002049\n",
      "UBERON:0002097\n",
      "UBERON:0002106\n",
      "Not filtering out immune cell for spleen\n",
      "UBERON:0002107\n",
      "UBERON:0002108\n",
      "UBERON:0002113\n",
      "UBERON:0002240\n",
      "UBERON:0002358\n",
      "UBERON:0002365\n",
      "UBERON:0002367\n",
      "UBERON:0002368\n",
      "UBERON:0002369\n",
      "UBERON:0002371\n",
      "UBERON:0002405\n",
      "Not filtering out immune cell for immune system\n",
      "UBERON:0003688\n",
      "UBERON:0003697\n",
      "UBERON:0003889\n",
      "UBERON:0007795\n",
      "UBERON:0009472\n",
      "UBERON:0018707\n",
      "UBERON:0035210\n"
     ]
    }
   ],
   "source": [
    "start_node = 'CL:0000548__0'\n",
    "tissue_counts = cc.groupby('tissue_ontology_term_id').sum(numeric_only=True)['n_cells']\n",
    "\n",
    "all_states_per_tissue = {}\n",
    "tissue_by_cell_type = []\n",
    "for tissue in uberon_by_celltype:\n",
    "    if \" (\" not in tissue:\n",
    "        print(tissue)\n",
    "        tissueId=tissue\n",
    "        tissue_term = uberon[tissueId]\n",
    "        tissue_label = tissue_term.name\n",
    "\n",
    "        end_nodes = uberon_by_celltype[tissue]\n",
    "        uberon_ancestors = [i.id for i in tissue_term.superclasses()]\n",
    "        if len(list(set(blacklist).intersection(uberon_ancestors)))==0:\n",
    "            end_nodes2 = [e for e in end_nodes if 'CL:0000988' not in [i.id for i in ontology[e].superclasses()]]\n",
    "            if len(end_nodes2)==0:\n",
    "                print(\"Not filtering out immune cell for\",tissue_label)\n",
    "            else:\n",
    "                end_nodes=end_nodes2\n",
    "        else:\n",
    "            print(\"Not filtering out immune cell for\",tissue_label)\n",
    "\n",
    "\n",
    "        tissue_ct_df = cc.groupby(['tissue_ontology_term_id','cell_type_ontology_term_id']).sum(numeric_only=True).reset_index()\n",
    "        tissue_ct_df = tissue_ct_df[tissue_ct_df['tissue_ontology_term_id']==tissue]\n",
    "        df = tissue_ct_df[['cell_type_ontology_term_id','n_cells']]\n",
    "\n",
    "        to_attach = pd.DataFrame()\n",
    "        to_attach['cell_type_ontology_term_id']=[i for i in all_cell_types_ids if i not in df['cell_type_ontology_term_id'].values]\n",
    "        to_attach['n_cells']=0\n",
    "\n",
    "        df = pd.concat([df,to_attach],axis=0)    \n",
    "        df['n_cells_rollup'] = df['n_cells']\n",
    "        df_rollup = rollup_across_cell_type_descendants(df,ignore_cols=['n_cells'])\n",
    "        df_rollup = df_rollup[df_rollup['n_cells_rollup'] > 0]\n",
    "\n",
    "        res = dict(zip(df_rollup['cell_type_ontology_term_id'],df_rollup[['n_cells','n_cells_rollup']].to_dict(orient='records')))\n",
    "\n",
    "        tissue_by_cell_type.append({\"id\": tissue, \"label\": tissue_label})\n",
    "\n",
    "\n",
    "\n",
    "        all_paths=[]\n",
    "        for end_node in end_nodes:\n",
    "            if end_node in traverse_node_counter:\n",
    "                i=0 #only get path to the first instance of a node.\n",
    "                paths = []    \n",
    "                dfs(all_parents,end_node+\"__\"+str(i),start_node,all_paths=paths)\n",
    "\n",
    "                paths = [i[::-1] for i in paths] \n",
    "                if len(paths) == 0:\n",
    "                    all_paths.append([end_node+\"__\"+str(i)])\n",
    "                else:\n",
    "                    all_paths.append(paths[0])\n",
    "\n",
    "        ### RULES ###\n",
    "        # 1. We only want to show terms that are CHILDREN, GRANDCHILDREN, SIBLINGS OF TARGET, or IN A PATH TO TARGET\n",
    "        visited_nodes_in_paths = list(set(sum(all_paths,[])))\n",
    "\n",
    "        valid_nodes = list(set(visited_nodes_in_paths))\n",
    "\n",
    "        a_copy = json.loads(json.dumps(a))\n",
    "        seen_nodes_per_tissue=set()\n",
    "        truncate_graph_per_tissue(a_copy,valid_nodes, tissue_counts[tissue], res) \n",
    "\n",
    "        delete_unknown_terms(a_copy)\n",
    "\n",
    "        # now, given this graph, populate what you need - specifically, we need \"notShownWhenExpanded\" and \"isExpanded\"\n",
    "        notShownWhenExpandedNodes=[]\n",
    "        isExpandedNodes=[]\n",
    "\n",
    "        getExpandedData(a_copy)\n",
    "        getShownData(a_copy)\n",
    "\n",
    "        assert(len(list(set([list(i.keys())[0] for i in notShownWhenExpandedNodes])))==len(notShownWhenExpandedNodes))        \n",
    "\n",
    "        notShownWhenExpanded = {}\n",
    "        for i in notShownWhenExpandedNodes:\n",
    "            notShownWhenExpanded.update(i)\n",
    "\n",
    "        all_states_per_tissue[tissue] = {'isExpandedNodes': list(set(isExpandedNodes)), 'notShownWhenExpandedNodes': notShownWhenExpanded, \"tissueCounts\": res}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "42641f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(tissue_by_cell_type, open('build_graph_output/allTissues.json','w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a2cbd637",
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(all_states_per_tissue,open('build_graph_output/ontologyTreeStatePerTissue.json','w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "665b6607",
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_unknown_terms(a)\n",
    "json.dump(a,open('build_graph_output/ontologyTree.json','w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "47ac0685",
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(all_states_per_cell_type,open('build_graph_output/ontologyTreeStatePerCellType.json','w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "afe5fd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(all_cell_types_final,open('build_graph_output/allCellTypes.json','w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9ad53603",
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(all_cell_type_owl_descriptions, open('build_graph_output/allCellTypeOwlDescriptions.json','w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f0321ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv build_graph_output/*.json frontend/src/views/CellCards/common/fixtures/."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
