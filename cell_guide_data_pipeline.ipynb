{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f111eea9",
   "metadata": {},
   "source": [
    "# CellGuide data pipeline prototype\n",
    "\n",
    "This assumes a local snapshot has been downloaded to the same location as this notebook.\n",
    "\n",
    "Here is a one-liner for downloading the latest prod snapshot:\n",
    "\n",
    "```\n",
    "AWS_PROFILE=single-cell-prod aws s3 sync s3://cellxgene-wmg-prod/$(AWS_PROFILE=single-cell-prod aws s3 cp s3://cellxgene-wmg-prod/latest_snapshot_identifier -)/ prod-snapshot/\n",
    "```\n",
    "\n",
    "**This file should be in the root folder of the `single-cell-data-portal` repo**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4406ff73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from backend.wmg.api.v1 import build_filter_dims_values\n",
    "from backend.wmg.data.ontology_labels import ontology_term_label, ontology_term_id_labels\n",
    "from backend.wmg.data.snapshot import WmgSnapshot\n",
    "from backend.wmg.data.query import WmgFiltersQueryCriteria\n",
    "import tiledb\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import openai\n",
    "from backend.wmg.data.utils import get_datasets_from_curation_api, get_collections_from_curation_api\n",
    "from backend.wmg.data.rollup import rollup_across_cell_type_descendants\n",
    "import glob\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67bc15db",
   "metadata": {},
   "outputs": [],
   "source": [
    "sn = \"prod-snapshot\"\n",
    "snapshot = WmgSnapshot(snapshot_identifier=\"\",\n",
    "    expression_summary_cube=tiledb.open(f'{sn}/expression_summary'),\n",
    "    marker_genes_cube=tiledb.open(f'{sn}/marker_genes'),\n",
    "    expression_summary_default_cube=tiledb.open(f'{sn}/expression_summary_default'),\n",
    "    expression_summary_fmg_cube=tiledb.open(f'{sn}/expression_summary_fmg'),                       \n",
    "    cell_counts_cube=tiledb.open(f'{sn}/cell_counts'),\n",
    "    cell_type_orderings=pd.read_json(f'{sn}/cell_type_orderings.json'),\n",
    "    primary_filter_dimensions=json.load(open(f'{sn}/primary_filter_dimensions.json','r')),\n",
    "    dataset_to_gene_ids=json.load(open(f'{sn}/dataset_to_gene_ids.json','r')), \n",
    "    filter_relationships=json.load(open(f'{sn}/filter_relationships.json','r')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b74885",
   "metadata": {},
   "source": [
    "## Generate all cell types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863a91fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cell_types = [{k: ontology_term_label(k)} for k in ontology_term_id_labels if k.startswith('CL:')]\n",
    "json.dump(all_cell_types,open('allCellTypes.json','w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fefbe9e",
   "metadata": {},
   "source": [
    "## Generate cell type descriptions using GPT 3.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73689325",
   "metadata": {},
   "source": [
    "System role:\n",
    "> You are a knowledgeable cell biologist that has professional experience writing and curating accurate and informative descriptions of cell types.\n",
    "\n",
    "User role:\n",
    "> I am making a knowledge-base about cell types. Each cell type is a term from the Cell Ontology and will have its own page with a detailed description of that cell type and its function. Please write me a description for \"{cell_type_name}\". Please return only the description and no other dialogue. The description should include information about the cell type's function. The description should be at least three paragraphs long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edaeed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.organization = \"org-4kBCayJVUBGqH42cJhzZYQ6o\"\n",
    "openai.api_key = \"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\"\n",
    "openai.Model.list()\n",
    "\n",
    "cell_type_descriptions = {}\n",
    "for cell_type in all_cell_types:\n",
    "    cid = list(cell_type.keys())[0]\n",
    "    cname = list(cell_type.values())[0]\n",
    "    print(cid, cname)\n",
    "    \n",
    "    succeeded=False\n",
    "    while not succeeded:\n",
    "        try:\n",
    "            result = openai.ChatCompletion.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a knowledgeable cell biologist that has professional experience writing and curating accurate and informative descriptions of cell types.\"},\n",
    "                    {\"role\": \"user\", \"content\": f\"I am making a knowledge-base about cell types. Each cell type is a term from the Cell Ontology and will have its own page with a detailed description of that cell type and its function. Please write me a description for \\\"{cname}\\\". Please return only the description and no other dialogue. The description should include information about the cell type's function. The description should be at least three paragraphs long.\"},\n",
    "                ]\n",
    "            ) \n",
    "            succeeded=True\n",
    "        except:\n",
    "            print(\"Trying again due to RLE\")\n",
    "            \n",
    "    print(result['choices'][0]['message']['content'])\n",
    "    \n",
    "    cell_type_descriptions[cid] = result['choices'][0]['message']['content']\n",
    "    \n",
    "    # dump at every iteration to save place\n",
    "    json.dump(cell_type_descriptions,open('allCellTypeDescriptions.json','w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ea82f4",
   "metadata": {},
   "source": [
    "## Generate source data per cell type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10b7cc76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: DEPLOYMENT_STAGE=test\n"
     ]
    }
   ],
   "source": [
    "%env DEPLOYMENT_STAGE=test\n",
    "\n",
    "def get_title_from_doi(doi):\n",
    "    url = f\"https://api.crossref.org/works/{doi}\"\n",
    "\n",
    "    # Send a GET request to the API\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # If the GET request is successful, the status code will be 200\n",
    "    if response.status_code == 200:\n",
    "        # Get the response data\n",
    "        data = response.json()\n",
    "\n",
    "        # Get the title and citation count from the data\n",
    "        try:\n",
    "            title = data['message']['title'][0]\n",
    "        except:\n",
    "            try:\n",
    "                title = data['message']['items'][0]['title'][0]\n",
    "            except:\n",
    "                return doi\n",
    "        return title\n",
    "    else:\n",
    "        return doi\n",
    "    \n",
    "def format_citation(metadata):\n",
    "    first_author = metadata['publisher_metadata']['authors'][0]\n",
    "    if \"family\" in first_author:\n",
    "        author_str = f\"{first_author['family']}, {first_author['given']} et al.\"\n",
    "    else:\n",
    "        author_str = f\"{first_author['name']} et al.\"\n",
    "    \n",
    "    journal = metadata['publisher_metadata']['journal']\n",
    "    year = metadata['publisher_metadata']['published_year']\n",
    "    \n",
    "    return f\"{author_str} ({year}) {journal}\"\n",
    "\n",
    "snapshot.build_dataset_metadata_dict()\n",
    "\n",
    "datasets = get_datasets_from_curation_api()\n",
    "collections = get_collections_from_curation_api()\n",
    "\n",
    "collections_dict = {collection['collection_id']: collection for collection in collections}\n",
    "datasets_dict = {dataset['dataset_id']: dataset for dataset in datasets}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f791b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "cts = [list(i.keys())[0] for i in all_cell_types]\n",
    "\n",
    "DATA = {}\n",
    "for i in cts:\n",
    "    criteria = WmgFiltersQueryCriteria(organism_ontology_term_id=\"NCBITaxon:9606\",\n",
    "                                       **dict(cell_type_ontology_term_ids=[i]))\n",
    "    res = build_filter_dims_values(criteria, snapshot)\n",
    "    datasets = res['datasets']\n",
    "    collections_to_datasets = {}\n",
    "    for dataset in datasets:\n",
    "        dataset = datasets_dict[dataset['id']]\n",
    "        \n",
    "        a = collections_to_datasets.get(dataset['collection_id'],{})\n",
    "        \n",
    "        a['collection_name'] = collections_dict[dataset['collection_id']]['name']\n",
    "        a['collection_url'] = collections_dict[dataset['collection_id']]['collection_url']\n",
    "        a['publication_url'] = collections_dict[dataset['collection_id']]['doi']\n",
    "        if collections_dict[dataset['collection_id']]['publisher_metadata']:\n",
    "            a['publication_title'] = format_citation(collections_dict[dataset['collection_id']])\n",
    "        else:\n",
    "            a['publication_title'] = \"Publication\"\n",
    "        a['tissue'] = dataset['tissue']\n",
    "        a['disease'] = dataset['disease']\n",
    "        a['organism'] = dataset['organism']\n",
    "        \n",
    "        collections_to_datasets[dataset['collection_id']]=a\n",
    "    \n",
    "    DATA[i] = list(collections_to_datasets.values())                                 \n",
    "    \n",
    "    # dump at every iteration to save place\n",
    "    json.dump(DATA,open('allSourceData.json','w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8673740",
   "metadata": {},
   "source": [
    "## Generate enriched genes and expression metrics per cell type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "52deef84",
   "metadata": {},
   "outputs": [],
   "source": [
    "markers_df = snapshot.marker_genes_cube.df[:]\n",
    "markers_df=markers_df[markers_df['effect_size_ttest']>0]\n",
    "markers_df=markers_df[markers_df['p_value_ttest']<1e-5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "52bf8e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "markers_df_agg = markers_df.groupby(['organism_ontology_term_id','cell_type_ontology_term_id','gene_ontology_term_id']).agg({'effect_size_ttest': 'max'}).reset_index()\n",
    "markers_df_agg2 = markers_df.groupby(['organism_ontology_term_id','cell_type_ontology_term_id','gene_ontology_term_id']).agg({'effect_size_ttest': 'mean'}).reset_index()\n",
    "markers_df_agg3 = markers_df.groupby(['organism_ontology_term_id','cell_type_ontology_term_id','gene_ontology_term_id']).agg({'effect_size_ttest': 'min'}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc6c552",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_25_per_group = markers_df_agg.groupby(['organism_ontology_term_id','cell_type_ontology_term_id']).apply(lambda x: x.nlargest(25, 'effect_size_ttest'))\n",
    "\n",
    "columns = ['organism_ontology_term_id','cell_type_ontology_term_id','gene_ontology_term_id']\n",
    "o_ct_genes = list(zip(*top_25_per_group[columns].values.T))\n",
    "\n",
    "expressions_df = snapshot.expression_summary_default_cube.df[:]\n",
    "\n",
    "cell_counts_df = snapshot.cell_counts_cube.df[:]\n",
    "\n",
    "cell_counts_df_rollup = rollup_across_cell_type_descendants(\n",
    "    cell_counts_df.groupby(['organism_ontology_term_id','cell_type_ontology_term_id']).sum(numeric_only=True).reset_index()\n",
    ")\n",
    "\n",
    "expressions_df_rollup = rollup_across_cell_type_descendants(\n",
    "    expressions_df.groupby(['organism_ontology_term_id','gene_ontology_term_id','cell_type_ontology_term_id']).sum(numeric_only=True).reset_index()\n",
    ")\n",
    "\n",
    "filt1 = expressions_df_rollup['cell_type_ontology_term_id'].isin(top_25_per_group['cell_type_ontology_term_id'].unique())\n",
    "\n",
    "filt2 = expressions_df_rollup['gene_ontology_term_id'].isin(top_25_per_group['gene_ontology_term_id'].unique())\n",
    "\n",
    "filt = np.logical_and(filt1,filt2)\n",
    "\n",
    "expressions_df_rollup = expressions_df_rollup[filt]\n",
    "\n",
    "expressions_df_rollup.index = pd.Index(list(zip(*expressions_df_rollup[['organism_ontology_term_id','cell_type_ontology_term_id','gene_ontology_term_id']].values.T)))\n",
    "\n",
    "cell_counts_df_rollup = cell_counts_df_rollup.set_index('cell_type_ontology_term_id')['n_cells']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da177691",
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_id_to_symbol={}\n",
    "all_genes = []\n",
    "for k in snapshot.primary_filter_dimensions['gene_terms']:\n",
    "    for i in snapshot.primary_filter_dimensions['gene_terms'][k]:\n",
    "        gene_id_to_symbol.update(i)\n",
    "        all_genes.append(list(i.keys())[0])\n",
    "\n",
    "gene_symbol_to_name = pd.read_csv('ncbi_dataset.tsv',sep='\\t')\n",
    "\n",
    "gene_symbol_to_name = gene_symbol_to_name.set_index('Symbol')['Description']\n",
    "\n",
    "gene_id_to_name={}\n",
    "for j in all_genes:\n",
    "    i = gene_id_to_symbol[j]\n",
    "    try:\n",
    "        gene_id_to_name[i] = gene_symbol_to_name[i]\n",
    "    except:\n",
    "        gene_id_to_name[i] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "953722ca",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Fabp7'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda/envs/scdp2/lib/python3.9/site-packages/pandas/core/indexes/base.py:3652\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3651\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3652\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3653\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/miniconda/envs/scdp2/lib/python3.9/site-packages/pandas/_libs/index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda/envs/scdp2/lib/python3.9/site-packages/pandas/_libs/index.pyx:171\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda/envs/scdp2/lib/python3.9/site-packages/pandas/_libs/index.pyx:214\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine._get_loc_duplicates\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda/envs/scdp2/lib/python3.9/site-packages/pandas/_libs/index.pyx:222\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine._maybe_get_bool_indexer\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda/envs/scdp2/lib/python3.9/site-packages/pandas/_libs/index.pyx:114\u001b[0m, in \u001b[0;36mpandas._libs.index._unpack_bool_indexer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Fabp7'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 15\u001b[0m\n\u001b[1;32m      8\u001b[0m     n_cells \u001b[38;5;241m=\u001b[39m cell_counts_df_rollup[ct]\n\u001b[1;32m     10\u001b[0m     a \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mget(ct,[])\n\u001b[1;32m     11\u001b[0m     a\u001b[38;5;241m.\u001b[39mappend({\n\u001b[1;32m     12\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mme\u001b[39m\u001b[38;5;124m'\u001b[39m: s\u001b[38;5;241m/\u001b[39mnnz \u001b[38;5;28;01mif\u001b[39;00m nnz \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m     13\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpc\u001b[39m\u001b[38;5;124m'\u001b[39m: nnz\u001b[38;5;241m/\u001b[39mn_cells,\n\u001b[1;32m     14\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msymbol\u001b[39m\u001b[38;5;124m'\u001b[39m: gene,\n\u001b[0;32m---> 15\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mgene_symbol_to_name\u001b[49m\u001b[43m[\u001b[49m\u001b[43mgene\u001b[49m\u001b[43m]\u001b[49m,\n\u001b[1;32m     16\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124morganism\u001b[39m\u001b[38;5;124m'\u001b[39m: o\n\u001b[1;32m     17\u001b[0m     })\n\u001b[1;32m     18\u001b[0m     data[ct]\u001b[38;5;241m=\u001b[39ma\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m#json.dump(data,open('allEnrichedGenes.json','w'))\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda/envs/scdp2/lib/python3.9/site-packages/pandas/core/series.py:1012\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1009\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[1;32m   1011\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[0;32m-> 1012\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_hashable(key):\n\u001b[1;32m   1015\u001b[0m     \u001b[38;5;66;03m# Otherwise index.get_value will raise InvalidIndexError\u001b[39;00m\n\u001b[1;32m   1016\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1017\u001b[0m         \u001b[38;5;66;03m# For labels that don't resolve as scalars like tuples and frozensets\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda/envs/scdp2/lib/python3.9/site-packages/pandas/core/series.py:1121\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[0;32m-> 1121\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[1;32m   1124\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[loc]\n",
      "File \u001b[0;32m~/miniconda/envs/scdp2/lib/python3.9/site-packages/pandas/core/indexes/base.py:3654\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3652\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3653\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3654\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3655\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3656\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3657\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3658\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3659\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Fabp7'"
     ]
    }
   ],
   "source": [
    "data={}\n",
    "for i in o_ct_genes:\n",
    "    o,ct,gene = i\n",
    "    gene = gene_id_to_symbol[gene]\n",
    "    \n",
    "    nnz = expressions_df_rollup['nnz'][i]\n",
    "    s = expressions_df_rollup['sum'][i]\n",
    "    n_cells = cell_counts_df_rollup[ct]\n",
    "    \n",
    "    a = data.get(ct,[])\n",
    "    a.append({\n",
    "        'me': s/nnz if nnz > 0 else 0,\n",
    "        'pc': nnz/n_cells,\n",
    "        'symbol': gene,\n",
    "        'name': gene_symbol_to_name[gene],\n",
    "        'organism': o\n",
    "    })\n",
    "    data[ct]=a\n",
    "    \n",
    "#json.dump(data,open('allEnrichedGenes.json','w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61efe06d",
   "metadata": {},
   "source": [
    "## Generate canonical marker genes data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1444c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_prefix_cols(prefix, cols):\n",
    "    i=1\n",
    "    prefix_cols = []\n",
    "    while True:\n",
    "        col = f\"{prefix}{i}\"\n",
    "        if col in cols:\n",
    "            prefix_cols.append(col)\n",
    "            i+=1\n",
    "        elif col.upper() in cols:\n",
    "            prefix_cols.append(col.upper())\n",
    "            i+=1            \n",
    "        elif col.lower() in cols:\n",
    "            prefix_cols.append(col.lower())\n",
    "            i+=1\n",
    "        else:\n",
    "            break   \n",
    "    return prefix_cols\n",
    "\n",
    "def get_all_suffix_cols(prefix,suffix, cols):\n",
    "    i=1\n",
    "    suffix_cols = []\n",
    "    while True:\n",
    "        col = f\"{prefix}{i}{suffix}\"\n",
    "        if col in cols:\n",
    "            suffix_cols.append(col)\n",
    "            i+=1\n",
    "        elif col.upper() in cols:\n",
    "            suffix_cols.append(col.upper())\n",
    "            i+=1            \n",
    "        elif col.lower() in cols:\n",
    "            suffix_cols.append(col.lower())\n",
    "            i+=1\n",
    "        else:\n",
    "            break    \n",
    "    return suffix_cols\n",
    "\n",
    "def get_gene_name(gene):\n",
    "    a = requests.get(f\"https://api.cellxgene.dev.single-cell.czi.technology/gene_info/v1/gene_info?gene={gene}\")\n",
    "    if a.status_code == 200:\n",
    "        r = a.json()\n",
    "        return r['name']\n",
    "    else:\n",
    "        return gene\n",
    "    \n",
    "def try_delete(d, k):\n",
    "    try:\n",
    "        del d[k]\n",
    "    except:\n",
    "        try:\n",
    "            del d[k[0]+k[1:].lower()]\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "def get_title_from_doi(doi):\n",
    "    url = f\"https://api.crossref.org/works/{doi}\"\n",
    "\n",
    "    # Send a GET request to the API\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # If the GET request is successful, the status code will be 200\n",
    "    if response.status_code == 200:\n",
    "        # Get the response data\n",
    "        data = response.json()\n",
    "\n",
    "        # Get the title and citation count from the data\n",
    "        try:\n",
    "            title = data['message']['title'][0]\n",
    "        except:\n",
    "            try:\n",
    "                title = data['message']['items'][0]['title'][0]\n",
    "            except:\n",
    "                return doi\n",
    "        return title\n",
    "    else:\n",
    "        return doi\n",
    "    \n",
    "\n",
    "\n",
    "def get_tissue_name(t):\n",
    "    t=t.replace(':','_')\n",
    "    urls = [\n",
    "        f\"https://www.ebi.ac.uk/ols4/api/ontologies/clo/terms/http%253A%252F%252Fpurl.obolibrary.org%252Fobo%252F{t}\",\n",
    "        f\"https://www.ebi.ac.uk/ols4/api/ontologies/envo/terms/http%253A%252F%252Fpurl.obolibrary.org%252Fobo%252F{t}\",\n",
    "        f\"https://www.ebi.ac.uk/ols4/api/ontologies/flopo/terms/http%253A%252F%252Fpurl.obolibrary.org%252Fobo%252F{t}\",\n",
    "        f\"https://www.ebi.ac.uk/ols4/api/ontologies/doid/terms/http%253A%252F%252Fpurl.obolibrary.org%252Fobo%252F{t}\",\n",
    "    ]\n",
    "    for url in urls:    \n",
    "        response = requests.get(url)\n",
    "        if response.status_code==200:\n",
    "            r = response.json()\n",
    "            return r['label']\n",
    "    return t\n",
    "\n",
    "pf = json.load(open('dev-sn/primary_filter_dimensions.json','r'))\n",
    "all_human_genes = [list(i.values())[0] for i in pf['gene_terms']['NCBITaxon:9606']]\n",
    "\n",
    "X = tiledb.open('dev-sn/cell_counts/')\n",
    "cc = X.df[:]\n",
    "tissue_original = list(set(cc['tissue_original_ontology_term_id']))\n",
    "tissue = list(set(cc['tissue_ontology_term_id']))\n",
    "\n",
    "m = {}\n",
    "[m.update(i) for i in pf['tissue_terms']['NCBITaxon:9606']];\n",
    "for i in tissue:\n",
    "    if i not in m:\n",
    "        m[i]=i\n",
    "        \n",
    "for i in tissue_original:\n",
    "    if i not in m:\n",
    "        m[i]=i\n",
    "\n",
    "files = glob.glob('tables/*.csv')\n",
    "\n",
    "parsed_table_entries = []\n",
    "\n",
    "seen=[]\n",
    "for file in files:\n",
    "    print(file)\n",
    "\n",
    "    df = pd.read_csv(file,skiprows=10)\n",
    "    assert df.columns[0]=='AS/1'\n",
    "\n",
    "    cols = list(df.columns)\n",
    "\n",
    "    ref_prefix = \"Ref/\"\n",
    "    ref_doi_suffix = \"/DOI\"\n",
    "\n",
    "    ref_prefixes = get_all_prefix_cols(ref_prefix,cols)\n",
    "    if len(ref_prefixes):\n",
    "        prefix=ref_prefixes[0].split('/')[0]+\"/\"\n",
    "        ref_suffixes=get_all_suffix_cols(prefix,ref_doi_suffix,cols)\n",
    "        ref_suffixes_notes=get_all_suffix_cols(prefix,\"/NOTES\",cols)\n",
    "    else:\n",
    "        ref_suffixes=[]\n",
    "        ref_suffixes_notes=[]\n",
    "\n",
    "\n",
    "    gene_prefix = \"BGene/\"\n",
    "    gene_label_suffix = \"/LABEL\"\n",
    "\n",
    "    gene_prefixes = get_all_prefix_cols(gene_prefix,cols)\n",
    "    if len(gene_prefixes):\n",
    "        prefix=gene_prefixes[0].split('/')[0]+\"/\"\n",
    "        gene_suffixes=get_all_suffix_cols(prefix,gene_label_suffix,cols)\n",
    "    else:\n",
    "        gene_suffixes=[]\n",
    "\n",
    "    tissue_prefix = \"AS/\"\n",
    "    protein_label_suffix = \"/ID\"\n",
    "\n",
    "    tissue_prefixes = get_all_prefix_cols(tissue_prefix,cols)\n",
    "    if len(tissue_prefixes):\n",
    "        prefix=tissue_prefixes[0].split('/')[0]+\"/\"\n",
    "        tissue_suffixes=get_all_suffix_cols(prefix,protein_label_suffix,cols) \n",
    "    else:\n",
    "        tissue_suffixes=[]\n",
    "\n",
    "    ct = \"CT/1\"\n",
    "    ctid = \"CT/1/ID\"\n",
    "    try:\n",
    "        assert len(ref_prefixes) > 0\n",
    "        assert len(ref_suffixes) > 0\n",
    "        assert len(gene_prefixes) > 0 \n",
    "        assert len(gene_suffixes) > 0 \n",
    "        assert len(tissue_suffixes) > 0\n",
    "    except:\n",
    "        print(\"Skipping\")\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    for n in range(df.shape[0]):\n",
    "        res = df.iloc[n].to_dict()\n",
    "        data_tmp = {i: res[i] for i in res if i in [ct, ctid] + ref_prefixes+ref_suffixes+gene_prefixes+gene_suffixes+tissue_suffixes+ref_suffixes_notes}\n",
    "        data = data_tmp.copy()\n",
    "        genes = []\n",
    "        gene_to_key = {}\n",
    "        for i in gene_prefixes:\n",
    "            data_tmp[i] = str(data_tmp[i]).split(' ')[0]\n",
    "            if data_tmp[i].upper() not in all_human_genes or data_tmp[i]=='nan':\n",
    "                try_delete(data,i)\n",
    "                try_delete(data,i+'/LABEL')\n",
    "            else:\n",
    "                data[i] = data_tmp[i].upper()\n",
    "                genes.append(data[i])\n",
    "                gene_to_key[data[i]] = i\n",
    "\n",
    "        valid_ref_accessors = []\n",
    "        for i in ref_suffixes:\n",
    "            i_prefix = '/'.join(i.split('/')[:-1])\n",
    "            \n",
    "            if str(data[i_prefix +'/DOI'])=='nan' or str(data[i_prefix +'/DOI'])=='No DOI':\n",
    "                try_delete(data,i)\n",
    "                try_delete(data,i_prefix)\n",
    "                try_delete(data,i_prefix+'/NOTES')\n",
    "            else:\n",
    "                data[i_prefix+'/DOI'] = data[i_prefix+'/DOI'].split(' ')[-1]\n",
    "                try_delete(data,i_prefix)\n",
    "                try_delete(data,i_prefix+'/NOTES')                \n",
    "                valid_ref_accessors.append(i_prefix)\n",
    "\n",
    "        refs = []\n",
    "        titles = []\n",
    "        for i in valid_ref_accessors:\n",
    "            doi = data[i+'/DOI']\n",
    "            title = get_title_from_doi(doi)\n",
    "            \n",
    "            refs.append(doi)\n",
    "            titles.append(title)\n",
    "            \n",
    "        refs = ';;'.join(refs)\n",
    "        titles = ';;'.join(titles)\n",
    "            \n",
    "        if not str(data[ctid]).startswith('CL:'):         \n",
    "            continue\n",
    "        \n",
    "        tissue_general = None\n",
    "        for i in tissue_suffixes[::-1]:\n",
    "            if data[i] in tissue:\n",
    "                tissue_general = data[i]\n",
    "                break\n",
    "\n",
    "        tissue_specific = None\n",
    "        for i in tissue_suffixes[::-1]:\n",
    "            if data[i] in tissue_original:\n",
    "                tissue_specific = data[i]\n",
    "                break\n",
    "\n",
    "        if tissue_general is None:\n",
    "            for i in tissue_suffixes:\n",
    "                if data[i].startswith(\"UBERON:\"):\n",
    "                    tissue_general=data[i]\n",
    "                    break\n",
    "                    \n",
    "        if tissue_specific is None:\n",
    "            for i in tissue_suffixes:\n",
    "                if data[i].startswith(\"UBERON:\"):\n",
    "                    tissue_specific=data[i]                    \n",
    "                    break\n",
    "                    \n",
    "        assert tissue_general is not None\n",
    "        assert tissue_specific is not None\n",
    "\n",
    "        for gene in genes:\n",
    "            label = str(data[gene_to_key[gene]+'/LABEL'])\n",
    "            if gene == label.upper() or label == 'nan':\n",
    "                label = get_gene_name(gene)\n",
    "\n",
    "\n",
    "            gene_dict = {\n",
    "                \"tissue_general\": tissue_general,\n",
    "                \"tissue_specific\": tissue_specific,\n",
    "                \"symbol\": gene,\n",
    "                \"name\": label,\n",
    "                \"publication\": refs,\n",
    "                \"publication_titles\": titles,\n",
    "                \"cell_type_ontology_term_id\": data[ctid]\n",
    "            }\n",
    "            hashed_dict = hash(json.dumps(gene_dict))\n",
    "            if hashed_dict not in seen:\n",
    "                parsed_table_entries.append(gene_dict)\n",
    "                seen.append(hashed_dict)\n",
    "                \n",
    "\n",
    "ts = list(set([i['tissue_general'] for i in parsed_table_entries]+[i['tissue_specific'] for i in parsed_table_entries]))\n",
    "\n",
    "tissues_by_id = {t: get_tissue_name(t) for t in ts}\n",
    "\n",
    "gene_infos = {}\n",
    "for entry in parsed_table_entries:\n",
    "    entry = entry.copy()\n",
    "    ct = entry['cell_type_ontology_term_id']\n",
    "    del entry['cell_type_ontology_term_id']\n",
    "    \n",
    "    a = gene_infos.get(ct,[])\n",
    "    entry['tissue_general'] = tissues_by_id.get(entry['tissue_general'],entry['tissue_general'])\n",
    "    entry['tissue_specific'] =tissues_by_id.get(entry['tissue_specific'],entry['tissue_specific'])\n",
    "    a.append(entry)\n",
    "    gene_infos[ct]=a\n",
    "\n",
    "\n",
    "json.dump(gene_infos,open('allCellTypeMarkerGenes.json','w'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
