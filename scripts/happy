#!/usr/bin/env python3
import random
from base64 import b64decode
from contextlib import contextmanager
from datetime import datetime
import io
import json
import os
import re
import subprocess
import tarfile
import time
from urllib.parse import urlparse

import boto3
from botocore.config import Config
import click
import tempfile
from string import Template
from terrasnek.api import TFC
from terrasnek import exceptions


class CliError(Exception):
    # Don't print core dumps for some kinds of exceptions
    pass


class HappyConfig:
    def __init__(self, config_file=".happy/config.json", env=None, substitutions=None, ctx=None):
        self.ctx = ctx
        with open(config_file) as f:
            self._data_text = f.read()
        self.env = env
        self.load_data(substitutions)
        try:
            if self.config_version != "v0":
                raise CliError(f'Config file {config_file} has invalid version number. Only version "v0" supported.')
        except KeyError:
            raise CliError(f"Config file {config_file} missing config_version field.")

    def load_data(self, substitutions=None):
        if substitutions:
            self._data = json.loads(Template(self._data_text).safe_substitute(substitutions))
        else:
            self._data = json.loads(self._data_text)
        self.set_env()

    def set_env(self):
        env_override = self.env
        env = self._data["default_env"]
        if env_override:
            env = env_override
        elif os.getenv("HAPPY_ENV"):
            env = os.getenv("HAPPY_ENV")

        try:
            self.env = env  # HACK HACK HACK
            self._data["env"] = env
            self._data.update(self._data["environments"][env])
        except KeyError:
            error = f'Invalid environment: "{env}". Check .happy/config for valid environments'
            raise CliError(error)

    def load_registries(self):
        if self._data.get("registries_loaded"):
            return True
        substitutions = self.ctx.obj["secret_mgr"].secrets
        self._data["registries_loaded"] = True
        self.load_data(substitutions)

    def __getattr__(self, field):
        if field in self._data:
            if field == "container_registries":
                self.load_registries()
            return self._data[field]
        return self.__dict__[field]


class TablePrinter:
    def __init__(self, headers):
        self.rows = []
        self.widths = []

        self.bump_widths(headers)
        self.headers = headers

    def bump_widths(self, data):
        for i in range(len(data)):
            try:
                self.widths[i] = max(len(data[i]), self.widths[i])
            except IndexError:
                self.widths.append(len(data[i]))

    def add_row(self, row):
        self.bump_widths(row)
        self.rows.append(row)

    def print(self):
        fmt_string = "  ".join(["{: <%s}" % width for width in self.widths])
        print(fmt_string.format(*self.headers))
        separators = ["-----" for i in range(len(self.headers))]
        print(fmt_string.format(*separators))
        for row in self.rows:
            print(fmt_string.format(*row))


class StackMeta:
    tag_map = {
        "app": "happy/app",
        "env": "happy/env",
        "instance": "happy/instance",
        "owner": "happy/meta/owner",
        "priority": "happy/meta/priority",
        "imagetag": "happy/meta/imagetag",
        "configsecret": "happy/meta/configsecret",
        "created": "happy/meta/created-at",
        "updated": "happy/meta/updated-at",
    }

    parameter_map = {
        "instance": "stack_name",
        "priority": "priority",
        "imagetag": "image_tag",
        "configsecret": "happy_config_secret"
    }

    def __init__(self, ctx, stack_name):
        self.stack_name = stack_name
        self.ctx = ctx
        config = ctx.obj["config"]
        self.meta = {
            "app": config.app,
            "env": config.env,
            "instance": self.stack_name,
        }

    def load(self, existing_tags):
        for short_tag, tag_name in self.tag_map.items():
            if tag_name in existing_tags:
                self.meta[short_tag] = existing_tags[tag_name]
            elif short_tag not in self.meta:
                self.meta[short_tag] = ""

    def __getattr__(self, tag):
        if tag in self.tag_map:
            return self.meta[tag]
        return self.__dict__[tag]

    def __setattr__(self, tag, value):
        if tag in self.tag_map:
            self.meta[tag] = value
        else:
            self.__dict__[tag] = value

    @property
    def tags(self):
        return {v: self.meta[k] for k, v in self.tag_map.items()}

    @property
    def parameters(self):
        return {v: self.meta[k] for k, v in self.parameter_map.items()}

    def update(self, tag, stack_mgr):
        stacks = stack_mgr.stacks

        # Track timestamps for this stack
        now = int(time.time())
        if not self.created:
            self.created = now

        self.imagetag = tag
        self.updated = now

        if not self.owner:
            self.owner = resolve_owner(self.ctx)

        if not self.priority:
            # Find the first available priority id and use it.
            existing_priorities = set()
            for stack in stacks.values():
                try:
                    stack_priority = int(stack.meta.priority)
                    existing_priorities.add(stack_priority)
                except ValueError:
                    # meta.priority was unparsable, might be empty. Either way, no existing value to avoid.
                    pass
            while True:
                # pick a random number between 1000 and 5000 that's not in use right now.
                random.seed()
                priority = random.randint(1000, 5000)
                if priority not in existing_priorities:
                    break
            self.priority = priority


class Stack:
    """Represents a Happy Stack"""
    def __init__(self, stack_mgr, stack_name):
        self.stack_mgr = stack_mgr
        self.stack_name = stack_name

        self._meta = None

    @property
    def workspace(self):
        # If the corresponding workspace is missing from TFE, we will intentionally return None
        return self.stack_mgr.get_stack_workspace(self.stack_name)

    @property
    def outputs(self):
        if self.workspace:
            return self.workspace.outputs
        return {}

    @property
    def status(self):
        if self.workspace and self.workspace.latest_run:
            status = self.workspace.latest_run["data"]["attributes"]["status"]
            if self.workspace.latest_run["data"]["attributes"]["is-destroy"]:
                status += " destroy"
            return status
        return "UNKNOWN"

    @property
    def meta(self):
        if not self._meta:
            self._meta = StackMeta(self.stack_mgr.ctx, self.stack_name)
            # Default to unknown if missing data
            tags = {"happy/meta/owner": "UNKNOWN", "happy/meta/imagetag": "UNKNOWN"}
            if self.workspace:  # Non existent workspace has no meta data
                try:
                    meta_var = self.workspace.vars.get("terraform", {}).get("happymeta_")
                except exceptions.TFCHTTPNotFound:
                    meta_var = None
                if meta_var:
                    if meta_var["attributes"]["sensitive"]:
                        raise Exception(f"Invalid meta var for stack {self.stack_name}, must not be sensitive")
                    tags = json.loads(meta_var["attributes"]["value"])
                else:
                    print(f"No happymeta_ variable for stack {self.stack_name}")
                    # Any valid environment will have a tags variable; if missing
                    # don't add to list
            self._meta.load(tags)
        return self._meta

    def _ensure_workspace(self):
        if not self.workspace:
            raise Exception(f"Could not find TFE workspace for stack {self.stack_name}")

    def apply(self, wait):
        """Saves the variables and applies the workspace"""
        self._ensure_workspace()
        self.workspace.set_var(
            "happymeta_",
            json.dumps(self.meta.tags),
            "Happy Path metadata",
            sensitive=False,
        )
        for k, v in self.meta.parameters.items():
            self.workspace.set_var(k, str(v), "", sensitive=False)
        self.workspace.reset_cache()  # Resets known vars

        with config_tarball(self.stack_mgr.config.terraform_directory) as targz_file:
            config_version_id = self.workspace.upload_version(targz_file.name)

        self.workspace.run_config_version(config_version_id)
        if wait:
            self.workspace.wait()
        return True

    def destroy(self):
        self._ensure_workspace()
        self.workspace.run(is_destroy=True)
        return self.workspace.wait()

    def watch(self):
        self._ensure_workspace()
        return self.workspace.wait()

    def cancelupdate(self, wait):
        self._ensure_workspace()
        # TODO(mbarrien): Check run status to see if it's in a cancellable state
        if self.workspace.latest_run_id:
            self.workspace.cancel_run()
        if wait:
            return self.workspace.wait()
        return True

    def print_outputs(self):
        print()
        print("Module Outputs --")
        for k, v in self.outputs.items():
            print(f"{k}: {v}")


class StackMgr:
    def __init__(self, ctx):
        self.ctx = ctx
        self.config = ctx.obj["config"]
        self.write_path = f"/happy/{self.config.env}/stacklist"
        # self.read_prefix = f"/happy/{self.config.env}/stacks"
        self._stacks = {}
        self.creator_workspace_name = f"env-{self.config.env}"
        secrets = ctx.obj["secret_mgr"].secrets
        self.tfe_api = TfeApi(secrets["tfe_url"], secrets["tfe_org"])

    def remove(self, stack_name):
        self._stacks = {} # Force a refresh of stacks.
        stack_names = set(self.stacks.keys())
        stack_names.remove(stack_name)

        param_client = AwsSession.get_client(self.ctx, "ssm")
        param_client.put_parameter(Name=self.write_path, Value=json.dumps(sorted(stack_names)), Overwrite=True)
        self._resync(wait=False)
        del self._stacks[stack_name]

    def add(self, stack_name):
        self._stacks = {} # Force a refresh of stacks.
        stack_names = set(self.stacks.keys())
        stack_names.add(stack_name)

        param_client = AwsSession.get_client(self.ctx, "ssm")
        param_client.put_parameter(Name=self.write_path, Value=json.dumps(sorted(stack_names)), Overwrite=True)
        success = self._resync()
        if not success:
            raise Exception("Error invoking Terraform to create stack")

        if not self.get_stack_workspace(stack_name):
            raise Exception("Could not find newly created workspace for our stack")

        stack = Stack(self, stack_name)
        self._stacks[stack_name] = stack
        return stack

    def _resync(self, wait=True):
        """Invoke a specific TFE workspace that creates/deletes TFE workspaces, with prepopulated variables for identifier tokens."""
        print("Resyncing workspaces")
        workspace = self.tfe_api.workspaces[self.creator_workspace_name]
        workspace.run()
        self.tfe_api.reset_workspaces_cache()
        if wait:
            return workspace.wait()
        return True

    @property
    def stacks(self):
        if self._stacks:
            return self._stacks

        param_client = AwsSession.get_client(self.ctx, "ssm")
        param = param_client.get_parameter(Name=self.write_path)
        stacklist = json.loads(param["Parameter"]["Value"])
        for stack_name in stacklist:
            self._stacks[stack_name] = Stack(self, stack_name)
        return self._stacks

    def get_stack_workspace(self, stack_name):
        workspace_name = f"{self.config.env}-{stack_name}"
        return self.tfe_api.workspaces.get(workspace_name)


# Singleton for handling aws sessions since this is oddly slow.
class AwsSession:
    session = None
    config = None
    clients = {}

    @classmethod
    def get_session(cls, ctx):
        if not cls.session:
            cls.session = boto3.session.Session(profile_name=ctx.obj["aws_profile"])
        return cls.session

    @classmethod
    def get_config(cls, ctx):
        if not cls.config:
            cls.config = Config(region_name="us-west-2", retries={"max_attempts": 2, "mode": "standard"})
        return cls.config

    @classmethod
    def get_client(cls, ctx, client_type):
        if not cls.clients.get(client_type):
            session = cls.get_session(ctx)
            cls.clients[client_type] = session.client(client_type, config=cls.get_config(ctx))
        return cls.clients[client_type]


class TFEWorkspace:
    def __init__(self, tfc, workspace):
        self.tfc = tfc
        self.workspace = workspace
        self.reset_cache()

    @property
    def workspace_id(self):
        return self.workspace["id"]

    @property
    def name(self):
        return self.workspace["attributes"]["name"]

    @property
    def latest_run_id(self):
        if not self._latest_run_id:
            latest_run = self.workspace["relationships"]["latest-run"]["data"]
            if latest_run:
                self._latest_run_id = latest_run["id"]
            # latest_run == None if never run before
        return self._latest_run_id

    @property
    def latest_run(self):
        if not self._latest_run:
            if self.latest_run_id:
                self._latest_run = self.tfc.runs.show(self.latest_run_id)
        return self._latest_run

    @property
    def latest_config_version_id(self):
        if self.latest_run:
            return self.latest_run["data"]["relationships"]["configuration-version"]["data"]["id"]
        return None

    def run(self, is_destroy=False):
        return self.run_config_version(self.latest_config_version_id, is_destroy=is_destroy)

    def run_config_version(self, config_version_id, is_destroy=False):
        print(f"Running {'DESTROY ' if is_destroy else ''}workspace {self.name}")
        run = self.tfc.runs.create(
            {
                "data": {
                    "attributes": {
                        "is-destroy": is_destroy,
                        "message": "Queued from happy cli",
                    },
                    "type": "runs",
                    "relationships": {
                        "workspace": {
                            "data": {
                                "type": "workspaces",
                                "id": self.workspace_id,
                            }
                        },
                        "configuration-version": {
                            "data": {
                                "type": "configuration-versions",
                                "id": config_version_id,
                            }
                        },
                    },
                }
            }
        )
        run_id = run["data"]["id"]
        self._latest_run_id = run_id  # The run we just created is now the latest.
        self._latest_run = None  # Reset the cache
        self._outputs = None
        return True

    def wait(self):
        RUN_DONE_STATUSES = {"applied", "discarded", "errored", "canceled", "force_canceled", "policy_soft_failed"}
        last_status = ""
        while last_status not in RUN_DONE_STATUSES:
            if last_status:  # Skip sleep on first time
                time.sleep(5)
            run = self.tfc.runs.show(self.latest_run_id)
            status = run["data"]["attributes"]["status"]
            if status != last_status:
                print(f"{datetime.now().strftime('%H:%M:%S')} - {status}")
                last_status = status

        if last_status != "applied":
            print(f"Error applying, ended in status {last_status}")
            return False
        return True

    @property
    def vars(self):
        """Get a nested dict of all the variables of the given workspace.

        Returns a 2-deep nested dict.
        Top-level dict has 2 possible entries for the 2 kinds of variables a workspace may have,
        "terraform" and "env". Value of that top level entry is itself a dict of key->value.
        The inner value is a dict object as returned by Terraform Enterprise API.
        """
        if not self._vars:
            workspace_vars = self.tfc.workspace_vars.list(self.workspace_id)
            self._vars = {}
            for workspace_var in workspace_vars["data"]:
                attributes = workspace_var["attributes"]
                self._vars.setdefault(attributes["category"], {})[attributes["key"]] = workspace_var
        return self._vars

    def set_var(self, key, value, description, sensitive=True):
        category = "terraform"  # Hard-coded, not allowing setting environment vars directly
        var_data = {
            "data": {
                "type": "vars",
                "attributes": {
                    "key": key,
                    "value": value,
                    "description": description,
                    "category": category,
                    "sensitive": sensitive,
                },
            },
        }
        if category in self.vars and key in self.vars[category]:
            self.tfc.workspace_vars.update(self.workspace_id, self.vars[category][key]["id"], var_data)
        else:
            self.tfc.workspace_vars.create(self.workspace_id, var_data)
    
    def reset_cache(self):
        self._vars = None
        self._outputs = None
        self._latest_run_id = None
        self._latest_run = None

    @property
    def outputs(self):
        if self._outputs:
            return self._outputs
        try:
            state_version = self.tfc.state_versions.get_current(self.workspace_id)
        except exceptions.TFCHTTPNotFound:
            return {}
        # terrasnek api lacks a way to append ?include=outputs to state_version requests,
        # so we have to iterate through all outputs and get them individually
        # TODO(mbarrien): Add code to Terrasnek
        outputs = state_version["data"]["relationships"]["outputs"]["data"]
        state_version_output_ids = (output["id"] for output in outputs)
        self._outputs = {}
        for state_version_output_id in state_version_output_ids:
            state_version_output = self.tfc.state_version_outputs.show(state_version_output_id)["data"]["attributes"]
            if not state_version_output["sensitive"]:
                key = state_version_output["name"]
                value = state_version_output["value"]
                self._outputs[key] = value
        return self._outputs

    def upload_version(self, filename):
        # Not using auto-queue-runs, will explicitly create later
        config_version = self.tfc.config_versions.create(
            self.workspace_id, {"data": {"type": "configuration-versions", "attributes": {"auto-queue-runs": False}}}
        )

        config_version_id = config_version["data"]["id"]
        upload_url = config_version["data"]["attributes"]["upload-url"]
        self.tfc.config_versions.upload(filename, upload_url)
        return config_version_id

    def cancel_run(self):
        self.tfc.runs.force_cancel(self.latest_run_id, {"comment": "Force cancelled by happy cli"})


class TfeApi:
    def __init__(self, url, org):
        self.url = url
        self.org = org
        self._tfc = None
        self._workspaces = None

    @property
    def tfc(self):
        if self._tfc:
            return self._tfc
        hostname = urlparse(self.url).hostname
        error = False
        try:
            with open(os.path.expanduser("~/.terraform.d/credentials.tfrc.json")) as f:
                credentials = json.load(f)["credentials"]
        except FileNotFoundError:
            error = True
        if error or hostname not in credentials:
            raise CliError(
                f"Terraform credentials for {hostname} not found. Run 'terraform login {hostname}' and follow the instructions"
            )
        token = credentials[hostname]["token"]
        tfc = TFC(token, url=self.url)
        tfc.set_org(self.org)
        self._tfc = tfc
        return tfc

    @property
    def workspaces(self):
        if not self._workspaces:
            workspaces = self.tfc.workspaces.list_all()
            self._workspaces = {workspace["attributes"]["name"]: TFEWorkspace(self.tfc, workspace) for workspace in workspaces}
        return self._workspaces

    def reset_workspaces_cache(self):
        self._workspaces = None


@click.group()
@click.option("--profile", default=None, help="AWS profile to use")
@click.option("--env", default=None, help="Switch happy envs")
@click.pass_context
def cli(ctx, profile, env):
    ctx.ensure_object(dict)
    config = HappyConfig(env=env, ctx=ctx)
    if not profile:
        profile = config.aws_profile
    ctx.obj["secret_mgr"] = SecretMgr(ctx)
    ctx.obj["config"] = config
    ctx.obj["aws_profile"] = profile
    ctx.obj["stack_mgr"] = StackMgr(ctx)
    ctx.obj["orchestrator"] = Orchestrator(ctx)


class SecretMgr:
    def __init__(self, ctx):
        self.ctx = ctx
        self._secrets = None

    @property
    def secrets(self):
        if self._secrets:
            return self._secrets
        config = self.ctx.obj["config"]
        secrets_client = AwsSession.get_client(self.ctx, "secretsmanager")
        secrets = secrets_client.get_secret_value(SecretId=config.secret_arn)["SecretString"]
        self._secrets = json.loads(secrets)
        return self._secrets


def run_aws_cmd(ctx, cmd, return_output=True, json_output=True):
    command = ["aws", "--profile", AwsSession.get_session(ctx).profile_name, "--region", AwsSession.get_config(ctx).region_name]
    command.extend(cmd)
    if return_output:
        output = subprocess.check_output(command)
    else:
        subprocess.check_call(command)
        return
    if not json_output:
        return output
    return json.loads(output)


def resolve_owner(ctx):
    # Figure out what our current identity is
    sts_client = AwsSession.get_client(ctx, "sts")
    identity = sts_client.get_caller_identity()["Arn"]
    return identity.split("/")[-1].split("@")[0]


def generate_tag(ctx):
    now = datetime.now().strftime("%m%d-%H%M%S")
    owner = resolve_owner(ctx)
    return f"{owner}-{now}"


class Orchestrator:
    def __init__(self, ctx):
        self.ctx = ctx
        self.secrets = ctx.obj["secret_mgr"].secrets

    def run_task(self, taskdef_arn, wait=False, show_logs=True):
        """Run a one-off ECS task and optionally wait"""
        cluster_arn = self.secrets["cluster_arn"]
        subnets = self.secrets["subnets"]
        security_groups = self.secrets["security_groups"]
        print(f"Using task definition {taskdef_arn}")
        ecs_client = AwsSession.get_client(self.ctx, "ecs")
        output = ecs_client.run_task(
            cluster=cluster_arn,
            taskDefinition=taskdef_arn,
            networkConfiguration={
                "awsvpcConfiguration": {
                    "subnets": subnets.split(","),
                    "securityGroups": security_groups.split(","),
                    "assignPublicIp": "DISABLED",
                }
            },
        )
        task_info = output["tasks"][0]
        print(f"Task {task_info['taskArn']} started")
        if not wait:
            return

        # Wait for the task to start.
        waiter = ecs_client.get_waiter("tasks_running")
        waiter.wait(cluster=cluster_arn, tasks=[task_info["taskArn"]])
        result = ecs_client.describe_tasks(cluster=cluster_arn, tasks=[task_info["taskArn"]])
        container = result["tasks"][0]["containers"][0]
        status = container["lastStatus"]
        if status != "RUNNING":
            reason = ""
            if "reason" in container:
                reason = container["reason"]
            print(f"Container did not start. Current status {status}: {reason}")
            return
        print(f"Task {task_info['taskArn']} running")

        # Wait for the task to exit.
        waiter = ecs_client.get_waiter("tasks_stopped")
        waiter.wait(cluster=cluster_arn, tasks=[task_info["taskArn"]])
        print(f"Task {task_info['taskArn']} stopped")
        result = ecs_client.describe_tasks(cluster=cluster_arn, tasks=[task_info["taskArn"]])
        container = result["tasks"][0]["containers"][0]
        log_stream = container["runtimeId"]
        if "reason" in container:
            status = container["lastStatus"]
            reason = container["reason"]
            print(f"Container exited with status {status}: {reason}")

        # Get logs
        print("getting taskdef info")
        result = ecs_client.describe_task_definition(taskDefinition=taskdef_arn)
        taskdef = result["taskDefinition"]
        container = taskdef["containerDefinitions"][0]
        log_group = container["logConfiguration"]["options"]["awslogs-group"]
        print("Log Events:")
        logs_client = AwsSession.get_client(self.ctx, "logs")
        result = logs_client.get_log_events(logGroupName=log_group, logStreamName=log_stream)
        logs = result["events"]
        for log in logs:
            print(log)
        print("done!")


def run_tasks(ctx, stack, task_type, wait=False, show_logs=True):
    print(f"Running tasks for {task_type}")
    config = ctx.obj["config"]
    orchestrator = ctx.obj["orchestrator"]
    task_outputs = config.tasks.get(task_type, [])
    if not task_outputs:
        print(f"Found no tasks for {task_type}")
    try:
        tasks = [stack.outputs[task_output] for task_output in task_outputs]
    except KeyError as exc:
        raise CliError(f"Stack {stack.stack_name} is missing output field '{exc.args[0]}' for task {task_type}")
    for task in tasks:
        orchestrator.run_task(task, wait=wait, show_logs=show_logs)


@cli.command()
@click.argument("stack_name")
@click.option("--reset", is_flag=True, default=False, help="Drop and recreate the dev db from the latest snapshot")
@click.pass_context
def migrate(ctx, stack_name, reset):
    """Run DB migration task for given stack"""
    stack_mgr = ctx.obj["stack_mgr"]
    stack = stack_mgr.stacks[stack_name]
    if reset:
        run_tasks(ctx, stack, "delete", wait=True, show_logs=True)
    run_tasks(ctx, stack, "migrate", wait=True, show_logs=True)


@cli.command()
@click.argument("stack_name")
@click.argument("service")
@click.option("--since", default="10m", help="Output logs since <number>s|m|h|d")
@click.pass_context
def logs(ctx, stack_name, service, since):
    """Tail the logs of a service (frontend, backend, upload, migrations)"""
    run_aws_cmd(
        ctx,
        ["logs", "tail", "--since", since, "--follow", f"{stack_name}/{service}"],
        return_output=False,
        json_output=False,
    )

@cli.command()
@click.argument("stack_name")
@click.option("--tag", help="Tag name for docker image. Leave empty to generate one automatically.", default=None)
@click.option("--wait/--no-wait", is_flag=True, default=True, help="wait for this to complete")
@click.pass_context
def create(ctx, stack_name, tag, wait):
    """Create a dev stack with a given tag"""
    stackmgr = ctx.obj["stack_mgr"]
    if stack_name in stackmgr.stacks:
        raise CliError(f"Stack {stack_name} already exists")

    stack_meta = StackMeta(ctx, stack_name)
    stack_meta.load({"happy/meta/configsecret": ctx.obj["config"].secret_arn})
    if not tag:
        tag = generate_tag(ctx)
        ctx.invoke(push, tag=tag)
    stack_meta.update(tag, stackmgr)
    print(f"creating {stack_name}")

    stack = stackmgr.add(stack_name)
    stack._meta = stack_meta  # TODO(mbarrien): Hack!
    success = stack.apply(wait)
    if not success:
        raise CliError("Apply failed, skipping migrations")
    ctx.invoke(migrate, stack_name=stack_name)
    stack.print_outputs()


@contextmanager
def config_tarball(source_dir):
    """Create a config tarball from given source_dir, then automatically delete it once we're done."""
    targz_file = tempfile.NamedTemporaryFile(delete=False)
    try:
        with tarfile.open(fileobj=targz_file, mode="w:gz") as tar:
            tar.add(source_dir, arcname="")
        with targz_file:
            yield targz_file
    finally:
        os.remove(targz_file.name)


@cli.command()
@click.argument("stack_name")
@click.option("--tag", help="Tag name for docker image. Leave empty to generate one automatically.", default=None)
@click.option("--wait/--no-wait", is_flag=True, default=True, help="wait for this to complete")
@click.option("--skip-migrations/--do-migrations", is_flag=True, default=False, help="Skip running migrations")
@click.pass_context
def update(ctx, stack_name, tag, wait, skip_migrations):
    """Update a dev stack tag version"""
    stackmgr = ctx.obj["stack_mgr"]
    try:
        stack = stackmgr.stacks[stack_name]
    except KeyError:
        raise CliError(f"Stack {stack_name} does not exist")

    print(f"updating {stack_name}")
    if not tag:
        tag = generate_tag(ctx)
        ctx.invoke(push, tag=tag)

    stack_meta = stack.meta
    # Reset the configsecret if it has changed
    stack_meta.load({"happy/meta/configsecret": ctx.obj["config"].secret_arn})
    stack_meta.update(tag, stackmgr)
    success = stack.apply(wait or not skip_migrations)
    if not success:
        raise CliError("Apply failed, skipping migrations")
    if not skip_migrations:
        ctx.invoke(migrate, stack_name=stack_name)
    stack.print_outputs()


@cli.command()
@click.argument("stack_name")
@click.option("--wait/--no-wait", is_flag=True, default=True, help="wait for this to complete")
@click.pass_context
def cancelupdate(ctx, stack_name, wait):
    """Cancel a dev stack update"""
    print(f"Canceling update of {stack_name}")
    stack_mgr = ctx.obj["stack_mgr"]
    stack = stack_mgr.stacks[stack_name]
    stack.cancelupdate(wait)
    stack.print_outputs()


@cli.command()
@click.argument("stack_name")
@click.pass_context
def delete(ctx, stack_name):
    """Delete a dev stack"""
    stackmgr = ctx.obj["stack_mgr"]
    try:
        stack = stackmgr.stacks[stack_name]
    except Exception:
        raise CliError(f"Stack {stack_name} doesn't exist in our list")

    print(f"deleting {stack_name}")

    try:
        run_tasks(ctx, stack, "delete", wait=False, show_logs=False)
        print(f"Database dropped")
    except CliError:
        print(f"Database task missing, skipping delete")

    success = stack.destroy()
    do_remove_workspace = False
    if not success:
        do_remove_workspace = input(
            f"Error while destroying {stack_name}; resources might remain. Continue to remove workspace (y/n)? "
        ) in ["Y", "y", "yes", "YES"]
    if success or do_remove_workspace:
        stackmgr.remove(stack_name)
        print(f"Delete done")
    else:
        print(f"Delete NOT done")


@cli.command(name="list")  # don't redefine list()
@click.pass_context
def list_command(ctx):
    """List dev stacks"""
    env = ctx.obj["config"].env
    stackmgr = ctx.obj["stack_mgr"]
    print(f"Listing stacks in environment '{env}'")
    headings = ["Name", "Owner", "Tag", "Status", "URLs"]
    tp = TablePrinter(headings)
    for name, info in stackmgr.stacks.items():
        url = info.outputs.get("frontend_url", "")
        status = info.status
        tp.add_row([name, info.meta.owner, info.meta.imagetag, status, url])
    tp.print()


@cli.command()
@click.option("--tag", help="Tag name for docker image. Leave empty to generate one automatically.", default=None)
@click.pass_context
def push(ctx, tag):
    """Build and push docker images to ECR"""
    config = ctx.obj["config"]
    container_registries = config.container_registries
    ecr_client = AwsSession.get_client(ctx, "ecr")

    if not tag:
        tag = generate_tag(ctx)
    print("Building images...")
    subprocess.check_call(["docker-compose", "build"])

    ecr_re = r"(?P<registry>\d*\.dkr.ecr.(?P<region>[^.]*)\.amazonaws.com)/.*"
    print("logging in to ECR...")
    # Assumption: All the ECR registries are within the same AWS account as the one configured
    # for the profile, and in the same region as the default one.
    # TODO(mbarrien): Ensure this is true, print an error if not.
    # TODO(mbarrien): If correct account and wrong region, create an ecr_client with config
    # for the correct region, and login to that.
    first_repo = next(iter(container_registries.values()))
    first_registry = first_repo.split("/")[0]

    # Equivalent to aws get-login-password
    auth = ecr_client.get_authorization_token()['authorizationData'][0]
    auth_token = b64decode(auth['authorizationToken']).decode()
    pwd = auth_token.split(':')[1].encode()
    cmd = subprocess.run(["docker", "login", "--username", "AWS", "--password-stdin", first_registry], input=pwd)

    print("Tagging images...")
    for image, registry in container_registries.items():
        subprocess.check_call(["docker", "tag", f"{image}:latest", f"{registry}:{tag}"])
    print("Pushing images...")
    for registry in container_registries.values():
        subprocess.check_call(["docker", "push", f"{registry}:{tag}"])
    print(f"Built and pushed docker images with tag: {tag}")


@cli.command()
@click.argument("stack_name")
@click.pass_context
def watch(ctx, stack_name):
    """Wait until a dev stack is updated"""
    stack_mgr = ctx.obj["stack_mgr"]
    stack = stack_mgr.stacks[stack_name]
    stack.watch()
    stack.print_outputs()


if __name__ == "__main__":
    obj = {}
    try:
        cli.main(obj=obj)
    except exceptions.TFCHTTPUnauthorized as err:
        print(f"Not authorized to access TFE. Try going to {obj['secret_mgr'].secrets['tfe_url']} in your browser then rerunning your command.")
    except CliError as err:
        print(f"ERROR: {err}")
        exit(1)
