{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b68160f6",
   "metadata": {},
   "source": [
    "# Upload a local datafile to add or replace a Dataset in a Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff1ec6e",
   "metadata": {},
   "source": [
    "The script in this notebook performs the upload of a local datafile to a given Collection (as identified by its Collection uuid), where the datafile becomes a Dataset accessible via the Data Portal UI. **In order to use this script, you must...**\n",
    "- have a Curation API key (obtained from upper-righthand dropdown in the Data Portal UI after logging in)\n",
    "- you must know the id of the Collection to which you wish to upload the datafile (from `/collections/<collection_id>` in url path in Data Portal UI)\n",
    "- decide upon a string tag (the `curator_tag`) to use to uniquely identify the resultant Dataset within its Collection.\n",
    "\n",
    "Important rules for uploads:\n",
    "- Uploads to a curator tag that has not been used yet in the given Collection will result in a new Dataset being created.\n",
    "- Uploads to a curator tag for which there already exists a Dataset in the given Collection will result in the existing Dataset being replaced by the new Dataset created from the datafile that you are uploading.\n",
    "- You can only add/replace Datasets in private Collections or revision Collections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c6c402",
   "metadata": {},
   "source": [
    "#### <font color='#bc00b0'>Please fill in the required values:</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38afd0cd",
   "metadata": {},
   "source": [
    "<font color='#bc00b0'>(Required) Provide the path to your api key file</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "7c8c55fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key_file = \"path/to/api-key.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e84d52e",
   "metadata": {},
   "source": [
    "<font color='#bc00b0'>(Required) Provide the absolute path to the h5ad datafile to upload</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "f46e1874",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"/absolute/path/to-datafile.h5ad\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104aea94",
   "metadata": {},
   "source": [
    "<font color='#bc00b0'>(Required) Enter your chosen `curator_tag`, which will serve as a unique identifier _within this Collection_ for the resultant Dataset. **Must possess the '.h5ad' suffix**.</font>\n",
    "    \n",
    "_We recommmend using a tagging scheme that 1) makes sense to you, and 2) will help organize and facilitate your \n",
    "automation of future uploads for adding new Datasets and replacing existing Datasets._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "b2198be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "curator_tag = \"arbitrary/tag/chosen-by-you.h5ad\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463870f1",
   "metadata": {},
   "source": [
    "<font color='#bc00b0'>(Required) Enter the uuid of the Collection to which you wish to add this datafile as a Dataset</font>\n",
    "\n",
    "_The Collection uuid can be found by looking at the url path in the address bar \n",
    "when viewing your Collection in the UI of the Data Portal website:_ `collections/{collection_id}`_. You can only add/replace Datasets in private Collections or revision Collections (and not public ones)._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "f8bb640a",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_id = \"01234567-89ab-cdef-0123-456789abcdef\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352deec2",
   "metadata": {},
   "source": [
    "### Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d70d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "import requests\n",
    "import threading\n",
    "from botocore.credentials import RefreshableCredentials\n",
    "from botocore.session import get_session\n",
    "from datetime import datetime, timezone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1708156b",
   "metadata": {},
   "source": [
    "### Use API key to obtain a temporary access token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "7f3abf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = open(api_key_file).read().strip()  \n",
    "access_token_headers = {\"x-api-key\": api_key}\n",
    "access_token_url = \"https://api.cellxgene.dev.single-cell.czi.technology/curation/v1/auth/token\"\n",
    "resp = requests.post(access_token_url, headers=access_token_headers)\n",
    "access_token = resp.json().get(\"access_token\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea089d0e",
   "metadata": {},
   "source": [
    "##### (optional, debug) verify status code of response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22702ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(resp.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ae1713",
   "metadata": {},
   "source": [
    "### Define the method for retrieving temporary s3 write credentials. These credentials will only work for _this_ Collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "af25c7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_credentials_url = f\"https://api.cellxgene.dev.single-cell.czi.technology/curation/v1/collections/{collection_id}/datasets/s3-upload-credentials\"\n",
    "s3_cred_headers = {\"Authorization\": f\"Bearer {access_token}\"}\n",
    "\n",
    "time_zone_info = datetime.now(timezone.utc).astimezone().tzinfo\n",
    "\n",
    "def retrieve_s3_credentials():\n",
    "    resp = requests.post(s3_credentials_url, headers=s3_cred_headers)\n",
    "    s3_creds = resp.json().get(\"Credentials\")\n",
    "    s3_creds_formatted = {\n",
    "        \"access_key\": s3_creds.get(\"AccessKeyId\"),\n",
    "        \"secret_key\": s3_creds.get(\"SecretAccessKey\"),\n",
    "        \"token\": s3_creds.get(\"SessionToken\"),\n",
    "        \"expiry_time\": datetime.fromtimestamp(s3_creds.get(\"Expiration\")).replace(tzinfo=time_zone_info).isoformat(),\n",
    "    }\n",
    "    print(\"Retrieved/refreshed s3 credentials\")\n",
    "    return s3_creds_formatted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdb1fd2",
   "metadata": {},
   "source": [
    "### Define callback method for logging upload progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "45dac5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_progress_cb():\n",
    "    lock = threading.Lock()\n",
    "    uploaded_bytes = 0\n",
    "    prev_percent = 0\n",
    "\n",
    "    def progress_cb(num_bytes):\n",
    "        nonlocal uploaded_bytes\n",
    "        nonlocal prev_percent\n",
    "        should_update_progress_printout = False\n",
    "        \n",
    "        lock.acquire()\n",
    "        uploaded_bytes += num_bytes\n",
    "        percent_of_total_upload = float(\"{:.1f}\".format(uploaded_bytes / filesize * 100))\n",
    "        if percent_of_total_upload > prev_percent:\n",
    "            should_update_progress_printout = True\n",
    "        prev_percent = percent_of_total_upload\n",
    "        lock.release()\n",
    "        \n",
    "        if should_update_progress_printout:\n",
    "            print(f\"{percent_of_total_upload}% uploaded\\r\", end=\"\")\n",
    "            \n",
    "    return progress_cb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f648e0b2",
   "metadata": {},
   "source": [
    "### Upload file using temporary s3 credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c332b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "session_creds = RefreshableCredentials.create_from_metadata(\n",
    "    metadata=retrieve_s3_credentials(),\n",
    "    refresh_using=retrieve_s3_credentials,\n",
    "    method=\"sts-assume-role\",\n",
    ")\n",
    "session = get_session()\n",
    "session._credentials = session_creds\n",
    "boto3_session = boto3.Session(botocore_session=session)\n",
    "s3 = boto3_session.client(\"s3\")\n",
    "\n",
    "filesize = os.path.getsize(filename)\n",
    "\n",
    "try:\n",
    "    print(f\"Uploading {filename} to Collection {collection_id} with tag '{curator_tag}'...\")\n",
    "    s3.upload_file(\n",
    "        Filename=filename,\n",
    "        Bucket=\"cellxgene-dataset-submissions-dev\",\n",
    "        Key=f\"{collection_id}/{curator_tag}\",\n",
    "        Callback=get_progress_cb(),\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(\"\\n\\n\\033[1m\\033[38;5;9mFAILED\\033[0m\")  # 'FAILED' in bold red\n",
    "    print(f\"\\n\\n{e}\")\n",
    "else:\n",
    "    print(\"\\n\\n\\033[1m\\033[38;5;10mSUCCESS\\033[0m\")  # 'SUCCESS' in bold green\n",
    "    print(f\"\\nFile {filename} successfully uploaded to Collection {collection_id} with tag {curator_tag}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
