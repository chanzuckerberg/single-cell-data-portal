import gc
import logging
import time

import anndata
import numpy
import numpy as np
from pandas import DataFrame
import scanpy
import tiledb
from scipy import sparse
from scipy.sparse import csr_matrix, coo_matrix

from backend.wmg.pipeline.integrated_corpus.extract import get_X_raw
from backend.wmg.data.constants import (
    RANKIT_RAW_EXPR_COUNT_FILTERING_MIN_THRESHOLD,
    GENE_EXPRESSION_COUNT_MIN_THRESHOLD,
    INCLUDED_ASSAYS,
)
from backend.wmg.data.rankit import rankit
from backend.wmg.data.tissue_mapper import TissueMapper
from backend.wmg.data.schemas.corpus_schema import INTEGRATED_ARRAY_NAME

logger = logging.getLogger(__name__)


def apply_pre_concatenation_filters(anndata_object: anndata.AnnData, min_genes: int = None) -> anndata.AnnData:
    min_genes = min_genes if min_genes is not None else GENE_EXPRESSION_COUNT_MIN_THRESHOLD
    logger.info("Applying filters: assay, and lowly-covered cells")
    # Filter out cells with low coverage (less than GENE_EXPRESSION_COUNT_MIN_THRESHOLD unique genes expressed)
    scanpy.pp.filter_cells(anndata_object, min_genes=min_genes)

    # Filter out cells generated by assays that dont provide gene length normalization
    included_assay_ontology_ids = list(INCLUDED_ASSAYS.keys())
    anndata_object = anndata_object[
        anndata_object.obs["assay_ontology_term_id"].isin(included_assay_ontology_ids), :
    ].copy()
    return anndata_object


def create_high_level_tissue(anndata_object: anndata.AnnData):
    logger.info("Obtaining high-level tissues")
    anndata_object.obs["tissue_original"] = anndata_object.obs["tissue"]
    anndata_object.obs["tissue_original_ontology_term_id"] = anndata_object.obs["tissue_ontology_term_id"]
    anndata_object.obs = get_high_level_tissue(anndata_object.obs)


def get_high_level_tissue(obs: DataFrame) -> DataFrame:

    obs = obs.copy()

    tissue_mapper = TissueMapper()  # TODO: Slow. Try to speed up.

    tissue_ids_and_labels = obs[["tissue_ontology_term_id", "tissue"]].drop_duplicates().astype(str)
    new_tissue_ids = {}
    new_tissue_labels = {}

    # Create mapping dictionaries and if needed add new categories to obs.tissue and obs.tissue_ontology_term_id
    for row in tissue_ids_and_labels.iterrows():

        current_id = row[1]["tissue_ontology_term_id"]
        current_label = row[1]["tissue"]

        new_tissue_ids[current_id] = tissue_mapper.get_high_level_tissue(current_id)
        new_tissue_labels[current_label] = tissue_mapper.get_label_from_writable_id(new_tissue_ids[current_id])

    # Use mapping dictionaries to obtain new values
    obs["tissue_ontology_term_id"] = obs["tissue_ontology_term_id"].map(new_tissue_ids).astype("category")
    obs["tissue"] = obs["tissue"].map(new_tissue_labels).astype("category")

    return obs


def transform_dataset_raw_counts_to_rankit(
    anndata_object: anndata.AnnData, corpus_path: str, global_var_index: numpy.ndarray, first_obs_idx: int
):
    """
    Apply rankit normalization to raw count expression values and save to the tiledb corpus object
    """
    array_name = f"{corpus_path}/{INTEGRATED_ARRAY_NAME}"
    expression_matrix = get_X_raw(anndata_object)
    stride = max(int(np.power(10, np.around(np.log10(1e9 / expression_matrix.shape[1])))), 10_000)
    with tiledb.open(array_name, mode="w") as array:
        for start in range(0, expression_matrix.shape[0], stride):
            end = min(start + stride, expression_matrix.shape[0])
            raw_expression_csr_matrix = sparse.csr_matrix(expression_matrix[start:end, :])

            # Compute RankIt
            rankit_integrated_csr_matrix = rankit(raw_expression_csr_matrix)

            rankit_integrated_coo_matrix = rankit_integrated_csr_matrix.tocoo(copy=False)

            global_rows = rankit_integrated_coo_matrix.row + start + first_obs_idx
            global_cols = global_var_index[rankit_integrated_coo_matrix.col]

            rankit_data = rankit_integrated_coo_matrix.data
            rankit_data_filtered = rankit_data.copy()
            rankit_data_filtered[rankit_data_filtered < RANKIT_RAW_EXPR_COUNT_FILTERING_MIN_THRESHOLD] = 0

            assert len(rankit_data) == len(global_rows)
            assert len(rankit_data) == len(global_cols)

            array[global_rows, global_cols] = {"rankit": rankit_data_filtered, "rankit_unfiltered": rankit_data}
            del (
                raw_expression_csr_matrix,
                rankit_integrated_coo_matrix,
                rankit_integrated_csr_matrix,
                global_rows,
                global_cols,
                rankit_data,
            )
            gc.collect()

    logger.debug(f"Saved {array_name}.")
