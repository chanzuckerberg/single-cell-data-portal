{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58e11ed0",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "This file should be located in the root directory of `single-cell-data-portal`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b429461c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from backend.wmg.pipeline.cube_pipeline import load_data_and_create_cube\n",
    "from backend.wmg.pipeline.integrated_corpus.extract import get_dataset_asset_urls\n",
    "from backend.wmg.data.utils import get_datasets_from_curation_api\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025c22e1",
   "metadata": {},
   "source": [
    "## Set environment variables\n",
    "These variables are used for retrieving dataset assets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c8d3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env DEPLOYMENT_STAGE=staging\n",
    "%env API_URL=https://api.cellxgene.staging.single-cell.czi.technology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72f05c6",
   "metadata": {},
   "source": [
    "## Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082c5cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_DATASETS = 3 # download the first 3 datasets\n",
    "DATASET_FOLDER_NAME = \"pipeline_test_datasets\" # folder to download the data into\n",
    "MAX_GB_FILESIZE = 0.1 # only download datasets with < 0.1GB filesize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a783377",
   "metadata": {},
   "source": [
    "## Create datasets directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ed8f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system(f\"mkdir -p {DATASET_FOLDER_NAME}\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e582b20",
   "metadata": {},
   "source": [
    "## Get dataset asset URLs and metadata\n",
    "The metadata will be used to filter out datasets that are too large to be processed locally on a laptop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5bf30c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_urls = get_dataset_asset_urls()\n",
    "datasets = get_datasets_from_curation_api()\n",
    "datasets_by_ids = dict(zip([d['dataset_id'] for d in datasets],datasets))\n",
    "print(len(dataset_urls),'datasets')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c556c61d",
   "metadata": {},
   "source": [
    "## Download the first `NUM_DATASETS` datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade58ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_datasets_downloaded = 0\n",
    "for dataset_id,dataset_url in dataset_urls.items():\n",
    "    \n",
    "    print(dataset_id)\n",
    "\n",
    "    dataset = datasets_by_ids[dataset_id]\n",
    "    \n",
    "    too_big=False\n",
    "    for asset in dataset['assets']:\n",
    "        if asset['filetype']=='H5AD' and asset['filesize']/1e9 > MAX_GB_FILESIZE:\n",
    "            too_big=True\n",
    "            break\n",
    "\n",
    "    if too_big:\n",
    "        print(f\"Dataset bigger than {MAX_GB_FILESIZE}GB, skipping\")\n",
    "        continue\n",
    "\n",
    "    os.system(f\"mkdir -p {DATASET_FOLDER_NAME}/{dataset_id}\")\n",
    "    os.system(f\"curl -o {DATASET_FOLDER_NAME}/{dataset_id}/local.h5ad {dataset_url}\")\n",
    "    num_datasets_downloaded+=1\n",
    "    \n",
    "    if num_datasets_downloaded == NUM_DATASETS:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc9ed78",
   "metadata": {},
   "source": [
    "## Run the pipeline on the downloaded datasets\n",
    "If running on mac, you might not be able to install pygraphviz with `pip`. If you're using conda, you can install it with `conda install -c conda-forge pygraphviz`.\n",
    "\n",
    "At the end, the pipeline will attempt to upload the cube to the WMG s3 bucket. This will fail and is expected. Load the snapshot locally to explore its contents and assess whether the pipeline was successful. To learn how to load the snapshot locally, take a look at `example_dev_notebooks/local_endpoint_runner.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909f5b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_path, stats = load_data_and_create_cube(DATASET_FOLDER_NAME, extract_data=False, validate_cube=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
