{
  "CxG-czi-4": {
    "id": "CxG-czi-4",
    "title": "scVI integrated-embeddings with explicit modeling of batch effects",
    "description": "scVI uses autoencoding-variational Bayesian optimization to learn the underlying latent state of gene expression and to approximate the distributions that underlie observed expression values, while accounting for batch effects and limited sensitivity.  These cell embeddings are derived from an scVI model trained on all primary Census cells while accounting for the batch effects of sequencing assay, dataset, donor, and suspension type (cell vs nucleus). Then embeddings were obtained as the latent space for all Census cells after performing a forward pass through the trained model. These embeddings are made in collaboration with the scVI team from Nir Yosef's laboratory. For questions about scVI please refer to the scverse discourse forum https://discourse.scverse.org/.",
    "embedding_name": "scvi",
    "primary_contact": {
      "name": "CELLxGENE Discover Team",
      "email": "soma@chanzuckerberg.com",
      "affiliation": "CZI"
    },
    "additional_contacts": [
      {
        "name": "Nir Yosef",
        "email": "nir.yosef@weizmann.ac.il",
        "affiliation": "Weizmann Institute of Science, Israel"
      },
      {
        "name": "Can Ergen",
        "email": "cergen@berkeley.edu",
        "affiliation": "UC Berkeley"
      },
      {
        "name": "Martin Kim",
        "email": "martinkim@berkeley.edu",
        "affiliation": "UC Berkeley"
      }
    ],
    "DOI": "10.1038/s41592-018-0229-2",
    "additional_information": "scVI was trained on primary cells from Census with at least 300 expressed genes, and on the top 8000 highly variable genes with the Census method highly_variable_genes, which implements ScanPy's Seurat flavor to be used on count data while accounting for batch effects. The scVI parameters are: n_layers=2, n_hidden=500 and dropout_rate=0.1, and the training was run for a total of 100 epochs. An embedding with n_latent=50 was obtained by performing a forward pass on the model to get the latent representation across all Census cells, without additional retraining. Model is available for scArches workflow and census data can be loaded as minified data to test for organ-level differential abundance and similar. For full details and a reproducible workflow please see: https://github.com/chanzuckerberg/cellxgene-census/blob/main/tools/models/scvi/README.md",
    "model_link": "s3://cellxgene-contrib-public/models/scvi/2024-02-12/mus_musculus/model.pt",
    "data_type": "obs_embedding",
    "census_version": "2023-12-15",
    "experiment_name": "mus_musculus",
    "measurement_name": "RNA",
    "n_embeddings": 5684805,
    "n_features": 50,
    "submission_date": "2024-02-12",
    "relative_uri": "/contrib/cell-census/soma/2023-12-15/CxG-czi-4",
    "indexes": [
      {
        "relative_uri": "/contrib/cell-census/soma/2023-12-15/indexes/CxG-czi-4",
        "type": "IVFFlat"
      }
    ]
  },
  "CxG-czi-5": {
    "id": "CxG-czi-5",
    "title": "scVI integrated-embeddings with explicit modeling of batch effects",
    "description": "scVI uses autoencoding-variational Bayesian optimization to learn the underlying latent state of gene expression and to approximate the distributions that underlie observed expression values, while accounting for batch effects and limited sensitivity.  These cell embeddings are derived from an scVI model trained on all primary Census cells while accounting for the batch effects of sequencing assay, dataset, donor, and suspension type (cell vs nucleus). Then embeddings were obtained as the latent space for all Census cells after performing a forward pass through the trained model. These embeddings are made in collaboration with the scVI team from Nir Yosef's laboratory. For questions about scVI please refer to the scverse discourse forum https://discourse.scverse.org/.",
    "embedding_name": "scvi",
    "primary_contact": {
      "name": "CELLxGENE Discover Team",
      "email": "soma@chanzuckerberg.com",
      "affiliation": "CZI"
    },
    "additional_contacts": [
      {
        "name": "Nir Yosef",
        "email": "nir.yosef@weizmann.ac.il",
        "affiliation": "Weizmann Institute of Science, Israel"
      },
      {
        "name": "Can Ergen",
        "email": "cergen@berkeley.edu",
        "affiliation": "UC Berkeley"
      },
      {
        "name": "Martin Kim",
        "email": "martinkim@berkeley.edu",
        "affiliation": "UC Berkeley"
      }
    ],
    "DOI": "10.1038/s41592-018-0229-2",
    "additional_information": "scVI was trained on primary cells from Census with at least 300 expressed genes, and on the top 8000 highly variable genes with the Census method highly_variable_genes, which implements ScanPy's Seurat flavor to be used on count data while accounting for batch effects. The scVI parameters are: n_layers=2, n_hidden=500 and dropout_rate=0.1, and the training was run for a total of 100 epochs. An embedding with n_latent=50 was obtained by performing a forward pass on the model to get the latent representation across all Census cells, without additional retraining. Model is available for scArches workflow and census data can be loaded as minified data to test for organ-level differential abundance and similar. For full details and a reproducible workflow please see: https://github.com/chanzuckerberg/cellxgene-census/blob/main/tools/models/scvi/README.md",
    "model_link": "s3://cellxgene-contrib-public/models/scvi/2024-02-12/homo_sapiens/model.pt",
    "data_type": "obs_embedding",
    "census_version": "2023-12-15",
    "experiment_name": "homo_sapiens",
    "measurement_name": "RNA",
    "n_embeddings": 62998417,
    "n_features": 50,
    "submission_date": "2024-02-12",
    "relative_uri": "/contrib/cell-census/soma/2023-12-15/CxG-czi-5",
    "indexes": [
      {
        "relative_uri": "/contrib/cell-census/soma/2023-12-15/indexes/CxG-czi-5",
        "type": "IVFFlat"
      }
    ]
  },
  "CxG-czi-6": {
    "id": "CxG-czi-6",
    "title": "Geneformer embeddings fine-tuned on CELLxGENE Census for multi-task learning",
    "description": "Geneformer is a foundation deep learning model pretrained on a large-scale corpus of human single-cell transcriptomes to gain a fundamental understanding of gene network dynamics. This foundation knowledge can now be democratized to a vast array of downstream tasks through transfer learning to accelerate discovery of key network regulators and candidate therapeutic targets. Geneformer was previously demonstrated to enable predictions with limited data in a diverse range of downstream applications relevant to gene regulation, chromatin dynamics, and disease modeling. Furthermore, Geneformer drove new biological insights that were experimentally verified with functional assays in cells, including discovering a novel transcription factor in cardiomyocytes with zero-shot learning and identifying candidate therapeutic targets for cardiomyopathy that improved contractility in an iPS cell model of the disease. Here, using the 12 layer Geneformer model pretrained with ~95 million single-cell transcriptomes with an expanded input size of 4096, we provide embeddings from the model fine-tuned using a multi-task learning strategy to distinguish context-specific cell states across cell types, tissues, developmental stages, and diseases.",
    "embedding_name": "geneformer",
    "primary_contact": {
      "name": "CELLxGENE Discover Team",
      "email": "soma@chanzuckerberg.com",
      "affiliation": "CZI"
    },
    "additional_contacts": [],
    "DOI": "10.1038/s41586-023-06139-9",
    "additional_information": "We used the fine-tuned multi-task model by Theodoris et al. (huggingface.co/ctheodoris/Geneformer), with CLS tokens. Embeddings were then generated using Geneformer's EmbExtractor module with emb_layer=0.\\nFor full details and a reproducible workflow please see: https://github.com/chanzuckerberg/cellxgene-census/blob/main/tools/models/geneformer/README.md",
    "model_link": "s3://cellxgene-contrib-public/models/geneformer/2024-07-01/homo_sapiens/fined-tuned-model/",
    "data_type": "obs_embedding",
    "census_version": "2024-07-01",
    "experiment_name": "homo_sapiens",
    "measurement_name": "RNA",
    "n_embeddings": 74322510,
    "n_features": 512,
    "submission_date": "2024-07-24",
    "relative_uri": "/contrib/cell-census/soma/2024-07-01/CxG-czi-6",
    "indexes": [
      {
        "relative_uri": "/contrib/cell-census/soma/2024-07-01/indexes/CxG-czi-6",
        "type": "IVFFlat"
      }
    ]
  },
  "CxG-czi-7": {
    "id": "CxG-czi-7",
    "title": "scVI integrated-embeddings with explicit modeling of batch effects",
    "description": "scVI uses autoencoding-variational Bayesian optimization to learn the underlying latent state of gene expression and to approximate the distributions that underlie observed expression values, while accounting for batch effects and limited sensitivity.  These cell embeddings are derived from an scVI model trained on all primary Census cells while accounting for the batch effects of sequencing assay, dataset, donor, and suspension type (cell vs nucleus). Then embeddings were obtained as the latent space for all Census cells after performing a forward pass through the trained model. These embeddings are made in collaboration with the scVI team from Nir Yosef's laboratory. For questions about scVI please refer to the scverse discourse forum https://discourse.scverse.org/.",
    "embedding_name": "scvi",
    "primary_contact": {
      "name": "CELLxGENE Discover Team",
      "email": "soma@chanzuckerberg.com",
      "affiliation": "CZI"
    },
    "additional_contacts": [
      {
        "name": "Nir Yosef",
        "email": "nir.yosef@weizmann.ac.il",
        "affiliation": "Weizmann Institute of Science, Israel"
      },
      {
        "name": "Can Ergen",
        "email": "cergen@berkeley.edu",
        "affiliation": "UC Berkeley"
      },
      {
        "name": "Martin Kim",
        "email": "martinkim@berkeley.edu",
        "affiliation": "UC Berkeley"
      }
    ],
    "DOI": "10.1038/s41592-018-0229-2",
    "additional_information": "scVI was trained on primary cells from Census with at least 300 expressed genes, and on the top 8000 highly variable genes with the Census method highly_variable_genes, which implements ScanPy's Seurat flavor to be used on count data while accounting for batch effects. The scVI parameters are: n_layers=2, n_hidden=500 and dropout_rate=0.1, and the training was run for a total of 100 epochs. An embedding with n_latent=50 was obtained by performing a forward pass on the model to get the latent representation across all Census cells, without additional retraining. Model is available for scArches workflow and census data can be loaded as minified data to test for organ-level differential abundance and similar. For full details and a reproducible workflow please see: https://github.com/chanzuckerberg/cellxgene-census/blob/main/tools/models/scvi/README.md",
    "model_link": "s3://cellxgene-contrib-public/models/scvi/2024-07-01/mus_musculus/model.pt",
    "data_type": "obs_embedding",
    "census_version": "2024-07-01",
    "experiment_name": "mus_musculus",
    "measurement_name": "RNA",
    "n_embeddings": 41233630,
    "n_features": 50,
    "submission_date": "2024-07-08",
    "relative_uri": "/contrib/cell-census/soma/2024-07-01/CxG-czi-7",
    "indexes": [
      {
        "relative_uri": "/contrib/cell-census/soma/2024-07-01/indexes/CxG-czi-7",
        "type": "IVFFlat"
      }
    ]
  },
  "CxG-czi-8": {
    "id": "CxG-czi-8",
    "title": "scVI integrated-embeddings with explicit modeling of batch effects",
    "description": "scVI uses autoencoding-variational Bayesian optimization to learn the underlying latent state of gene expression and to approximate the distributions that underlie observed expression values, while accounting for batch effects and limited sensitivity.  These cell embeddings are derived from an scVI model trained on all primary Census cells while accounting for the batch effects of sequencing assay, dataset, donor, and suspension type (cell vs nucleus). Then embeddings were obtained as the latent space for all Census cells after performing a forward pass through the trained model. These embeddings are made in collaboration with the scVI team from Nir Yosef's laboratory. For questions about scVI please refer to the scverse discourse forum https://discourse.scverse.org/.",
    "embedding_name": "scvi",
    "primary_contact": {
      "name": "CELLxGENE Discover Team",
      "email": "soma@chanzuckerberg.com",
      "affiliation": "CZI"
    },
    "additional_contacts": [
      {
        "name": "Nir Yosef",
        "email": "nir.yosef@weizmann.ac.il",
        "affiliation": "Weizmann Institute of Science, Israel"
      },
      {
        "name": "Can Ergen",
        "email": "cergen@berkeley.edu",
        "affiliation": "UC Berkeley"
      },
      {
        "name": "Martin Kim",
        "email": "martinkim@berkeley.edu",
        "affiliation": "UC Berkeley"
      }
    ],
    "DOI": "10.1038/s41592-018-0229-2",
    "additional_information": "scVI was trained on primary cells from Census with at least 300 expressed genes, and on the top 8000 highly variable genes with the Census method highly_variable_genes, which implements ScanPy's Seurat flavor to be used on count data while accounting for batch effects. The scVI parameters are: n_layers=2, n_hidden=500 and dropout_rate=0.1, and the training was run for a total of 100 epochs. An embedding with n_latent=50 was obtained by performing a forward pass on the model to get the latent representation across all Census cells, without additional retraining. Model is available for scArches workflow and census data can be loaded as minified data to test for organ-level differential abundance and similar. For full details and a reproducible workflow please see: https://github.com/chanzuckerberg/cellxgene-census/blob/main/tools/models/scvi/README.md",
    "model_link": "s3://cellxgene-contrib-public/models/scvi/2024-07-01/homo_sapiens/model.pt",
    "data_type": "obs_embedding",
    "census_version": "2024-07-01",
    "experiment_name": "homo_sapiens",
    "measurement_name": "RNA",
    "n_embeddings": 74322510,
    "n_features": 50,
    "submission_date": "2024-07-08",
    "relative_uri": "/contrib/cell-census/soma/2024-07-01/CxG-czi-8",
    "indexes": [
      {
        "relative_uri": "/contrib/cell-census/soma/2024-07-01/indexes/CxG-czi-8",
        "type": "IVFFlat"
      }
    ]
  },
  "CxG-czi-1": {
    "id": "CxG-czi-1",
    "title": "Geneformer embeddings fine-tuned for CELLxGENE Census cell subclass classification",
    "description": "Geneformer is a foundation transformer model pretrained on a large-scale corpus of ~30 million single cell transcriptomes to enable context-aware predictions in settings with limited data in network biology. These cell embeddings are derived from a Geneformer model CZI fine-tuned for cell subclass classification. As the fine-tuning procedure remains experimental and wasn't performed by the Geneformer authors, these embeddings should not be used to assess performance of the Geneformer pre-trained model.",
    "embedding_name": "geneformer",
    "primary_contact": {
      "name": "CELLxGENE Discover Team",
      "email": "soma@chanzuckerberg.com",
      "affiliation": "CZI"
    },
    "additional_contacts": [],
    "DOI": "10.1038/s41586-023-06139-9",
    "additional_information": "Beginning with the geneformer-12L-30M pretrained model published by Theodoris et al. (huggingface.co/ctheodoris/Geneformer), a BertForSequenceClassification model was trained to predict cell subclass (as annotated in CELLxGENE Discover see https://cellxgene.cziscience.com/collections). Embeddings were then generated using Geneformer's EmbExtractor module with emb_layer=0.\\nFor full details and a reproducible workflow please see: https://github.com/chanzuckerberg/cellxgene-census/blob/main/tools/models/geneformer/README.md",
    "model_link": "s3://cellxgene-contrib-public/models/geneformer/2023-12-15/homo_sapiens/fined-tuned-model/",
    "data_type": "obs_embedding",
    "census_version": "2023-12-15",
    "experiment_name": "homo_sapiens",
    "measurement_name": "RNA",
    "n_embeddings": 62998417,
    "n_features": 512,
    "submission_date": "2023-12-11",
    "relative_uri": "/contrib/cell-census/soma/2023-12-15/CxG-czi-1",
    "indexes": [
      {
        "relative_uri": "/contrib/cell-census/soma/2023-12-15/indexes/CxG-czi-1",
        "type": "IVFFlat"
      }
    ]
  },
  "CxG-contrib-1": {
    "id": "CxG-contrib-1",
    "title": "scGPT: Towards Building a Foundation Model for Single-Cell Multi-omics Using Generative AI",
    "description": "Utilizing the burgeoning single-cell sequencing data, we have pioneered the construction of a foundation model for single-cell biology, scGPT, which is based on generative pre-trained transformer across a repository of over 33 million cells. Our findings illustrate that scGPT, a generative pre-trained transformer, effectively distills critical biological insights concerning genes and cells. Through the further adaptation of transfer learning, scGPT can be optimized to achieve superior performance across diverse downstream applications.",
    "embedding_name": "scgpt",
    "primary_contact": {
      "name": "Bo Wang",
      "email": "bowang@vectorinstitute.ai",
      "affiliation": "Bo Wang Lab, University of Toronto"
    },
    "additional_contacts": [],
    "DOI": "10.1101/2023.04.30.538439",
    "additional_information": "",
    "model_link": "https://github.com/bowang-lab/scGPT",
    "data_type": "obs_embedding",
    "census_version": "2023-12-15",
    "experiment_name": "homo_sapiens",
    "measurement_name": "RNA",
    "n_embeddings": 62998417,
    "n_features": 512,
    "submission_date": "2023-11-09",
    "relative_uri": "/contrib/cell-census/soma/2023-12-15/CxG-contrib-1",
    "indexes": [
      {
        "relative_uri": "/contrib/cell-census/soma/2023-12-15/indexes/CxG-contrib-1",
        "type": "IVFFlat"
      }
    ]
  },
  "CxG-contrib-7": {
    "id": "CxG-contrib-7",
    "title": "scGPT: Towards Building a Foundation Model for Single-Cell Multi-omics Using Generative AI",
    "description": "Utilizing the burgeoning single-cell sequencing data, we have pioneered the construction of a foundation model for single-cell biology, scGPT, which is based on generative pre-trained transformer across a repository of over 33 million cells. Our findings illustrate that scGPT, a generative pre-trained transformer, effectively distills critical biological insights concerning genes and cells. Through the further adaptation of transfer learning, scGPT can be optimized to achieve superior performance across diverse downstream applications.",
    "embedding_name": "scgpt",
    "primary_contact": {
      "name": "Bo Wang",
      "email": "bowang@vectorinstitute.ai",
      "affiliation": "Bo Wang Lab, University of Toronto"
    },
    "additional_contacts": [],
    "DOI": "10.1101/2023.04.30.538439",
    "additional_information": "",
    "model_link": "https://github.com/bowang-lab/scGPT",
    "data_type": "obs_embedding",
    "census_version": "2024-07-01",
    "experiment_name": "homo_sapiens",
    "measurement_name": "RNA",
    "n_embeddings": 74322510,
    "n_features": 512,
    "submission_date": "2024-07-08",
    "relative_uri": "/contrib/cell-census/soma/2024-07-01/CxG-contrib-7",
    "indexes": [
      {
        "relative_uri": "/contrib/cell-census/soma/2024-07-01/indexes/CxG-contrib-7",
        "type": "IVFFlat"
      }
    ]
  },
  "CxG-contrib-2": {
    "id": "CxG-contrib-2",
    "title": "Universal Cell Embeddings (UCE)",
    "description": "The Universal Cell Embedding model is a true foundation model for single cell biology. It can represent any cell, from any species, tissue or disease state in the same fixed embedding space, with no model retraining or fine-tuning (zero-shot). The UCE model is composed of 33 transformer layers and more than 650 million parameters. UCE was trained in a self supervised manner, so cell organization (e.g. into cell types) is an emergent behavior of the model, rather than of pre-existing human defined cell types.",
    "embedding_name": "uce",
    "primary_contact": {
      "name": "Yanay Rosen",
      "email": "yanay@stanford.edu",
      "affiliation": "Leskovec Lab, Stanford University"
    },
    "additional_contacts": [],
    "DOI": "10.1101/2023.11.28.568918",
    "additional_information": "",
    "model_link": "https://github.com/snap-stanford/UCE",
    "data_type": "obs_embedding",
    "census_version": "2023-12-15",
    "experiment_name": "homo_sapiens",
    "measurement_name": "RNA",
    "n_embeddings": 62996451,
    "n_features": 1280,
    "submission_date": "2023-11-15",
    "relative_uri": "/contrib/cell-census/soma/2023-12-15/CxG-contrib-2",
    "indexes": [
      {
        "relative_uri": "/contrib/cell-census/soma/2023-12-15/indexes/CxG-contrib-2",
        "type": "IVFFlat"
      }
    ]
  },
  "CxG-contrib-3": {
    "id": "CxG-contrib-3",
    "title": "Universal Cell Embeddings (UCE)",
    "description": "The Universal Cell Embedding model is a true foundation model for single cell biology. It can represent any cell, from any species, tissue or disease state in the same fixed embedding space, with no model retraining or fine-tuning (zero-shot). The UCE model is composed of 33 transformer layers and more than 650 million parameters. UCE was trained in a self supervised manner, so cell organization (e.g. into cell types) is an emergent behavior of the model, rather than of pre-existing human defined cell types.",
    "embedding_name": "uce",
    "primary_contact": {
      "name": "Yanay Rosen",
      "email": "yanay@stanford.edu",
      "affiliation": "Leskovec Lab, Stanford University"
    },
    "additional_contacts": [],
    "DOI": "10.1101/2023.11.28.568918",
    "additional_information": "",
    "model_link": "https://github.com/snap-stanford/UCE",
    "data_type": "obs_embedding",
    "census_version": "2023-12-15",
    "experiment_name": "mus_musculus",
    "measurement_name": "RNA",
    "n_embeddings": 5684805,
    "n_features": 1280,
    "submission_date": "2023-11-15",
    "relative_uri": "/contrib/cell-census/soma/2023-12-15/CxG-contrib-3",
    "indexes": [
      {
        "relative_uri": "/contrib/cell-census/soma/2023-12-15/indexes/CxG-contrib-3",
        "type": "IVFFlat"
      }
    ]
  },
  "CxG-contrib-4": {
    "id": "CxG-contrib-4",
    "title": "Non-negative Matrix Factorization (NMF) of human CellCensus transcriptomes (10x v1/2/3 + microwell-seq) at a rank of 200",
    "description": "Typical dimension reduction with NMF using methods implemented and publicly available in RcppML. NMF decomposes a matrix A into WDH, where W is a matrix of transcripts by factors, D is a diagonal scaling vector that gives contributions of each factor to the model, and H is a matrix of cell embeddints giving factors by cells.",
    "embedding_name": "nmf",
    "primary_contact": {
      "name": "Zachary DeBruine",
      "email": "debruinz@gvsu.edu",
      "affiliation": "DeBruine Lab, Applied Computing Institute, Grand Valley State University, Grand Rapids, MI"
    },
    "additional_contacts": [
      {
        "name": "Timothy J. Triche Jr.",
        "email": "tim.triche@vai.org",
        "affiliation": "Triche Lab, Van Andel Research Institute, Grand Rapids, MI"
      }
    ],
    "DOI": "",
    "additional_information": "",
    "model_link": "",
    "data_type": "obs_embedding",
    "census_version": "2023-12-15",
    "experiment_name": "homo_sapiens",
    "measurement_name": "RNA",
    "n_embeddings": 28534015,
    "n_features": 200,
    "submission_date": "2023-11-17",
    "relative_uri": "/contrib/cell-census/soma/2023-12-15/CxG-contrib-4",
    "indexes": []
  },
  "CxG-contrib-6": {
    "id": "CxG-contrib-6",
    "title": "Non-negative Matrix Factorization (NMF) of human CellCensus transcriptomes (10x v1/2/3 + microwell-seq) at a rank of 200",
    "description": "Typical dimension reduction with NMF using methods implemented and publicly available in RcppML. NMF decomposes a matrix A into WDH, where W is a matrix of transcripts by factors, D is a diagonal scaling vector that gives contributions of each factor to the model, and H is a matrix of cell embeddints giving factors by cells.",
    "embedding_name": "nmf",
    "primary_contact": {
      "name": "Zachary DeBruine",
      "email": "debruinz@gvsu.edu",
      "affiliation": "DeBruine Lab, Applied Computing Institute, Grand Valley State University, Grand Rapids, MI"
    },
    "additional_contacts": [
      {
        "name": "Timothy J. Triche Jr.",
        "email": "tim.triche@vai.org",
        "affiliation": "Triche Lab, Van Andel Research Institute, Grand Rapids, MI"
      }
    ],
    "DOI": "",
    "additional_information": "",
    "model_link": "",
    "data_type": "var_embedding",
    "census_version": "2023-12-15",
    "experiment_name": "homo_sapiens",
    "measurement_name": "RNA",
    "n_embeddings": 60664,
    "n_features": 200,
    "submission_date": "2023-11-17",
    "relative_uri": "/contrib/cell-census/soma/2023-12-15/CxG-contrib-6",
    "indexes": []
  },
  "CxG-czi-9": {
    "id": "CxG-czi-9",
    "embedding_name": "scvi",
    "title": "scVI integrated-embeddings with explicit modeling of batch effects",
    "description": "scVI uses autoencoding-variational Bayesian optimization to learn the underlying latent state of gene expression and to approximate the distributions that underlie observed expression values, while accounting for batch effects and limited sensitivity. \\nThese cell embeddings are derived from an scVI model trained on all primary Census cells while accounting for the batch effects of sequencing assay, dataset, donor, and suspension type (cell vs nucleus). Then embeddings were obtained as the latent space for all Census cells after performing a forward pass through the trained model.\\nThese embeddings are made in collaboration with the scVI team from Nir Yosef's laboratory. For questions about scVI please refer to the scverse discourse forum https://discourse.scverse.org/.",
    "primary_contact": {
      "name": "CELLxGENE Discover Team",
      "email": "soma@chanzuckerberg.com",
      "affiliation": "CZI"
    },
    "additional_contacts": [
      {
        "name": "Nir Yosef",
        "email": "nir.yosef@weizmann.ac.il",
        "affiliation": "Weizmann Institute of Science, Israel"
      },
      {
        "name": "Can Ergen",
        "email": "cergen@berkeley.edu",
        "affiliation": "UC Berkeley"
      },
      {
        "name": "Martin Kim",
        "email": "martinkim@berkeley.edu",
        "affiliation": "UC Berkeley"
      }
    ],
    "DOI": "10.1038/s41592-018-0229-2",
    "model_link": "s3://cellxgene-contrib-public/models/scvi/2025-01-30/mus_musculus/model.pt",
    "data_type": "obs_embedding",
    "census_version": "2025-01-30",
    "experiment_name": "mus_musculus",
    "measurement_name": "RNA",
    "n_embeddings": 42776863,
    "n_features": 50,
    "submission_date": "2025-01-30",
    "relative_uri": "/contrib/cell-census/soma/2025-01-30/CxG-czi-9",
    "indexes": [
      {
        "relative_uri": "/contrib/cell-census/soma/2025-01-30/indexes/CxG-czi-9",
        "type": "IVFFlat"
      }
    ]
  },
  "CxG-czi-10": {
    "id": "CxG-czi-10",
    "embedding_name": "scvi",
    "title": "scVI integrated-embeddings with explicit modeling of batch effects",
    "description": "scVI uses autoencoding-variational Bayesian optimization to learn the underlying latent state of gene expression and to approximate the distributions that underlie observed expression values, while accounting for batch effects and limited sensitivity. \\nThese cell embeddings are derived from an scVI model trained on all primary Census cells while accounting for the batch effects of sequencing assay, dataset, donor, and suspension type (cell vs nucleus). Then embeddings were obtained as the latent space for all Census cells after performing a forward pass through the trained model.\\nThese embeddings are made in collaboration with the scVI team from Nir Yosef's laboratory. For questions about scVI please refer to the scverse discourse forum https://discourse.scverse.org/.",
    "primary_contact": {
      "name": "CELLxGENE Discover Team",
      "email": "soma@chanzuckerberg.com",
      "affiliation": "CZI"
    },
    "additional_contacts": [
      {
        "name": "Nir Yosef",
        "email": "nir.yosef@weizmann.ac.il",
        "affiliation": "Weizmann Institute of Science, Israel"
      },
      {
        "name": "Can Ergen",
        "email": "cergen@berkeley.edu",
        "affiliation": "UC Berkeley"
      },
      {
        "name": "Martin Kim",
        "email": "martinkim@berkeley.edu",
        "affiliation": "UC Berkeley"
      }
    ],
    "DOI": "10.1038/s41592-018-0229-2",
    "model_link": "s3://cellxgene-contrib-public/models/scvi/2025-01-30/homo_sapiens/model.pt",
    "data_type": "obs_embedding",
    "census_version": "2025-01-30",
    "experiment_name": "homo_sapiens",
    "measurement_name": "RNA",
    "n_embeddings": 106118167,
    "n_features": 50,
    "submission_date": "2025-01-30",
    "relative_uri": "/contrib/cell-census/soma/2025-01-30/CxG-czi-10",
    "indexes": [
      {
        "relative_uri": "/contrib/cell-census/soma/2025-01-30/indexes/CxG-czi-10",
        "type": "IVFFlat"
      }
    ]
  },
  "CxG-czi-11": {
    "id": "CxG-czi-11",
    "embedding_name": "geneformer",
    "title": "Geneformer embeddings fine-tuned on CELLxGENE Census for multi-task learning",
    "description": "Geneformer is a foundation deep learning model pretrained on a large-scale corpus of human single-cell transcriptomes to gain a fundamental understanding of gene network dynamics. This foundation knowledge can now be democratized to a vast array of downstream tasks through transfer learning to accelerate discovery of key network regulators and candidate therapeutic targets. Geneformer was previously demonstrated to enable predictions with limited data in a diverse range of downstream applications relevant to gene regulation, chromatin dynamics, and disease modeling. Furthermore, Geneformer drove new biological insights that were experimentally verified with functional assays in cells, including discovering a novel transcription factor in cardiomyocytes with zero-shot learning and identifying candidate therapeutic targets for cardiomyopathy that improved contractility in an iPS cell model of the disease. Here, using the 12 layer Geneformer model pretrained with ~95 million single-cell transcriptomes with an expanded input size of 4096, we provide embeddings from the model fine-tuned using a multi-task learning strategy to distinguish context-specific cell states across cell types, tissues, developmental stages, and diseases.",
    "primary_contact": {
      "name": "CELLxGENE Discover Team",
      "email": "soma@chanzuckerberg.com",
      "affiliation": "CZI"
    },
    "additional_contacts": [],
    "DOI": "10.1038/s41586-023-06139-9",
    "model_link": "https://huggingface.co/ctheodoris/Geneformer/tree/main/fine_tuned_models/gf-12L-95M-i4096_MTLCellClassifier_CELLxGENE_240522",
    "data_type": "obs_embedding",
    "census_version": "2025-01-30",
    "experiment_name": "homo_sapiens",
    "measurement_name": "RNA",
    "n_embeddings": 106118167,
    "n_features": 512,
    "submission_date": "2025-01-30",
    "relative_uri": "/contrib/cell-census/soma/2025-01-30/CxG-czi-11",
    "indexes": [
      {
        "relative_uri": "/contrib/cell-census/soma/2025-01-30/indexes/CxG-czi-11",
        "type": "IVFFlat"
      }
    ]
  }
}
